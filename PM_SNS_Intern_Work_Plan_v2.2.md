# [PM] SNS ë¶„ì„ í”„ë¡œì íŠ¸ ì¸í„´ ì—…ë¬´ ê³„íšì„œ
## PM-International Korea íŒ€íŒŒíŠ¸ë„ˆ SNS í™œë™ ë¶„ì„ - ê¸°ìˆ  êµ¬í˜„ ê°€ì´ë“œ

**ë²„ì „**: v2.2  
**ì‘ì„±ì¼**: 2025ë…„ 10ì›” 27ì¼  
**í”„ë¡œì íŠ¸ ê¸°ê°„**: 12ì£¼ (3ê°œì›”)  
**ë‹´ë‹¹**: ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì¸í„´

---

## ğŸ“‘ ëª©ì°¨

I. í”„ë¡œì íŠ¸ ê°œìš”
   1. í”„ë¡œì íŠ¸ ëª©í‘œ (Objectives)
   2. í•µì‹¬ ì§ˆë¬¸ (Key Questions)
   3. ì„±ê³µ ê¸°ì¤€ (Success Metrics)
   4. ì „ì²´ ì•„í‚¤í…ì²˜ ê°œìš”

II. ë°ì´í„° ìˆ˜ì§‘ ë°©ë²•ë¡  (í”Œë«í¼ë³„ ìƒì„¸)
   1. ë„¤ì´ë²„ ë¸”ë¡œê·¸ (Naver Blog)
   2. ìœ íŠœë¸Œ (YouTube)
   3. ì¸ìŠ¤íƒ€ê·¸ë¨ (Instagram)
   4. ì¹´ì¹´ì˜¤ìŠ¤í† ë¦¬ (KakaoStory)
   5. í˜ì´ìŠ¤ë¶ (Facebook)

III. ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ë° êµ¬ì¶•
   1. Azure SQL Database vs Cosmos DB
   2. ìŠ¤í‚¤ë§ˆ ì„¤ê³„
   3. ì¸ë±ìŠ¤ ì „ëµ
   4. ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜

IV. OCR ë° ë™ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸ ì²˜ë¦¬
   1. ì´ë¯¸ì§€ OCR ì „ëµ
   2. ë™ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œ
   3. ë¹„ìš© ìµœì í™” ë°©ì•ˆ

V. ìë™í™” íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
   1. Azure Data Factory ì„¤ê³„
   2. ETL í”„ë¡œì„¸ìŠ¤
   3. ìŠ¤ì¼€ì¤„ë§ ë° ëª¨ë‹ˆí„°ë§
   4. ì—ëŸ¬ í•¸ë“¤ë§

VI. ë°ì´í„° ë¶„ì„ ë° ëª¨ë¸ë§
   1. íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)
   2. í´ëŸ¬ìŠ¤í„°ë§ (K-Means)
   3. ê°ì„± ë¶„ì„ (Sentiment Analysis)
   4. íšŒê·€ ë¶„ì„ (Regression)
   5. í†µê³„ì  ìœ ì˜ì„± ê²€ì¦

VII. ì‹œê°í™” ë° ëŒ€ì‹œë³´ë“œ
   1. Power BI ì—°ë™
   2. í•µì‹¬ KPI ì •ì˜
   3. ëŒ€ì‹œë³´ë“œ ì„¤ê³„

VIII. ê¸°ìˆ  ìŠ¤íƒ ë° í™˜ê²½ ì„¤ì •
   1. í•„ìš” ë„êµ¬ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬
   2. ê°œë°œ í™˜ê²½ êµ¬ì¶•
   3. ë²„ì „ ê´€ë¦¬ (Git)

IX. í”„ë¡œì íŠ¸ ì¼ì • ë° ë§ˆì¼ìŠ¤í†¤
   1. 12ì£¼ íƒ€ì„ë¼ì¸
   2. ì£¼ì°¨ë³„ ì‚°ì¶œë¬¼
   3. ë¦¬ìŠ¤í¬ ê´€ë¦¬

X. ì¸ìˆ˜ì¸ê³„ ë° ë¬¸ì„œí™”
   1. ì½”ë“œ ë¬¸ì„œí™” ê°€ì´ë“œ
   2. ìš´ì˜ ë§¤ë‰´ì–¼
   3. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

ë¶€ë¡
   A. Python ì½”ë“œ ì „ì²´ (ëª¨ë“ˆë³„)
   B. SQL ìŠ¤í¬ë¦½íŠ¸
   C. Azure ë¦¬ì†ŒìŠ¤ ì„¤ì • ê°€ì´ë“œ
   D. API ì¸ì¦ ì„¤ì • ê°€ì´ë“œ

---

# I. í”„ë¡œì íŠ¸ ê°œìš”

## 1. í”„ë¡œì íŠ¸ ëª©í‘œ (Objectives)

### 1.1 í•µì‹¬ ëª©í‘œ

> **"PM-International Korea íŒ€íŒŒíŠ¸ë„ˆì˜ SNS í™œë™ì„ ìë™ìœ¼ë¡œ ìˆ˜ì§‘Â·ë¶„ì„í•˜ì—¬, ë°ì´í„° ê¸°ë°˜ ë§ˆì¼€íŒ… ì „ëµ ìˆ˜ë¦½ì„ ì§€ì›í•œë‹¤."**

### 1.2 êµ¬ì²´ì  ëª©í‘œ

**1) ë°ì´í„° ìˆ˜ì§‘ ìë™í™”**
- 5ê°œ SNS í”Œë«í¼ì—ì„œ **ì¼ì¼ 500+ ê²Œì‹œë¬¼** ìë™ ìˆ˜ì§‘
- ì´ë¯¸ì§€ OCR ë° ë™ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸ í¬í•¨
- ì—ëŸ¬ìœ¨ **5% ì´í•˜** ìœ ì§€

**2) ë°ì´í„° ë¶„ì„**
- íŒ€íŒŒíŠ¸ë„ˆë¥¼ **4-6ê°œ í´ëŸ¬ìŠ¤í„°**ë¡œ ìœ í˜•í™”
- SNS í™œë™ê³¼ ë§¤ì¶œì˜ **ìƒê´€ê´€ê³„** ê·œëª… (RÂ² > 0.6)
- ê³ ì„±ê³¼ì **ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤** 10ê°œ ë°œêµ´

**3) ì¸ì‚¬ì´íŠ¸ ì œê³µ**
- Power BI ëŒ€ì‹œë³´ë“œ êµ¬ì¶•
- ì›”ê°„ ë¦¬í¬íŠ¸ ìë™ ìƒì„±
- ë¦¬ìŠ¤í¬ ê²Œì‹œë¬¼ ì¡°ê¸° íƒì§€ ì‹œìŠ¤í…œ

### 1.3 Out of Scope (ë²”ìœ„ ì™¸)

âŒ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ (ë°°ì¹˜ ì²˜ë¦¬ë§Œ)  
âŒ ê°ì • ì¶”ì  (ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ ë“± ê³ ê¸‰ ML)  
âŒ ë‹¤êµ­ì–´ ì§€ì› (í•œêµ­ì–´ë§Œ)  
âŒ ëª¨ë°”ì¼ ì•± ê°œë°œ

---

## 2. í•µì‹¬ ì§ˆë¬¸ (Key Questions)

ì´ í”„ë¡œì íŠ¸ê°€ ë‹µí•´ì•¼ í•  **êµ¬ì²´ì  ì§ˆë¬¸ë“¤**:

### 2.1 í™œë™ íŒ¨í„´

**Q1**: íŒ€íŒŒíŠ¸ë„ˆëŠ” í‰ê· ì ìœ¼ë¡œ ê° í”Œë«í¼ì—ì„œ ì–¼ë§ˆë‚˜ ìì£¼ ê²Œì‹œí•˜ëŠ”ê°€?  
**Q2**: ê³ ì„±ê³¼ìì™€ ì €ì„±ê³¼ìì˜ ê²Œì‹œ ë¹ˆë„ ì°¨ì´ëŠ”?  
**Q3**: ì–´ë–¤ ì‹œê°„ëŒ€ì— ê°€ì¥ ë§ì´ ê²Œì‹œí•˜ëŠ”ê°€?

### 2.2 ì½˜í…ì¸  ìœ í˜•

**Q4**: ì–´ë–¤ ì½˜í…ì¸  ìœ í˜•(í…ìŠ¤íŠ¸/ì´ë¯¸ì§€/ì˜ìƒ)ì´ ê°€ì¥ ë§ì€ê°€?  
**Q5**: í•´ì‹œíƒœê·¸ëŠ” í‰ê·  ëª‡ ê°œë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?  
**Q6**: ê°€ì¥ íš¨ê³¼ì ì¸ í•´ì‹œíƒœê·¸ ì¡°í•©ì€?

### 2.3 ì°¸ì—¬ìœ¨ (Engagement)

**Q7**: í”Œë«í¼ë³„ í‰ê·  ì°¸ì—¬ìœ¨(ì¢‹ì•„ìš”+ëŒ“ê¸€/ì¡°íšŒìˆ˜)ì€?  
**Q8**: ì–´ë–¤ ìš”ì¸ì´ ë†’ì€ ì°¸ì—¬ìœ¨ê³¼ ì—°ê´€ë˜ëŠ”ê°€?  
**Q9**: ê°ì„±(ê¸ì •/ë¶€ì •)ê³¼ ì°¸ì—¬ìœ¨ì˜ ê´€ê³„ëŠ”?

### 2.4 ì„±ê³¼ ì˜í–¥

**Q10**: SNS í™œë™ ë¹ˆë„ì™€ ì›” ë§¤ì¶œì˜ ìƒê´€ê´€ê³„ëŠ”? (ìƒê´€ê³„ìˆ˜ r)  
**Q11**: ì–´ë–¤ í”Œë«í¼ì´ ë§¤ì¶œ ê¸°ì—¬ë„ê°€ ê°€ì¥ ë†’ì€ê°€?  
**Q12**: ë‹¤ì¤‘ í”Œë«í¼ í™œìš© ì‹œ ë§¤ì¶œ ì¦ëŒ€ íš¨ê³¼ëŠ”?

### 2.5 ë¦¬ìŠ¤í¬

**Q13**: í—ˆìœ„Â·ê³¼ì¥ ê´‘ê³ ë¡œ ì˜ì‹¬ë˜ëŠ” ê²Œì‹œë¬¼ ë¹„ìœ¨ì€?  
**Q14**: ë¶€ì •ì  ê°ì„±ì˜ ê²Œì‹œë¬¼ ë¹„ìœ¨ì€?

---

## 3. ì„±ê³µ ê¸°ì¤€ (Success Metrics)

### 3.1 ì •ëŸ‰ì  ì§€í‘œ

| ì§€í‘œ | ëª©í‘œ | ì¸¡ì • ë°©ë²• |
|------|------|-----------|
| **ë°ì´í„° ìˆ˜ì§‘ëŸ‰** | ì›” 15,000+ ê²Œì‹œë¬¼ | ë°ì´í„°ë² ì´ìŠ¤ ë ˆì½”ë“œ ìˆ˜ |
| **ìˆ˜ì§‘ ì •í™•ë„** | 95% ì´ìƒ | ìˆ˜ë™ ìƒ˜í”Œë§ ê²€ì¦ (100ê±´) |
| **OCR ì •í™•ë„** | 80% ì´ìƒ | ìœ¡ì•ˆ ê²€ì¦ (50ê±´) |
| **íŒŒì´í”„ë¼ì¸ ê°€ë™ë¥ ** | 98% ì´ìƒ | ì¼ì¼ ì‹¤í–‰ ì„±ê³µë¥  |
| **ëª¨ë¸ ì„¤ëª…ë ¥** | RÂ² > 0.6 | íšŒê·€ ë¶„ì„ ê²°ê³¼ |
| **ëŒ€ì‹œë³´ë“œ ì™„ì„±ë„** | 10+ ì°¨íŠ¸ | Power BI ë¦¬í¬íŠ¸ |

### 3.2 ì •ì„±ì  ì§€í‘œ

âœ… í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €ê°€ ëŒ€ì‹œë³´ë“œë§Œ ë³´ê³  ì¸ì‚¬ì´íŠ¸ íŒŒì•… ê°€ëŠ¥  
âœ… íŒ€íŒŒíŠ¸ë„ˆê°€ ìì‹ ì˜ SNS ì „ëµ ê°œì„ ì  ë°œê²¬  
âœ… í–¥í›„ ìœ ì§€ë³´ìˆ˜ ê°€ëŠ¥í•œ ì½”ë“œ ë° ë¬¸ì„œ  
âœ… ë‹¤ë¥¸ êµ­ê°€ ë²•ì¸ì—ë„ ì ìš© ê°€ëŠ¥í•œ êµ¬ì¡°

---

## 4. ì „ì²´ ì•„í‚¤í…ì²˜ ê°œìš”

### 4.1 ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SNS í”Œë«í¼ (Data Sources)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ë„¤ì´ë²„ ë¸”ë¡œê·¸  â”‚  ìœ íŠœë¸Œ  â”‚  ì¸ìŠ¤íƒ€ê·¸ë¨  â”‚  ì¹´ì¹´ì˜¤ìŠ¤í† ë¦¬  â”‚  í˜ì´ìŠ¤ë¶  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚              â”‚           â”‚             â”‚            â”‚
         â”‚  API + Selenium  API      Instagram    Selenium    Graph API
         â”‚                           Graph API                      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚                          â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                   â”‚   Python Crawlers (Azure VM)        â”‚
                   â”‚   - naver_crawler.py                â”‚
                   â”‚   - youtube_crawler.py              â”‚
                   â”‚   - instagram_crawler.py            â”‚
                   â”‚   - kakao_crawler.py                â”‚
                   â”‚   - facebook_crawler.py             â”‚
                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â”‚ JSON/CSV
                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Azure Blob Storage (Raw Data)    â”‚
                   â”‚  /raw/naver/2024-10-27/           â”‚
                   â”‚  /raw/youtube/2024-10-27/         â”‚
                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â”‚ Trigger
                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Azure Data Factory Pipeline      â”‚
                   â”‚  1. Copy Activity (Blob â†’ Staging)â”‚
                   â”‚  2. Data Flow (Transform + OCR)   â”‚
                   â”‚  3. Copy Activity (SQL Load)      â”‚
                   â”‚  4. Stored Procedure (Aggregation)â”‚
                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â”‚ INSERT/UPDATE
                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Azure SQL Database               â”‚
                   â”‚  - dim_Users                      â”‚
                   â”‚  - fact_Posts                     â”‚
                   â”‚  - fact_VideoTranscripts          â”‚
                   â”‚  - agg_DailyMetrics               â”‚
                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â”‚ Query
                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Python Analysis (Local/Notebook) â”‚
                   â”‚  - clustering.py                  â”‚
                   â”‚  - sentiment_analysis.py          â”‚
                   â”‚  - regression.py                  â”‚
                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â”‚ Results
                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Power BI Dashboard               â”‚
                   â”‚  - ì‹¤ì‹œê°„ KPI ëª¨ë‹ˆí„°ë§              â”‚
                   â”‚  - í´ëŸ¬ìŠ¤í„°ë³„ ë¶„ì„                   â”‚
                   â”‚  - í”Œë«í¼ ë¹„êµ                       â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 ë°ì´í„° í”Œë¡œìš° (Data Flow)

**Step 1: ë°ì´í„° ìˆ˜ì§‘ (Daily 02:00)**
```
Python Crawlers â†’ API/Selenium â†’ JSON â†’ Blob Storage (/raw/)
```

**Step 2: ë°ì´í„° ë³€í™˜ (Daily 03:00)**
```
Blob Storage â†’ Data Factory â†’ Transform (OCR, í•´ì‹œíƒœê·¸ ì¶”ì¶œ) â†’ Staging Tables
```

**Step 3: ë°ì´í„° ì ì¬ (Daily 04:00)**
```
Staging â†’ MERGE INTO â†’ Production Tables (dim_Users, fact_Posts)
```

**Step 4: ì§‘ê³„ (Daily 05:00)**
```
Production Tables â†’ Stored Procedure â†’ agg_DailyMetrics
```

**Step 5: ë¶„ì„ (Weekly)**
```
SQL Database â†’ Python Analysis â†’ Results â†’ Power BI
```

---

# II. ë°ì´í„° ìˆ˜ì§‘ ë°©ë²•ë¡  (í”Œë«í¼ë³„ ìƒì„¸)

## 1. ë„¤ì´ë²„ ë¸”ë¡œê·¸ (Naver Blog)

### 1.1 ë¬¸ì œì  ë¶„ì„

**í˜„ì¬ ì½”ë“œì˜ ë¬¸ì œ:**
1. **í•´ì‹œíƒœê·¸ í•„í„°ë§ ì‹¤íŒ¨**: Naver Search APIëŠ” ê²€ìƒ‰ ê²°ê³¼ì— í•´ì‹œíƒœê·¸ë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŒ
2. **ë¶ˆì™„ì „í•œ í¬ë¡¤ë§**: ì´ë¯¸ì§€ URLë§Œ ìˆ˜ì§‘, OCR ë¯¸ì‹¤í–‰
3. **ë™ì˜ìƒ ëˆ„ë½**: ë™ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì§‘ ì•ˆ í•¨

### 1.2 í•´ê²°ì±…: 2ë‹¨ê³„ ì ‘ê·¼ë²•

**Phase 1: APIë¡œ URL ìˆ˜ì§‘ (ë¹ ë¦„, í•©ë²•ì )**
- Naver Search API ì‚¬ìš©
- í‚¤ì›Œë“œë³„ ë¸”ë¡œê·¸ URL ë¦¬ìŠ¤íŠ¸ í™•ë³´
- ì¼ì¼ ì œí•œ: 25,000 calls

**Phase 2: Seleniumìœ¼ë¡œ ë³¸ë¬¸ í¬ë¡¤ë§ (ìƒì„¸, ëŠë¦¼)**
- ê° URL ë°©ë¬¸í•˜ì—¬ ì „ì²´ HTML íŒŒì‹±
- ë³¸ë¬¸, ì´ë¯¸ì§€, ë™ì˜ìƒ, í•´ì‹œíƒœê·¸ ì¶”ì¶œ
- í•´ì‹œíƒœê·¸ë¡œ í•„í„°ë§

### 1.3 êµ¬í˜„ ì½”ë“œ

#### Step 1: Naver Search APIë¡œ URL ìˆ˜ì§‘

```python
import requests
import time
from datetime import datetime, timedelta

class NaverBlogSearcher:
    def __init__(self, client_id, client_secret):
        self.client_id = client_id
        self.client_secret = client_secret
        self.base_url = "https://openapi.naver.com/v1/search/blog.json"
    
    def search_keyword(self, keyword, max_results=100, days_back=30):
        """
        í‚¤ì›Œë“œë¡œ ë¸”ë¡œê·¸ ê²€ìƒ‰
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜ (API ì œí•œ: 1000)
            days_back: ìµœê·¼ ë©°ì¹  ë°ì´í„° (ì˜ˆ: 30ì¼)
        
        Returns:
            List[dict]: ë¸”ë¡œê·¸ URL ë¦¬ìŠ¤íŠ¸
        """
        headers = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret
        }
        
        results = []
        
        # APIëŠ” í•œ ë²ˆì— ìµœëŒ€ 100ê°œì”© ë°˜í™˜
        for start in range(1, min(max_results, 1000), 100):
            params = {
                "query": keyword,
                "display": 100,
                "start": start,
                "sort": "date"  # ìµœì‹ ìˆœ
            }
            
            try:
                response = requests.get(self.base_url, headers=headers, params=params)
                response.raise_for_status()
                data = response.json()
                
                items = data.get('items', [])
                if not items:
                    break  # ë” ì´ìƒ ê²°ê³¼ ì—†ìŒ
                
                # ë‚ ì§œ í•„í„°ë§
                cutoff_date = datetime.now() - timedelta(days=days_back)
                
                for item in items:
                    # postdate: "20241027" í˜•ì‹
                    post_date_str = item.get('postdate', '')
                    try:
                        post_date = datetime.strptime(post_date_str, '%Y%m%d')
                        if post_date < cutoff_date:
                            continue  # ë„ˆë¬´ ì˜¤ë˜ëœ ê²Œì‹œë¬¼
                    except:
                        pass
                    
                    results.append({
                        'title': self._clean_html(item.get('title', '')),
                        'link': item.get('link', ''),
                        'description': self._clean_html(item.get('description', '')),
                        'bloggername': item.get('bloggername', ''),
                        'bloggerlink': item.get('bloggerlink', ''),
                        'postdate': post_date_str
                    })
                
                print(f"[{keyword}] Collected {len(results)} results so far...")
                time.sleep(0.1)  # Rate limiting
                
            except Exception as e:
                print(f"Error searching keyword '{keyword}': {e}")
                break
        
        return results
    
    def _clean_html(self, text):
        """HTML íƒœê·¸ ì œê±°"""
        import re
        clean = re.sub('<.*?>', '', text)
        return clean

# ì‚¬ìš© ì˜ˆì‹œ
searcher = NaverBlogSearcher(
    client_id="9v7cOolOk2ctSQXc73sd",
    client_secret="9jHcXVNQwZ"
)

keywords = [
    "í”¼ì— ì¸í„°ë‚´ì…”ë„", "í”¼ì— ì½”ë¦¬ì•„", "PMì¸í„°ë‚´ì…”ë„", "ë…ì¼í”¼ì— ",
    "í•ë¼ì¸", "í”¼íŠ¸ë¼ì¸", "FitLine",
    "ë² ì´ì‹ìŠ¤", "ë² ì´ì§ìŠ¤", "Basics",
    "í”„ë¡œì…°ì´í”„", "í”„ë¡œì‰ì´í”„", "ProShape",
    "ì—‘í‹°ë°”ì´ì¦ˆ", "Activize",
    "íŒŒì›Œì¹µí…Œì¼", "PowerCocktail",
    "ë¦¬ìŠ¤í† ë ˆì´íŠ¸", "Restorate"
]

all_blog_urls = []
for kw in keywords:
    results = searcher.search_keyword(kw, max_results=100, days_back=30)
    all_blog_urls.extend(results)
    print(f"Keyword '{kw}': {len(results)} blogs")

# ì¤‘ë³µ ì œê±° (ë™ì¼ URL)
unique_urls = {item['link']: item for item in all_blog_urls}
all_blog_urls = list(unique_urls.values())
print(f"\nTotal unique blogs: {len(all_blog_urls)}")
```

#### Step 2: Seleniumìœ¼ë¡œ ë³¸ë¬¸ í¬ë¡¤ë§ + í•´ì‹œíƒœê·¸ í•„í„°ë§

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import re
import time

class NaverBlogCrawler:
    def __init__(self, headless=True):
        options = webdriver.ChromeOptions()
        if headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        self.driver = webdriver.Chrome(options=options)
        self.wait = WebDriverWait(self.driver, 10)
    
    def crawl_blog_post(self, url):
        """
        ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ í¬ë¡¤ë§
        
        Returns:
            dict: {
                'url': str,
                'title': str,
                'content_text': str,
                'hashtags': list,
                'images': list,
                'videos': list,
                'author_id': str,
                'post_date': str,
                'like_count': int,
                'comment_count': int
            }
        """
        try:
            self.driver.get(url)
            time.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # iframe ì „í™˜ (ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” iframe ë‚´ë¶€ì— ë³¸ë¬¸)
            try:
                iframe = self.wait.until(
                    EC.presence_of_element_located((By.ID, 'mainFrame'))
                )
                self.driver.switch_to.frame(iframe)
            except:
                print(f"iframe not found for {url}")
                return None
            
            # HTML íŒŒì‹±
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # ì œëª©
            title_elem = soup.select_one('div.se-title-text')
            title = title_elem.get_text(strip=True) if title_elem else ""
            
            # ë³¸ë¬¸
            content_div = soup.select_one('div.se-main-container')
            content_text = content_div.get_text(separator='\n', strip=True) if content_div else ""
            
            # í•´ì‹œíƒœê·¸ ì¶”ì¶œ
            hashtags = self._extract_hashtags(content_text)
            
            # ì´ë¯¸ì§€ URL
            images = []
            for img in soup.select('img.se-image-resource'):
                img_url = img.get('data-lazy-src') or img.get('src')
                if img_url:
                    images.append(img_url)
            
            # ë™ì˜ìƒ URL
            videos = []
            for video in soup.select('iframe[src*="youtube"], iframe[src*="naver"]'):
                video_url = video.get('src')
                if video_url:
                    videos.append(video_url)
            
            # ì‘ì„±ì ID (ë¸”ë¡œê·¸ ì£¼ì†Œì—ì„œ ì¶”ì¶œ)
            # ì˜ˆ: https://blog.naver.com/user_id/123456
            author_match = re.search(r'blog\.naver\.com/([^/]+)/', url)
            author_id = author_match.group(1) if author_match else ""
            
            # ë°œí–‰ì¼ (ì´ë¯¸ APIì—ì„œ ê°€ì ¸ì˜´, ì—¬ê¸°ì„œëŠ” ìƒëµ)
            
            # ì¢‹ì•„ìš”/ëŒ“ê¸€ ìˆ˜ (ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” ì‰½ê²Œ ê°€ì ¸ì˜¤ê¸° ì–´ë ¤ì›€, API ì‘ë‹µ ì‚¬ìš©)
            
            # iframeì—ì„œ ë‚˜ì˜¤ê¸°
            self.driver.switch_to.default_content()
            
            return {
                'url': url,
                'title': title,
                'content_text': content_text,
                'hashtags': hashtags,
                'images': images,
                'videos': videos,
                'author_id': author_id
            }
            
        except Exception as e:
            print(f"Error crawling {url}: {e}")
            return None
    
    def _extract_hashtags(self, text):
        """í•œê¸€/ì˜ë¬¸ í•´ì‹œíƒœê·¸ ì¶”ì¶œ"""
        pattern = r'#[ê°€-í£a-zA-Z0-9_]+'
        return re.findall(pattern, text)
    
    def close(self):
        self.driver.quit()

# ì‚¬ìš© ì˜ˆì‹œ
crawler = NaverBlogCrawler(headless=True)

# íƒ€ê²Ÿ í•´ì‹œíƒœê·¸
TARGET_HASHTAGS = [
    '#í”¼ì— ì¸í„°ë‚´ì…”ë„', '#í”¼ì— ì½”ë¦¬ì•„', '#PMì¸í„°ë‚´ì…”ë„', '#ë…ì¼í”¼ì— ',
    '#í•ë¼ì¸', '#í”¼íŠ¸ë¼ì¸', '#FitLine',
    '#ë² ì´ì‹ìŠ¤', '#ë² ì´ì§ìŠ¤', '#Basics',
    '#í”„ë¡œì…°ì´í”„', '#í”„ë¡œì‰ì´í”„', '#ProShape',
    '#ì—‘í‹°ë°”ì´ì¦ˆ', '#Activize',
    '#íŒŒì›Œì¹µí…Œì¼', '#PowerCocktail',
    '#ë¦¬ìŠ¤í† ë ˆì´íŠ¸', '#Restorate'
]

filtered_posts = []

for blog_info in all_blog_urls[:10]:  # í…ŒìŠ¤íŠ¸: ì²˜ìŒ 10ê°œë§Œ
    post_data = crawler.crawl_blog_post(blog_info['link'])
    
    if post_data:
        # í•´ì‹œíƒœê·¸ í•„í„°ë§
        if any(tag in post_data['hashtags'] for tag in TARGET_HASHTAGS):
            # APIì—ì„œ ê°€ì ¸ì˜¨ ì •ë³´ ë³‘í•©
            post_data.update({
                'post_date': blog_info['postdate'],
                'author_name': blog_info['bloggername']
            })
            filtered_posts.append(post_data)
            print(f"âœ“ Included: {post_data['title'][:30]}...")
        else:
            print(f"âœ— Filtered out (no target hashtag): {blog_info['title'][:30]}...")
    
    time.sleep(1)  # í¬ë¡¤ë§ ê°„ ëŒ€ê¸°

crawler.close()

print(f"\nìµœì¢… ìˆ˜ì§‘: {len(filtered_posts)}ê°œ ê²Œì‹œë¬¼")
```

#### Step 3: OCR ì²˜ë¦¬ (EasyOCR)

```python
import easyocr
from PIL import Image
from io import BytesIO
import requests

class OCRProcessor:
    def __init__(self, languages=['ko', 'en'], gpu=False):
        """
        Args:
            languages: ì¸ì‹ ì–¸ì–´ ë¦¬ìŠ¤íŠ¸
            gpu: GPU ì‚¬ìš© ì—¬ë¶€ (False = CPU)
        """
        print("Loading EasyOCR model...")
        self.reader = easyocr.Reader(languages, gpu=gpu)
        print("EasyOCR ready!")
    
    def extract_text_from_url(self, image_url, confidence_threshold=0.5):
        """
        ì´ë¯¸ì§€ URLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Args:
            image_url: ì´ë¯¸ì§€ URL
            confidence_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’ (0.0-1.0)
        
        Returns:
            str: ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        try:
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            response = requests.get(image_url, timeout=10)
            response.raise_for_status()
            
            # PIL Image ë³€í™˜
            img = Image.open(BytesIO(response.content))
            
            # OCR ì‹¤í–‰
            results = self.reader.readtext(img)
            
            # ì‹ ë¢°ë„ í•„í„°ë§
            texts = [r[1] for r in results if r[2] >= confidence_threshold]
            
            return '\n'.join(texts)
            
        except Exception as e:
            print(f"OCR error for {image_url}: {e}")
            return ""

# ì‚¬ìš© ì˜ˆì‹œ
ocr = OCRProcessor(languages=['ko', 'en'], gpu=False)

for post in filtered_posts:
    ocr_results = {}
    
    # ì²˜ìŒ 5ê°œ ì´ë¯¸ì§€ë§Œ OCR (ë¹„ìš©/ì‹œê°„ ì ˆì•½)
    for img_url in post['images'][:5]:
        text = ocr.extract_text_from_url(img_url)
        if text:
            ocr_results[img_url] = text
    
    post['ocr_results'] = ocr_results
    print(f"OCR completed for: {post['title'][:30]}... ({len(ocr_results)} images)")
```

#### Step 4: ë™ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œ

```python
from youtube_transcript_api import YouTubeTranscriptApi
import re

class VideoTranscriptExtractor:
    def extract_youtube_transcript(self, youtube_url):
        """
        YouTube ì˜ìƒì—ì„œ ìë§‰ ì¶”ì¶œ
        
        Args:
            youtube_url: YouTube URL ë˜ëŠ” iframe src
        
        Returns:
            str: ìë§‰ í…ìŠ¤íŠ¸
        """
        # Video ID ì¶”ì¶œ
        video_id = self._extract_video_id(youtube_url)
        if not video_id:
            return ""
        
        try:
            # ìë§‰ ë‹¤ìš´ë¡œë“œ (í•œêµ­ì–´ ìš°ì„ , ì—†ìœ¼ë©´ ì˜ì–´)
            transcript = YouTubeTranscriptApi.get_transcript(
                video_id,
                languages=['ko', 'en']
            )
            
            # í…ìŠ¤íŠ¸ ê²°í•©
            full_text = ' '.join([entry['text'] for entry in transcript])
            return full_text
            
        except Exception as e:
            print(f"Transcript error for {video_id}: {e}")
            return ""
    
    def _extract_video_id(self, url):
        """YouTube URLì—ì„œ video ID ì¶”ì¶œ"""
        patterns = [
            r'(?:youtube\.com/watch\?v=|youtu\.be/)([^&\n?]+)',
            r'youtube\.com/embed/([^&\n?]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        return None

# ì‚¬ìš© ì˜ˆì‹œ
video_extractor = VideoTranscriptExtractor()

for post in filtered_posts:
    transcripts = {}
    
    for video_url in post['videos']:
        if 'youtube' in video_url:
            transcript = video_extractor.extract_youtube_transcript(video_url)
            if transcript:
                transcripts[video_url] = transcript
    
    post['video_transcripts'] = transcripts
```

#### Step 5: CSV ì €ì¥

```python
import pandas as pd
import json

def save_to_csv(posts, filename='naver_blog_data.csv'):
    """
    ìˆ˜ì§‘ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥
    """
    # ë°ì´í„° ë³€í™˜
    rows = []
    for post in posts:
        row = {
            'platform': 'Naver Blog',
            'post_url': post['url'],
            'author_id': post['author_id'],
            'author_name': post['author_name'],
            'title': post['title'],
            'content_text': post['content_text'],
            'post_date': post['post_date'],
            'hashtags': ', '.join(post['hashtags']),
            'image_count': len(post['images']),
            'video_count': len(post['videos']),
            'ocr_text': '\n---\n'.join(post['ocr_results'].values()),
            'video_transcripts': '\n---\n'.join(post['video_transcripts'].values())
        }
        rows.append(row)
    
    # DataFrame ìƒì„±
    df = pd.DataFrame(rows)
    
    # CSV ì €ì¥
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"Saved {len(df)} posts to {filename}")
    
    return df

# ì‹¤í–‰
df = save_to_csv(filtered_posts, 'naver_blog_crawl_result.csv')
print(df.head())
```

### 1.4 ì„±ëŠ¥ ìµœì í™”

**ë³‘ë ¬ ì²˜ë¦¬ (ë©€í‹°ìŠ¤ë ˆë”©):**

```python
from concurrent.futures import ThreadPoolExecutor, as_completed

def crawl_with_threading(urls, max_workers=5):
    """
    ë³‘ë ¬ë¡œ í¬ë¡¤ë§ (5ê°œ ë™ì‹œ)
    """
    crawler = NaverBlogCrawler(headless=True)
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        future_to_url = {
            executor.submit(crawler.crawl_blog_post, url): url 
            for url in urls
        }
        
        # Collect results
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                result = future.result()
                if result:
                    results.append(result)
            except Exception as e:
                print(f"Error with {url}: {e}")
    
    crawler.close()
    return results

# ì‚¬ìš©
results = crawl_with_threading([b['link'] for b in all_blog_urls], max_workers=5)
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„:**
- API ê²€ìƒ‰: 5ë¶„ (500 URLs)
- Selenium í¬ë¡¤ë§: 30ë¶„ (500 URLs, 5ë³‘ë ¬)
- OCR: 60ë¶„ (2,500 ì´ë¯¸ì§€, 5 images/post)
- **ì´ 95ë¶„**

---

## 2. ìœ íŠœë¸Œ (YouTube)

### 2.1 YouTube Data API v3 ì‚¬ìš©

**API ì œí•œ:**
- ì¼ì¼ í• ë‹¹ëŸ‰: **10,000 units**
- Search ìš”ì²­: 100 units
- Video ìƒì„¸ ì¡°íšŒ: 1 unit
- ìµœëŒ€: **ì¼ ì•½ 100 ê²€ìƒ‰** ë˜ëŠ” **10,000 ìƒì„¸ ì¡°íšŒ**

### 2.2 êµ¬í˜„ ì½”ë“œ

```python
from googleapiclient.discovery import build
from datetime import datetime, timedelta

class YouTubeCrawler:
    def __init__(self, api_key):
        self.youtube = build('youtube', 'v3', developerKey=api_key)
    
    def search_videos(self, keyword, max_results=50, days_back=30):
        """
        í‚¤ì›Œë“œë¡œ ì˜ìƒ ê²€ìƒ‰
        """
        # ë‚ ì§œ í•„í„°
        published_after = (datetime.now() - timedelta(days=days_back)).isoformat() + 'Z'
        
        try:
            request = self.youtube.search().list(
                part="snippet",
                q=keyword,
                type="video",
                maxResults=min(max_results, 50),  # API ì œí•œ
                order="date",  # ìµœì‹ ìˆœ
                publishedAfter=published_after
            )
            response = request.execute()
            
            videos = []
            for item in response['items']:
                video_id = item['id']['videoId']
                videos.append({
                    'video_id': video_id,
                    'url': f"https://www.youtube.com/watch?v={video_id}",
                    'title': item['snippet']['title'],
                    'description': item['snippet']['description'],
                    'channel_title': item['snippet']['channelTitle'],
                    'channel_id': item['snippet']['channelId'],
                    'published_at': item['snippet']['publishedAt']
                })
            
            return videos
            
        except Exception as e:
            print(f"YouTube search error: {e}")
            return []
    
    def get_video_stats(self, video_ids):
        """
        ì˜ìƒ í†µê³„ (ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€ ìˆ˜) ê°€ì ¸ì˜¤ê¸°
        
        Args:
            video_ids: list of video IDs (ìµœëŒ€ 50ê°œ)
        """
        try:
            request = self.youtube.videos().list(
                part="statistics,snippet",
                id=','.join(video_ids)
            )
            response = request.execute()
            
            stats = {}
            for item in response['items']:
                video_id = item['id']
                stats[video_id] = {
                    'view_count': int(item['statistics'].get('viewCount', 0)),
                    'like_count': int(item['statistics'].get('likeCount', 0)),
                    'comment_count': int(item['statistics'].get('commentCount', 0)),
                    'duration': item['snippet'].get('duration', ''),
                    'tags': item['snippet'].get('tags', [])
                }
            
            return stats
            
        except Exception as e:
            print(f"Stats error: {e}")
            return {}

# ì‚¬ìš© ì˜ˆì‹œ
youtube_crawler = YouTubeCrawler(api_key='YOUR_YOUTUBE_API_KEY')

youtube_keywords = [
    "í”¼ì— ì¸í„°ë‚´ì…”ë„", "PM International Korea", "FitLine",
    "í•ë¼ì¸ í›„ê¸°", "ProShape ë‹¤ì´ì–´íŠ¸"
]

all_videos = []
for kw in youtube_keywords:
    videos = youtube_crawler.search_videos(kw, max_results=50, days_back=30)
    all_videos.extend(videos)

# ì¤‘ë³µ ì œê±°
unique_videos = {v['video_id']: v for v in all_videos}
all_videos = list(unique_videos.values())

# í†µê³„ ê°€ì ¸ì˜¤ê¸° (50ê°œì”© ë°°ì¹˜)
for i in range(0, len(all_videos), 50):
    batch = all_videos[i:i+50]
    video_ids = [v['video_id'] for v in batch]
    stats = youtube_crawler.get_video_stats(video_ids)
    
    for video in batch:
        vid = video['video_id']
        if vid in stats:
            video.update(stats[vid])

print(f"Total YouTube videos: {len(all_videos)}")
```

### 2.3 ìë§‰ ë‹¤ìš´ë¡œë“œ

```python
from youtube_transcript_api import YouTubeTranscriptApi

def get_all_transcripts(video_list):
    """
    ì˜ìƒ ë¦¬ìŠ¤íŠ¸ì—ì„œ ëª¨ë“  ìë§‰ ë‹¤ìš´ë¡œë“œ
    """
    for video in video_list:
        try:
            transcript = YouTubeTranscriptApi.get_transcript(
                video['video_id'],
                languages=['ko', 'en']
            )
            video['transcript'] = ' '.join([t['text'] for t in transcript])
            print(f"âœ“ Transcript: {video['title'][:30]}...")
        except:
            video['transcript'] = ""
            print(f"âœ— No transcript: {video['title'][:30]}...")

get_all_transcripts(all_videos)
```

---

## 3. ì¸ìŠ¤íƒ€ê·¸ë¨ (Instagram)

### 3.1 Instagram Graph API ì„¤ì •

**í•„ìˆ˜ ì¡°ê±´:**
1. Facebook ê°œë°œì ê³„ì •
2. Instagram Business Account
3. Facebook Pageì™€ Instagram ì—°ê²°

### 3.2 Access Token ë°œê¸‰ (ìˆ˜ë™)

**ë‹¨ê³„:**
1. https://developers.facebook.com/ ì ‘ì†
2. "My Apps" â†’ "Create App"
3. App Type: "Business"
4. App ì„¤ì • â†’ Instagram Graph API ì¶”ê°€
5. Tools â†’ Graph API Explorer
6. Permissions: `instagram_basic`, `instagram_manage_insights`, `pages_read_engagement`
7. "Generate Access Token" í´ë¦­
8. **Short-lived Token**(1ì‹œê°„) ë°œê¸‰
9. Exchange for **Long-lived Token**(60ì¼)

**Token êµí™˜ ì½”ë“œ:**

```python
import requests

def exchange_for_long_lived_token(short_token, app_id, app_secret):
    """
    Short-lived token (1ì‹œê°„) â†’ Long-lived token (60ì¼)
    """
    url = "https://graph.facebook.com/v18.0/oauth/access_token"
    params = {
        'grant_type': 'fb_exchange_token',
        'client_id': app_id,
        'client_secret': app_secret,
        'fb_exchange_token': short_token
    }
    
    response = requests.get(url, params=params)
    data = response.json()
    
    long_token = data.get('access_token')
    expires_in = data.get('expires_in')  # ì´ˆ ë‹¨ìœ„ (ì•½ 5184000 = 60ì¼)
    
    print(f"Long-lived token expires in {expires_in / 86400:.0f} days")
    return long_token

# ì‚¬ìš©
SHORT_TOKEN = "YOUR_SHORT_LIVED_TOKEN"
APP_ID = "YOUR_APP_ID"
APP_SECRET = "YOUR_APP_SECRET"

long_token = exchange_for_long_lived_token(SHORT_TOKEN, APP_ID, APP_SECRET)
print(f"Long-lived token: {long_token}")
```

### 3.3 Instagram ë°ì´í„° ìˆ˜ì§‘

```python
import requests
import time

class InstagramCrawler:
    def __init__(self, access_token):
        self.access_token = access_token
        self.base_url = "https://graph.facebook.com/v18.0"
    
    def get_user_media(self, instagram_account_id, limit=100):
        """
        ì‚¬ìš©ìì˜ ë¯¸ë””ì–´(ê²Œì‹œë¬¼) ê°€ì ¸ì˜¤ê¸°
        
        Args:
            instagram_account_id: Instagram Business Account ID
            limit: ìµœëŒ€ ê°œìˆ˜
        """
        url = f"{self.base_url}/{instagram_account_id}/media"
        params = {
            'fields': 'id,caption,media_type,media_url,permalink,timestamp,like_count,comments_count',
            'access_token': self.access_token,
            'limit': limit
        }
        
        all_media = []
        
        while True:
            try:
                response = requests.get(url, params=params)
                response.raise_for_status()
                data = response.json()
                
                media_list = data.get('data', [])
                all_media.extend(media_list)
                
                # ë‹¤ìŒ í˜ì´ì§€
                if 'paging' in data and 'next' in data['paging']:
                    url = data['paging']['next']
                    params = {}  # next URLì— ì´ë¯¸ íŒŒë¼ë¯¸í„° í¬í•¨
                else:
                    break
                
                time.sleep(0.5)  # Rate limit
                
            except Exception as e:
                print(f"Instagram API error: {e}")
                break
        
        return all_media
    
    def get_hashtag_search(self, instagram_account_id, hashtag):
        """
        í•´ì‹œíƒœê·¸ë¡œ ê²€ìƒ‰ (ìì‹ ì˜ ë¯¸ë””ì–´ë§Œ)
        
        ì£¼ì˜: Instagram APIëŠ” ìì‹ ì˜ ê³„ì • ë¯¸ë””ì–´ë§Œ ê²€ìƒ‰ ê°€ëŠ¥
        """
        # Hashtag ID ì¡°íšŒ
        url = f"{self.base_url}/ig_hashtag_search"
        params = {
            'user_id': instagram_account_id,
            'q': hashtag,
            'access_token': self.access_token
        }
        
        response = requests.get(url, params=params)
        data = response.json()
        
        if not data.get('data'):
            return []
        
        hashtag_id = data['data'][0]['id']
        
        # í•´ì‹œíƒœê·¸ê°€ ë‹¬ë¦° ë¯¸ë””ì–´ ì¡°íšŒ
        url = f"{self.base_url}/{hashtag_id}/recent_media"
        params = {
            'user_id': instagram_account_id,
            'fields': 'id,caption,media_type,media_url,permalink,timestamp,like_count,comments_count',
            'access_token': self.access_token
        }
        
        response = requests.get(url, params=params)
        data = response.json()
        
        return data.get('data', [])

# ì‚¬ìš© ì˜ˆì‹œ
instagram = InstagramCrawler(access_token='YOUR_LONG_LIVED_TOKEN')

# Instagram Account ID ì°¾ê¸° (ìˆ˜ë™: Graph API Explorerì—ì„œ í™•ì¸)
INSTAGRAM_ACCOUNT_ID = "YOUR_IG_ACCOUNT_ID"

media_list = instagram.get_user_media(INSTAGRAM_ACCOUNT_ID, limit=100)
print(f"Collected {len(media_list)} Instagram posts")
```

### 3.4 Rate Limiting ê´€ë¦¬

**Instagram API ì œí•œ:**
- ì‹œê°„ë‹¹ **200 calls**
- ì´ˆê³¼ ì‹œ: HTTP 429 (Too Many Requests)

**ëŒ€ì‘ ì „ëµ:**

```python
import time
from datetime import datetime, timedelta

class RateLimiter:
    def __init__(self, max_calls=190, time_window=3600):
        """
        Args:
            max_calls: ì‹œê°„ë‹¹ ìµœëŒ€ í˜¸ì¶œ ìˆ˜ (ì•ˆì „ ë§ˆì§„ 10)
            time_window: ì‹œê°„ ì°½ (ì´ˆ)
        """
        self.max_calls = max_calls
        self.time_window = time_window
        self.calls = []
    
    def wait_if_needed(self):
        """í•„ìš” ì‹œ ëŒ€ê¸°"""
        now = datetime.now()
        
        # ì‹œê°„ ì°½ ë°–ì˜ í˜¸ì¶œ ì œê±°
        self.calls = [c for c in self.calls if now - c < timedelta(seconds=self.time_window)]
        
        if len(self.calls) >= self.max_calls:
            # ê°€ì¥ ì˜¤ë˜ëœ í˜¸ì¶œì´ ì‹œê°„ ì°½ ë°–ìœ¼ë¡œ ë‚˜ê°ˆ ë•Œê¹Œì§€ ëŒ€ê¸°
            oldest_call = self.calls[0]
            wait_time = (oldest_call + timedelta(seconds=self.time_window) - now).total_seconds()
            
            if wait_time > 0:
                print(f"Rate limit reached. Waiting {wait_time:.0f}s...")
                time.sleep(wait_time + 1)
        
        self.calls.append(now)

# ì‚¬ìš©
limiter = RateLimiter(max_calls=190, time_window=3600)

for i in range(300):
    limiter.wait_if_needed()
    # API í˜¸ì¶œ
    print(f"Call {i+1}")
```

---

## 4. ì¹´ì¹´ì˜¤ìŠ¤í† ë¦¬ (KakaoStory)

### 4.1 ë¬¸ì œì 

**ê³µì‹ API ì—†ìŒ:**
- Kakao Developersì—ì„œ KakaoStory APIëŠ” **ì„œë¹„ìŠ¤ ì¢…ë£Œ**
- ì›¹ ìŠ¤í¬ë˜í•‘ í•„ìš”

**ë²•ì  ê³ ë ¤ì‚¬í•­:**
- ë¡œê·¸ì¸ í•„ìš” (íƒ€ì¸ ê³„ì • ìë™ ë¡œê·¸ì¸ ê¸ˆì§€)
- ë¹„ê³µê°œ ê²Œì‹œë¬¼ í¬ë¡¤ë§ ë¶ˆê°€
- ê°œì¸ì •ë³´ë³´í˜¸ë²• ì¤€ìˆ˜

### 4.2 ëŒ€ì•ˆ: ê³µê°œ ê²Œì‹œë¬¼ë§Œ ìˆ˜ì§‘ (ì œí•œì )

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
import time

class KakaoStoryCrawler:
    def __init__(self, username, password):
        """
        ì£¼ì˜: ìë™ ë¡œê·¸ì¸ì€ Kakao ì´ìš©ì•½ê´€ ìœ„ë°˜ ê°€ëŠ¥ì„±
        ì—°êµ¬ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©, ì‹¤ì œ ë°°í¬ ì‹œ ë²•ë¥  ìë¬¸ í•„ìš”
        """
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        self.driver = webdriver.Chrome(options=options)
        self.username = username
        self.password = password
    
    def login(self):
        """ì¹´ì¹´ì˜¤ ë¡œê·¸ì¸"""
        self.driver.get('https://story.kakao.com/')
        time.sleep(2)
        
        # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
        # (ì‹¤ì œ êµ¬í˜„ ì‹œ ìš”ì†Œ ì°¾ê¸° ë¡œì§ í•„ìš”)
        # ì£¼ì˜: KakaoëŠ” CAPTCHA ìˆì„ ìˆ˜ ìˆìŒ
        
        pass  # êµ¬í˜„ ìƒëµ (ë²•ì  ì´ìŠˆ)
    
    def search_public_posts(self, keyword):
        """ê³µê°œ ê²Œì‹œë¬¼ ê²€ìƒ‰ (ì œí•œì )"""
        # ì¹´ì¹´ì˜¤ìŠ¤í† ë¦¬ëŠ” ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì œí•œì 
        pass

# ê¶Œì¥í•˜ì§€ ì•ŠìŒ
```

**ëŒ€ì•ˆ:**
- íŒ€íŒŒíŠ¸ë„ˆì—ê²Œ ì§ì ‘ ê³µê°œ URL ì œê³µ ìš”ì²­
- ë˜ëŠ” ì¹´ì¹´ì˜¤ìŠ¤í† ë¦¬ ë°ì´í„° ìˆ˜ì§‘ í¬ê¸° (ë‹¤ë¥¸ 4ê°œ í”Œë«í¼ìœ¼ë¡œ ì¶©ë¶„)

---

## 5. í˜ì´ìŠ¤ë¶ (Facebook)

### 5.1 Graph API ì‚¬ìš©

**Instagramê³¼ ë™ì¼í•œ App ì‚¬ìš© ê°€ëŠ¥**

```python
class FacebookCrawler:
    def __init__(self, access_token):
        self.access_token = access_token
        self.base_url = "https://graph.facebook.com/v18.0"
    
    def get_page_posts(self, page_id, limit=100):
        """
        Facebook Pageì˜ ê²Œì‹œë¬¼ ê°€ì ¸ì˜¤ê¸°
        """
        url = f"{self.base_url}/{page_id}/posts"
        params = {
            'fields': 'id,message,created_time,permalink_url,likes.summary(true),comments.summary(true),shares',
            'access_token': self.access_token,
            'limit': limit
        }
        
        response = requests.get(url, params=params)
        data = response.json()
        
        posts = []
        for post in data.get('data', []):
            posts.append({
                'post_id': post['id'],
                'message': post.get('message', ''),
                'created_time': post['created_time'],
                'permalink': post.get('permalink_url', ''),
                'like_count': post.get('likes', {}).get('summary', {}).get('total_count', 0),
                'comment_count': post.get('comments', {}).get('summary', {}).get('total_count', 0),
                'share_count': post.get('shares', {}).get('count', 0)
            })
        
        return posts

# ì‚¬ìš©
facebook = FacebookCrawler(access_token='YOUR_ACCESS_TOKEN')
PAGE_ID = "PMInternationalKorea"  # ì˜ˆì‹œ
posts = facebook.get_page_posts(PAGE_ID, limit=100)
```

---

# III. ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ë° êµ¬ì¶•

## 1. Azure SQL Database vs Cosmos DB

### 1.1 ë¹„êµí‘œ

| ê¸°ì¤€ | Azure SQL Database | Azure Cosmos DB |
|------|-------------------|-----------------|
| **ë°ì´í„° ëª¨ë¸** | ê´€ê³„í˜• (RDBMS) | NoSQL (ë¬¸ì„œí˜•) |
| **ì¿¼ë¦¬ ì–¸ì–´** | T-SQL | SQL API, MongoDB API |
| **ìŠ¤í‚¤ë§ˆ** | ê³ ì • ìŠ¤í‚¤ë§ˆ | ìœ ì—° ìŠ¤í‚¤ë§ˆ |
| **íŠ¸ëœì­ì…˜** | ACID ë³´ì¥ | Eventually consistent |
| **í™•ì¥ì„±** | Vertical (Scale-up) | Horizontal (Scale-out) |
| **ê°€ê²©** | S1: $30/ì›” | 400 RU/s: $25/ì›” |
| **Power BI ì—°ë™** | Native | ODBC í•„ìš” |
| **ì í•©ì„±** | â­â­â­â­â­ | â­â­â­ |

### 1.2 ì„ íƒ: Azure SQL Database

**ì´ìœ :**
1. **ì •í˜• ë°ì´í„°**: SNS ê²Œì‹œë¬¼ ë°ì´í„°ëŠ” ì¼ì •í•œ êµ¬ì¡°
2. **JOIN í•„ìš”**: ì‚¬ìš©ì-ê²Œì‹œë¬¼ ê´€ê³„ ë¶„ì„
3. **Power BI ìµœì í™”**: Native connector
4. **ë¹„ìš© íš¨ìœ¨**: ì¤‘ì†Œê·œëª¨ ë°ì´í„°ì— ì í•©
5. **íŒ€ ìµìˆ™ë„**: SQL ë¬¸ë²• ë„ë¦¬ ì‚¬ìš©ë¨

---

## 2. ìŠ¤í‚¤ë§ˆ ì„¤ê³„

### 2.1 ERD (Entity-Relationship Diagram)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dim_Users     â”‚1       âˆâ”‚    fact_Posts       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ user_id (PK)    â”‚         â”‚ post_id (PK)        â”‚
â”‚ username        â”‚         â”‚ user_id (FK)        â”‚
â”‚ platform        â”‚         â”‚ platform            â”‚
â”‚ profile_url     â”‚         â”‚ post_url            â”‚
â”‚ followers_count â”‚         â”‚ title               â”‚
â”‚ posts_count     â”‚         â”‚ content_text        â”‚
â”‚ created_date    â”‚         â”‚ ocr_text            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ hashtags            â”‚
                            â”‚ published_date      â”‚
                            â”‚ like_count          â”‚
                            â”‚ comment_count       â”‚
                            â”‚ view_count          â”‚
                            â”‚ engagement_rate     â”‚
                            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚1
                                  â”‚
                                  â”‚âˆ
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚ fact_VideoTranscripts   â”‚
                      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                      â”‚ transcript_id (PK)      â”‚
                      â”‚ post_id (FK)            â”‚
                      â”‚ video_url               â”‚
                      â”‚ transcript_text         â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 í…Œì´ë¸” ì •ì˜ (SQL DDL)

```sql
-- ==============================
-- 1. dim_Users (ì‚¬ìš©ì ì°¨ì›)
-- ==============================
CREATE TABLE dim_Users (
    user_id INT IDENTITY(1,1) PRIMARY KEY,
    username NVARCHAR(100) NOT NULL,
    platform NVARCHAR(50) NOT NULL,
    profile_url NVARCHAR(500),
    followers_count INT DEFAULT 0,
    posts_count INT DEFAULT 0,
    created_date DATE DEFAULT GETDATE(),
    updated_date DATETIME DEFAULT GETDATE(),
    
    -- ë³µí•© ê³ ìœ  í‚¤ (ë™ì¼ ì‚¬ìš©ìê°€ ì—¬ëŸ¬ í”Œë«í¼ ì‚¬ìš© ê°€ëŠ¥)
    CONSTRAINT UQ_User_Platform UNIQUE (username, platform)
);

-- ì¸ë±ìŠ¤
CREATE INDEX IX_Users_Platform ON dim_Users(platform);
CREATE INDEX IX_Users_Username ON dim_Users(username);

-- ==============================
-- 2. fact_Posts (ê²Œì‹œë¬¼ íŒ©íŠ¸)
-- ==============================
CREATE TABLE fact_Posts (
    post_id BIGINT IDENTITY(1,1) PRIMARY KEY,
    user_id INT NOT NULL,
    platform NVARCHAR(50) NOT NULL,
    post_url NVARCHAR(500) NOT NULL,
    title NVARCHAR(500),
    content_text NVARCHAR(MAX),
    ocr_text NVARCHAR(MAX),
    hashtags NVARCHAR(MAX),  -- JSON ë°°ì—´: ["#í•´ì‹œíƒœê·¸1", "#í•´ì‹œíƒœê·¸2"]
    published_date DATETIME NOT NULL,
    like_count INT DEFAULT 0,
    comment_count INT DEFAULT 0,
    view_count INT DEFAULT 0,
    share_count INT DEFAULT 0,
    
    -- ê³„ì‚° í•„ë“œ (Computed Column)
    engagement_rate AS (
        CASE 
            WHEN view_count > 0 
            THEN CAST(like_count + comment_count AS FLOAT) / view_count
            ELSE 0
        END
    ) PERSISTED,
    
    created_date DATETIME DEFAULT GETDATE(),
    updated_date DATETIME DEFAULT GETDATE(),
    
    -- ì™¸ë˜ í‚¤
    CONSTRAINT FK_Posts_Users FOREIGN KEY (user_id) 
        REFERENCES dim_Users(user_id) ON DELETE CASCADE,
    
    -- ê³ ìœ  ì œì•½ (ì¤‘ë³µ ê²Œì‹œë¬¼ ë°©ì§€)
    CONSTRAINT UQ_Post_URL UNIQUE (post_url)
);

-- ì¸ë±ìŠ¤
CREATE INDEX IX_Posts_User ON fact_Posts(user_id);
CREATE INDEX IX_Posts_Platform ON fact_Posts(platform);
CREATE INDEX IX_Posts_PublishedDate ON fact_Posts(published_date DESC);
CREATE INDEX IX_Posts_Engagement ON fact_Posts(engagement_rate DESC);

-- Full-Text Search (ë³¸ë¬¸ ê²€ìƒ‰ìš©)
CREATE FULLTEXT INDEX ON fact_Posts(content_text)
    KEY INDEX PK__fact_Pos__XXXXXXXX;

-- ==============================
-- 3. fact_VideoTranscripts (ë™ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸)
-- ==============================
CREATE TABLE fact_VideoTranscripts (
    transcript_id BIGINT IDENTITY(1,1) PRIMARY KEY,
    post_id BIGINT NOT NULL,
    video_url NVARCHAR(500) NOT NULL,
    transcript_text NVARCHAR(MAX),
    created_date DATETIME DEFAULT GETDATE(),
    
    -- ì™¸ë˜ í‚¤
    CONSTRAINT FK_Transcripts_Posts FOREIGN KEY (post_id)
        REFERENCES fact_Posts(post_id) ON DELETE CASCADE
);

-- ì¸ë±ìŠ¤
CREATE INDEX IX_Transcripts_Post ON fact_VideoTranscripts(post_id);

-- ==============================
-- 4. agg_DailyMetrics (ì¼ë³„ ì§‘ê³„)
-- ==============================
CREATE TABLE agg_DailyMetrics (
    metric_id BIGINT IDENTITY(1,1) PRIMARY KEY,
    date DATE NOT NULL,
    user_id INT NOT NULL,
    platform NVARCHAR(50) NOT NULL,
    total_posts INT DEFAULT 0,
    total_likes INT DEFAULT 0,
    total_comments INT DEFAULT 0,
    total_views INT DEFAULT 0,
    avg_engagement_rate FLOAT DEFAULT 0,
    created_date DATETIME DEFAULT GETDATE(),
    
    -- ë³µí•© ê³ ìœ  í‚¤
    CONSTRAINT UQ_Daily_User_Platform UNIQUE (date, user_id, platform),
    
    -- ì™¸ë˜ í‚¤
    CONSTRAINT FK_Daily_Users FOREIGN KEY (user_id)
        REFERENCES dim_Users(user_id) ON DELETE CASCADE
);

-- ì¸ë±ìŠ¤
CREATE INDEX IX_Daily_Date ON agg_DailyMetrics(date DESC);
CREATE INDEX IX_Daily_User ON agg_DailyMetrics(user_id);

-- ==============================
-- 5. log_CrawlingJobs (í¬ë¡¤ë§ ì‘ì—… ë¡œê·¸)
-- ==============================
CREATE TABLE log_CrawlingJobs (
    job_id BIGINT IDENTITY(1,1) PRIMARY KEY,
    platform NVARCHAR(50) NOT NULL,
    start_time DATETIME NOT NULL,
    end_time DATETIME,
    status NVARCHAR(20),  -- 'Running', 'Success', 'Failed'
    posts_collected INT DEFAULT 0,
    errors_count INT DEFAULT 0,
    error_message NVARCHAR(MAX),
    created_date DATETIME DEFAULT GETDATE()
);

-- ì¸ë±ìŠ¤
CREATE INDEX IX_Log_Platform_Date ON log_CrawlingJobs(platform, start_time DESC);
```

### 2.3 ìƒ˜í”Œ ë°ì´í„° INSERT

```sql
-- ì‚¬ìš©ì ì¶”ê°€
INSERT INTO dim_Users (username, platform, profile_url, followers_count, posts_count)
VALUES 
    ('user_naver_123', 'Naver Blog', 'https://blog.naver.com/user_naver_123', 1500, 320),
    ('user_youtube_456', 'YouTube', 'https://youtube.com/@user456', 8500, 120),
    ('user_insta_789', 'Instagram', 'https://instagram.com/user789', 3200, 580);

-- ê²Œì‹œë¬¼ ì¶”ê°€
INSERT INTO fact_Posts (
    user_id, platform, post_url, title, content_text, hashtags, 
    published_date, like_count, comment_count, view_count
)
VALUES
    (1, 'Naver Blog', 'https://blog.naver.com/user_naver_123/12345', 
     'í”¼ì— ì¸í„°ë‚´ì…”ë„ FitLine Basics í•œ ë‹¬ í›„ê¸°', 
     'ì•ˆë…•í•˜ì„¸ìš”~ ì˜¤ëŠ˜ì€ ì œê°€ í•œ ë‹¬ ë™ì•ˆ ë¨¹ì–´ë³¸ í•ë¼ì¸ ë² ì´ì‹ìŠ¤ í›„ê¸°ë¥¼ ë‚¨ê²¨ìš”...',
     '["#í”¼ì— ì¸í„°ë‚´ì…”ë„", "#FitLine", "#ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ"]',
     '2024-10-15 10:30:00', 45, 12, 1200);

-- ë™ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€
INSERT INTO fact_VideoTranscripts (post_id, video_url, transcript_text)
VALUES
    (1, 'https://youtube.com/watch?v=abcd1234', 
     'ì•ˆë…•í•˜ì„¸ìš” ì—¬ëŸ¬ë¶„ ì˜¤ëŠ˜ì€ í”¼ì— ì¸í„°ë‚´ì…”ë„ ì œí’ˆì— ëŒ€í•´ ì´ì•¼ê¸°í•´ë³¼ê²Œìš”...');
```

---

## 3. ì¸ë±ìŠ¤ ì „ëµ

### 3.1 B-Tree ì¸ë±ìŠ¤

**ìš©ë„**: ë²”ìœ„ ê²€ìƒ‰, ì •ë ¬

```sql
-- ë‚ ì§œ ë²”ìœ„ ê²€ìƒ‰ (ìµœê·¼ 30ì¼ ê²Œì‹œë¬¼)
CREATE INDEX IX_Posts_PublishedDate 
ON fact_Posts(published_date DESC);

-- ì‚¬ìš©ìë³„ ê²Œì‹œë¬¼ ì¡°íšŒ
CREATE INDEX IX_Posts_UserID 
ON fact_Posts(user_id);
```

### 3.2 Filtered Index

**ìš©ë„**: íŠ¹ì • ì¡°ê±´ ë°ì´í„°ë§Œ ì¸ë±ì‹±

```sql
-- ê³ ì°¸ì—¬ìœ¨ ê²Œì‹œë¬¼ë§Œ (engagement_rate > 0.05)
CREATE INDEX IX_Posts_HighEngagement
ON fact_Posts(engagement_rate)
WHERE engagement_rate > 0.05;

-- ìµœê·¼ 6ê°œì›” ë°ì´í„°ë§Œ
CREATE INDEX IX_Posts_Recent
ON fact_Posts(published_date)
WHERE published_date >= DATEADD(MONTH, -6, GETDATE());
```

### 3.3 Full-Text Index

**ìš©ë„**: í…ìŠ¤íŠ¸ ê²€ìƒ‰ (LIKE '%í‚¤ì›Œë“œ%' ëŒ€ì‹ )

```sql
-- Full-Text Catalog ìƒì„±
CREATE FULLTEXT CATALOG ftCatalog AS DEFAULT;

-- Full-Text Index ìƒì„±
CREATE FULLTEXT INDEX ON fact_Posts(content_text, title)
    KEY INDEX PK__fact_Pos__XXXXXXXX
    ON ftCatalog;

-- ì‚¬ìš© ì˜ˆì‹œ
SELECT * FROM fact_Posts
WHERE CONTAINS(content_text, '"í”¼ì— ì¸í„°ë‚´ì…”ë„" OR "FitLine"');
```

---

## 4. ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜

### 4.1 CSV â†’ Azure SQL Database

```python
import pyodbc
import pandas as pd

class AzureSQLUploader:
    def __init__(self, server, database, username, password):
        conn_str = (
            f"DRIVER={{ODBC Driver 17 for SQL Server}};"
            f"SERVER={server};"
            f"DATABASE={database};"
            f"UID={username};"
            f"PWD={password}"
        )
        self.conn = pyodbc.connect(conn_str)
        self.cursor = self.conn.cursor()
    
    def upsert_users(self, users_df):
        """
        ì‚¬ìš©ì ë°ì´í„° ì—…ì„œíŠ¸ (ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì‚½ì…)
        """
        for _, row in users_df.iterrows():
            self.cursor.execute("""
                MERGE dim_Users AS target
                USING (VALUES (?, ?)) AS source (username, platform)
                ON target.username = source.username AND target.platform = source.platform
                WHEN MATCHED THEN
                    UPDATE SET 
                        followers_count = ?,
                        posts_count = ?,
                        updated_date = GETDATE()
                WHEN NOT MATCHED THEN
                    INSERT (username, platform, profile_url, followers_count, posts_count)
                    VALUES (?, ?, ?, ?, ?);
            """, 
                row['username'], row['platform'],
                row['followers_count'], row['posts_count'],
                row['username'], row['platform'], row['profile_url'], 
                row['followers_count'], row['posts_count']
            )
        
        self.conn.commit()
        print(f"Upserted {len(users_df)} users")
    
    def insert_posts(self, posts_df):
        """
        ê²Œì‹œë¬¼ ì‚½ì… (ì¤‘ë³µ URLì€ ë¬´ì‹œ)
        """
        inserted = 0
        for _, row in posts_df.iterrows():
            try:
                # user_id ì¡°íšŒ
                self.cursor.execute("""
                    SELECT user_id FROM dim_Users 
                    WHERE username = ? AND platform = ?
                """, row['username'], row['platform'])
                
                result = self.cursor.fetchone()
                if not result:
                    print(f"User not found: {row['username']}")
                    continue
                
                user_id = result[0]
                
                # ê²Œì‹œë¬¼ ì‚½ì…
                self.cursor.execute("""
                    INSERT INTO fact_Posts (
                        user_id, platform, post_url, title, content_text, 
                        ocr_text, hashtags, published_date, 
                        like_count, comment_count, view_count
                    )
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                    user_id, row['platform'], row['post_url'], row['title'],
                    row['content_text'], row.get('ocr_text', ''),
                    row.get('hashtags', '[]'), row['published_date'],
                    row.get('like_count', 0), row.get('comment_count', 0),
                    row.get('view_count', 0)
                )
                inserted += 1
                
            except pyodbc.IntegrityError:
                # ì¤‘ë³µ URL (UNIQUE ì œì•½ ìœ„ë°˜)
                continue
        
        self.conn.commit()
        print(f"Inserted {inserted} posts")
    
    def close(self):
        self.cursor.close()
        self.conn.close()

# ì‚¬ìš© ì˜ˆì‹œ
uploader = AzureSQLUploader(
    server='your-server.database.windows.net',
    database='PMI_SNS_DB',
    username='admin',
    password='your_password'
)

# CSV ì½ê¸°
df_users = pd.read_csv('users.csv')
df_posts = pd.read_csv('posts.csv')

# ì—…ë¡œë“œ
uploader.upsert_users(df_users)
uploader.insert_posts(df_posts)

uploader.close()
```

---

# IV. OCR ë° ë™ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸ ì²˜ë¦¬

## 1. ì´ë¯¸ì§€ OCR ì „ëµ

### 1.1 ë„êµ¬ ë¹„êµ (ì¬ì •ë¦¬)

| ë„êµ¬ | ë¹„ìš© | ë¬´ë£Œ í•œë„ | í•œê¸€ ì •í™•ë„ | ì„¤ì • ë‚œì´ë„ | **ì¶”ì²œ ë‹¨ê³„** |
|------|------|-----------|------------|-----------|-------------|
| **EasyOCR** | ë¬´ë£Œ | ë¬´ì œí•œ | 85% | â­ | POC |
| **Azure CV** | ìœ ë£Œ | 5K/ì›” | 95% | â­â­ | Pilot |
| **Google Vision** | ìœ ë£Œ | 1K/ì›” | 95% | â­â­ | - |
| **Naver Clova** | ìœ ë£Œ | ì—†ìŒ | 98% | â­â­â­ | Production |

### 1.2 ë‹¨ê³„ë³„ ì „ëµ

**Phase 1: POC (ë¹„ìš© $0/ì›”)**
- **ë„êµ¬**: EasyOCR
- **ìƒ˜í”Œ**: 1,000ê°œ ì´ë¯¸ì§€
- **ëª©ì **: ì •í™•ë„ ê²€ì¦

**Phase 2: Pilot (ë¹„ìš© $5/ì›”)**
- **ë„êµ¬**: Azure Computer Vision (5,000ê±´ ë¬´ë£Œ í‹°ì–´)
- **ê·œëª¨**: 10,000ê°œ ì´ë¯¸ì§€
- **ëª©ì **: ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

**Phase 3: Production (ë¹„ìš© $100-200/ì›”)**
- **ë„êµ¬**: Azure CV (ê¸°ë³¸) + Naver Clova (í•œê¸€ íŠ¹í™”)
- **ê·œëª¨**: 100,000+ ì´ë¯¸ì§€
- **ëª©ì **: ì•ˆì •ì  ìš´ì˜

### 1.3 êµ¬í˜„ ì½”ë“œ (Azure Computer Vision)

```python
from azure.cognitiveservices.vision.computervision import ComputerVisionClient
from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes
from msrest.authentication import CognitiveServicesCredentials
import time

class AzureOCR:
    def __init__(self, endpoint, subscription_key):
        self.client = ComputerVisionClient(
            endpoint, 
            CognitiveServicesCredentials(subscription_key)
        )
    
    def extract_text_from_url(self, image_url):
        """
        ì´ë¯¸ì§€ URLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Azure Computer Vision API ì‚¬ìš©
        """
        try:
            # OCR ë¹„ë™ê¸° ìš”ì²­
            read_response = self.client.read(image_url, raw=True)
            
            # Operation ID ì¶”ì¶œ
            operation_location = read_response.headers["Operation-Location"]
            operation_id = operation_location.split("/")[-1]
            
            # ê²°ê³¼ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
            for _ in range(30):
                result = self.client.get_read_result(operation_id)
                if result.status not in [OperationStatusCodes.running, OperationStatusCodes.not_started]:
                    break
                time.sleep(1)
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            texts = []
            if result.status == OperationStatusCodes.succeeded:
                for page in result.analyze_result.read_results:
                    for line in page.lines:
                        texts.append(line.text)
            
            return '\n'.join(texts)
            
        except Exception as e:
            print(f"Azure OCR error for {image_url}: {e}")
            return ""

# ì‚¬ìš© ì˜ˆì‹œ
azure_ocr = AzureOCR(
    endpoint='https://your-resource.cognitiveservices.azure.com/',
    subscription_key='YOUR_SUBSCRIPTION_KEY'
)

for post in posts_list:
    for img_url in post['images'][:5]:
        ocr_text = azure_ocr.extract_text_from_url(img_url)
        post['ocr_results'][img_url] = ocr_text
```

---

## 2. ë™ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œ

### 2.1 ë°©ë²• ë¹„êµ

| ë°©ë²• | ë¹„ìš© | ì •í™•ë„ | ì†ë„ | **ì¶”ì²œ** |
|------|------|--------|------|---------|
| YouTube ìë§‰ ë‹¤ìš´ë¡œë“œ | ë¬´ë£Œ | 85-90% | ë¹ ë¦„ | â­â­â­â­â­ |
| Whisper (Self-host) | ë¬´ë£Œ | 95%+ | ëŠë¦¼ | â­â­â­â­â­ |
| Google Speech-to-Text | ìœ ë£Œ (60ë¶„/ì›” ë¬´ë£Œ) | 95% | ë¹ ë¦„ | â­â­â­â­ |
| Azure Speech Service | ìœ ë£Œ (5ì‹œê°„/ì›” ë¬´ë£Œ) | 95% | ë¹ ë¦„ | â­â­â­â­ |

### 2.2 Whisper êµ¬í˜„ (Self-hosted)

```python
import whisper
import yt_dlp
import os

class WhisperTranscriber:
    def __init__(self, model_size='base'):
        """
        Args:
            model_size: 'tiny', 'base', 'small', 'medium', 'large'
        """
        print(f"Loading Whisper {model_size} model...")
        self.model = whisper.load_model(model_size)
        print("Whisper ready!")
    
    def transcribe_youtube_video(self, youtube_url):
        """
        YouTube ì˜ìƒì—ì„œ ìŒì„± ì¶”ì¶œ â†’ í…ìŠ¤íŠ¸ ë³€í™˜
        """
        # Step 1: YouTube ì˜ìƒ ë‹¤ìš´ë¡œë“œ (ì˜¤ë””ì˜¤ë§Œ)
        ydl_opts = {
            'format': 'bestaudio/best',
            'postprocessors': [{
                'key': 'FFmpegExtractAudio',
                'preferredcodec': 'mp3',
                'preferredquality': '192',
            }],
            'outtmpl': 'temp_audio.%(ext)s',
        }
        
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([youtube_url])
        
        # Step 2: Whisperë¡œ ìŒì„± ì¸ì‹
        audio_file = 'temp_audio.mp3'
        result = self.model.transcribe(audio_file, language='ko')
        
        # Step 3: ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(audio_file)
        
        return result['text']

# ì‚¬ìš© ì˜ˆì‹œ
transcriber = WhisperTranscriber(model_size='base')

for post in posts_with_videos:
    for video_url in post['videos']:
        if 'youtube' in video_url:
            transcript = transcriber.transcribe_youtube_video(video_url)
            post['video_transcripts'][video_url] = transcript
            print(f"Transcribed: {video_url}")
```

**Whisper ëª¨ë¸ í¬ê¸° ì„ íƒ:**
- **tiny**: ë¹ ë¦„ (1ë¶„ ì˜ìƒ = 5ì´ˆ), ì •í™•ë„ ë‚®ìŒ (80%)
- **base**: ì ë‹¹ (1ë¶„ = 10ì´ˆ), ì •í™•ë„ 85%
- **small**: ëŠë¦¼ (1ë¶„ = 30ì´ˆ), ì •í™•ë„ 90%
- **medium**: ë§¤ìš° ëŠë¦¼ (1ë¶„ = 2ë¶„), ì •í™•ë„ 95%

**ê¶Œì¥**: **base** ëª¨ë¸ (ì •í™•ë„-ì†ë„ ê· í˜•)

---

# V. ìë™í™” íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

## 1. Azure Data Factory ì„¤ê³„

### 1.1 íŒŒì´í”„ë¼ì¸ êµ¬ì¡°

```
Pipeline: Daily_SNS_Data_Collection
â”œâ”€ Activity 1: Execute Python Script (Crawlers)
â”‚   â”œâ”€ Naver Blog Crawler
â”‚   â”œâ”€ YouTube Crawler
â”‚   â”œâ”€ Instagram Crawler
â”‚   â””â”€ Facebook Crawler
â”‚
â”œâ”€ Activity 2: Copy to Blob Storage
â”‚   â””â”€ /raw/{platform}/{date}/posts.json
â”‚
â”œâ”€ Activity 3: Data Flow (Transform)
â”‚   â”œâ”€ Parse JSON
â”‚   â”œâ”€ Clean Text
â”‚   â”œâ”€ Extract Hashtags
â”‚   â”œâ”€ OCR Images (Azure CV)
â”‚   â””â”€ Filter by Target Hashtags
â”‚
â”œâ”€ Activity 4: Copy to SQL Database
â”‚   â”œâ”€ Staging Tables (stg_Users, stg_Posts)
â”‚   â””â”€ MERGE INTO Production Tables
â”‚
â””â”€ Activity 5: Stored Procedure (Aggregation)
    â””â”€ EXEC sp_AggregateDailyMetrics
```

### 1.2 Activity ìƒì„¸ ì„¤ì •

**Activity 1: Execute Python Script**

Azure VMì—ì„œ Python í¬ë¡¤ëŸ¬ ì‹¤í–‰:

```bash
# Azure VMì— SSH ì ‘ì†
ssh azureuser@your-vm-ip

# í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
cd /home/azureuser/pmi-sns-crawler
python3 main.py --platform all --days 1

# ê²°ê³¼ ì—…ë¡œë“œ
az storage blob upload-batch \
    --destination /raw/naver/$(date +%Y-%m-%d)/ \
    --source ./output/ \
    --account-name yourstorageaccount
```

**Activity 2: Copy to Blob Storage**

Data Factoryì˜ Copy Activity ì‚¬ìš©:
- Source: Azure VM (SFTP)
- Sink: Azure Blob Storage
- Path: `/raw/{platform}/{date}/posts.json`

**Activity 3: Data Flow (Transform)**

```
Source: Blob Storage JSON files
â”‚
â”œâ”€ Derived Column: Extract hashtags (regex)
â”œâ”€ Filter: WHERE hashtags CONTAINS TARGET_HASHTAGS
â”œâ”€ Lookup: Call Azure CV API for OCR
â”œâ”€ Select: Choose columns
â”‚
Sink: Staging Tables (stg_Users, stg_Posts)
```

**Activity 4: Copy to SQL Database**

```sql
-- MERGE ë¬¸ìœ¼ë¡œ Upsert
MERGE fact_Posts AS target
USING stg_Posts AS source
ON target.post_url = source.post_url
WHEN MATCHED THEN
    UPDATE SET 
        like_count = source.like_count,
        comment_count = source.comment_count,
        updated_date = GETDATE()
WHEN NOT MATCHED THEN
    INSERT (user_id, platform, post_url, title, content_text, ...)
    VALUES (source.user_id, source.platform, ...);
```

**Activity 5: Stored Procedure**

```sql
CREATE PROCEDURE sp_AggregateDailyMetrics
    @target_date DATE
AS
BEGIN
    -- ì¼ë³„ ì§‘ê³„ ê³„ì‚°
    INSERT INTO agg_DailyMetrics (date, user_id, platform, total_posts, ...)
    SELECT 
        @target_date AS date,
        user_id,
        platform,
        COUNT(*) AS total_posts,
        SUM(like_count) AS total_likes,
        SUM(comment_count) AS total_comments,
        SUM(view_count) AS total_views,
        AVG(engagement_rate) AS avg_engagement_rate
    FROM fact_Posts
    WHERE CAST(published_date AS DATE) = @target_date
    GROUP BY user_id, platform;
END
```

### 1.3 ìŠ¤ì¼€ì¤„ë§

**Trigger ì„¤ì •:**
- **Type**: Schedule Trigger
- **Frequency**: Daily
- **Time**: 02:00 (KST)
- **Days**: All days

**ì˜ì¡´ì„± ì²´ì¸:**
```
02:00 â†’ Python Crawlers (60ë¶„)
03:00 â†’ Blob Upload (5ë¶„)
03:05 â†’ Data Flow Transform (30ë¶„)
03:35 â†’ SQL Load (10ë¶„)
03:45 â†’ Aggregation (5ë¶„)
03:50 â†’ Pipeline Complete
```

---

## 2. ETL í”„ë¡œì„¸ìŠ¤

### 2.1 Extract (ì¶”ì¶œ)

**Python í¬ë¡¤ëŸ¬ ì‹¤í–‰:**

```python
# main.py
import argparse
from crawlers import NaverCrawler, YouTubeCrawler, InstagramCrawler
from datetime import datetime, timedelta
import json

def main(platform, days_back):
    # ë‚ ì§œ ë²”ìœ„
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days_back)
    
    results = []
    
    if platform in ['all', 'naver']:
        naver = NaverCrawler()
        results.extend(naver.crawl(start_date, end_date))
    
    if platform in ['all', 'youtube']:
        youtube = YouTubeCrawler()
        results.extend(youtube.crawl(start_date, end_date))
    
    # ... ë‹¤ë¥¸ í”Œë«í¼
    
    # JSON ì €ì¥
    output_file = f'output/{platform}_{datetime.now().strftime("%Y%m%d")}.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"Saved {len(results)} posts to {output_file}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--platform', choices=['all', 'naver', 'youtube', 'instagram', 'facebook'], required=True)
    parser.add_argument('--days', type=int, default=1)
    args = parser.parse_args()
    
    main(args.platform, args.days)
```

### 2.2 Transform (ë³€í™˜)

**Data Flow ë¡œì§ (pseudo-code):**

```python
# Azure Data Flow Transformation
def transform_posts(raw_data):
    # 1. JSON íŒŒì‹±
    df = parse_json(raw_data)
    
    # 2. í•´ì‹œíƒœê·¸ ì¶”ì¶œ
    df['hashtags'] = df['content_text'].apply(extract_hashtags)
    
    # 3. íƒ€ê²Ÿ í•´ì‹œíƒœê·¸ í•„í„°ë§
    TARGET_HASHTAGS = ['#í”¼ì— ì¸í„°ë‚´ì…”ë„', '#FitLine', ...]
    df = df[df['hashtags'].apply(lambda tags: any(t in tags for t in TARGET_HASHTAGS))]
    
    # 4. OCR (Azure CV API í˜¸ì¶œ)
    for idx, row in df.iterrows():
        ocr_texts = []
        for img_url in row['images']:
            ocr_text = call_azure_cv_api(img_url)
            ocr_texts.append(ocr_text)
        df.at[idx, 'ocr_text'] = '\n'.join(ocr_texts)
    
    # 5. NULL ì²˜ë¦¬
    df['like_count'].fillna(0, inplace=True)
    df['comment_count'].fillna(0, inplace=True)
    
    # 6. ë‚ ì§œ í˜•ì‹ í†µì¼
    df['published_date'] = pd.to_datetime(df['published_date'])
    
    return df
```

### 2.3 Load (ì ì¬)

**Bulk Insert with pyodbc:**

```python
def bulk_insert_posts(df, connection):
    """
    ëŒ€ëŸ‰ INSERT (Fast Executemany)
    """
    cursor = connection.cursor()
    cursor.fast_executemany = True
    
    # ì‚¬ìš©ì ID ì¡°íšŒ (ë¯¸ë¦¬ ìºì‹±)
    user_map = get_user_id_map(connection)
    
    # INSERT ì¤€ë¹„
    insert_sql = """
        INSERT INTO fact_Posts (
            user_id, platform, post_url, title, content_text,
            ocr_text, hashtags, published_date,
            like_count, comment_count, view_count
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """
    
    # ë°ì´í„° ë³€í™˜
    rows = []
    for _, post in df.iterrows():
        user_id = user_map.get((post['username'], post['platform']))
        if not user_id:
            continue
        
        rows.append((
            user_id, post['platform'], post['post_url'], post['title'],
            post['content_text'], post['ocr_text'],
            json.dumps(post['hashtags']), post['published_date'],
            post['like_count'], post['comment_count'], post['view_count']
        ))
    
    # Bulk INSERT
    cursor.executemany(insert_sql, rows)
    connection.commit()
    
    print(f"Inserted {len(rows)} posts")
```

---

## 3. ìŠ¤ì¼€ì¤„ë§ ë° ëª¨ë‹ˆí„°ë§

### 3.1 ì—ëŸ¬ í•¸ë“¤ë§

**Retry ë¡œì§:**

```python
import time
from functools import wraps

def retry(max_attempts=3, delay=5):
    """
    ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë°ì½”ë ˆì´í„°
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    print(f"Attempt {attempt + 1} failed: {e}")
                    if attempt < max_attempts - 1:
                        print(f"Retrying in {delay}s...")
                        time.sleep(delay)
                    else:
                        print("Max attempts reached. Giving up.")
                        raise
        return wrapper
    return decorator

# ì‚¬ìš© ì˜ˆì‹œ
@retry(max_attempts=3, delay=10)
def crawl_naver_blog(url):
    # í¬ë¡¤ë§ ë¡œì§
    pass
```

**ë¡œê·¸ ì €ì¥:**

```python
import logging
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    filename=f'logs/crawler_{datetime.now().strftime("%Y%m%d")}.log',
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

# ì‚¬ìš©
logger.info("Starting Naver crawler...")
logger.error(f"Failed to crawl {url}: {error}")
```

### 3.2 ì•Œë¦¼ (Email/Slack)

**Azure Data Factory ì•Œë¦¼:**

```json
{
  "pipeline": "Daily_SNS_Data_Collection",
  "trigger": {
    "type": "Schedule",
    "schedule": "0 2 * * *"
  },
  "activities": [
    {
      "name": "Send Email on Failure",
      "type": "WebActivity",
      "dependsOn": ["All Activities"],
      "onFailure": {
        "action": "SendEmail",
        "to": "data-team@pm-international.com",
        "subject": "Pipeline Failed",
        "body": "@{activity('crawl').error}"
      }
    }
  ]
}
```

---

# VI. ë°ì´í„° ë¶„ì„ ë° ëª¨ë¸ë§

## 1. íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)

### 1.1 ê¸°ë³¸ í†µê³„

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ë°ì´í„° ë¡œë“œ
df = pd.read_sql("SELECT * FROM fact_Posts", connection)

# ê¸°ë³¸ ì •ë³´
print(df.info())
print(df.describe())

# í”Œë«í¼ë³„ ê²Œì‹œë¬¼ ìˆ˜
print(df['platform'].value_counts())

# ì¼ë³„ ê²Œì‹œë¬¼ ì¶”ì´
df['date'] = pd.to_datetime(df['published_date']).dt.date
daily_posts = df.groupby('date').size()
daily_posts.plot(title='Daily Posts Trend', figsize=(12,6))
plt.show()

# ì°¸ì—¬ìœ¨ ë¶„í¬
sns.histplot(df['engagement_rate'].dropna(), bins=50)
plt.title('Engagement Rate Distribution')
plt.show()
```

### 1.2 ìƒê´€ê´€ê³„ ë¶„ì„

```python
# ìˆ«ìí˜• ë³€ìˆ˜ë§Œ
numeric_cols = ['like_count', 'comment_count', 'view_count', 'engagement_rate', 'followers_count']
corr_matrix = df[numeric_cols].corr()

# Heatmap
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()
```

---

## 2. í´ëŸ¬ìŠ¤í„°ë§ (K-Means)

### 2.1 í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§

```python
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import numpy as np

# ì‚¬ìš©ìë³„ ì§‘ê³„ í”¼ì²˜
user_features = df.groupby('user_id').agg({
    'post_id': 'count',  # ì´ ê²Œì‹œë¬¼ ìˆ˜
    'like_count': 'sum',
    'comment_count': 'sum',
    'engagement_rate': 'mean',
    'platform': lambda x: x.nunique()  # í™œë™ í”Œë«í¼ ìˆ˜
}).rename(columns={
    'post_id': 'total_posts',
    'like_count': 'total_likes',
    'comment_count': 'total_comments',
    'engagement_rate': 'avg_engagement',
    'platform': 'platform_count'
})

# ì£¼ê°„ ê²Œì‹œ ë¹ˆë„ ê³„ì‚°
df['week'] = pd.to_datetime(df['published_date']).dt.isocalendar().week
posts_per_week = df.groupby(['user_id', 'week']).size().groupby('user_id').mean()
user_features['posts_per_week'] = posts_per_week

# í•´ì‹œíƒœê·¸ ë‹¤ì–‘ì„±
def hashtag_diversity(hashtags_series):
    all_tags = []
    for tags_json in hashtags_series:
        try:
            tags = json.loads(tags_json)
            all_tags.extend(tags)
        except:
            pass
    if len(all_tags) == 0:
        return 0
    return len(set(all_tags)) / len(all_tags)

hashtag_div = df.groupby('user_id')['hashtags'].apply(hashtag_diversity)
user_features['hashtag_diversity'] = hashtag_div

print(user_features.head())
```

### 2.2 ìµœì  K ì°¾ê¸° (Elbow + Silhouette)

```python
# ì •ê·œí™”
scaler = StandardScaler()
X_scaled = scaler.fit_transform(user_features)

# Elbow Method
inertias = []
silhouettes = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)
    silhouettes.append(silhouette_score(X_scaled, kmeans.labels_))

# ì‹œê°í™”
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))

ax1.plot(K_range, inertias, 'bo-')
ax1.set_xlabel('Number of Clusters (k)')
ax1.set_ylabel('Inertia')
ax1.set_title('Elbow Method')

ax2.plot(K_range, silhouettes, 'ro-')
ax2.set_xlabel('Number of Clusters (k)')
ax2.set_ylabel('Silhouette Score')
ax2.set_title('Silhouette Analysis')

plt.show()

# ìµœì  K ì„ íƒ (ì˜ˆ: k=4)
optimal_k = 4
print(f"Optimal K: {optimal_k}")
```

### 2.3 í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ ë° í”„ë¡œíŒŒì¼ë§

```python
# ìµœì  Kë¡œ í´ëŸ¬ìŠ¤í„°ë§
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
user_features['cluster'] = kmeans.fit_predict(X_scaled)

# í´ëŸ¬ìŠ¤í„°ë³„ í”„ë¡œíŒŒì¼
for i in range(optimal_k):
    print(f"\n===== Cluster {i} =====")
    cluster_data = user_features[user_features['cluster'] == i]
    print(f"Size: {len(cluster_data)} users ({len(cluster_data)/len(user_features)*100:.1f}%)")
    print(cluster_data.describe())

# í´ëŸ¬ìŠ¤í„° ëª…ëª…
cluster_names = {
    0: "Super Engagers",
    1: "Steady Contributors",
    2: "Casual Sharers",
    3: "Dormant"
}

user_features['cluster_name'] = user_features['cluster'].map(cluster_names)

# ì‹œê°í™” (PCA 2D)
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(10,8))
for i, name in cluster_names.items():
    mask = user_features['cluster'] == i
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], label=name, s=50, alpha=0.6)

plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('User Clusters (PCA)')
plt.legend()
plt.show()
```

---

## 3. ê°ì„± ë¶„ì„ (Sentiment Analysis)

### 3.1 KoBERT ì‚¬ìš©

```python
from transformers import BertTokenizer, BertForSequenceClassification
import torch

# KoBERT ëª¨ë¸ ë¡œë“œ
tokenizer = BertTokenizer.from_pretrained('monologg/kobert')
model = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=3)

def sentiment_analysis(text):
    """
    ê°ì„± ë¶„ì„: 0=ë¶€ì •, 1=ì¤‘ë¦½, 2=ê¸ì •
    """
    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
    outputs = model(**inputs)
    probs = torch.softmax(outputs.logits, dim=1)
    sentiment = torch.argmax(probs).item()
    confidence = probs[0][sentiment].item()
    
    return {
        'sentiment': sentiment,
        'label': ['negative', 'neutral', 'positive'][sentiment],
        'confidence': confidence
    }

# ì „ì²´ ê²Œì‹œë¬¼ì— ì ìš© (ìƒ˜í”Œë§)
sample_posts = df.sample(min(1000, len(df)))

sentiments = []
for _, post in sample_posts.iterrows():
    result = sentiment_analysis(post['content_text'][:500])  # ì²˜ìŒ 500ìë§Œ
    sentiments.append(result)

df_sample = sample_posts.copy()
df_sample['sentiment'] = [s['sentiment'] for s in sentiments]
df_sample['sentiment_label'] = [s['label'] for s in sentiments]
df_sample['sentiment_confidence'] = [s['confidence'] for s in sentiments]

# ê°ì„± ë¶„í¬
print(df_sample['sentiment_label'].value_counts())

# ê°ì„±ê³¼ ì°¸ì—¬ìœ¨ì˜ ê´€ê³„
sns.boxplot(data=df_sample, x='sentiment_label', y='engagement_rate')
plt.title('Engagement Rate by Sentiment')
plt.show()
```

---

## 4. íšŒê·€ ë¶„ì„ (Regression)

### 4.1 ëª¨ë¸ êµ¬ì¶• (statsmodels)

```python
import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler

# ì¢…ì†ë³€ìˆ˜: ì›” ë§¤ì¶œ (ê°€ìƒ ë°ì´í„°, ì‹¤ì œë¡œëŠ” PMI ë‚´ë¶€ ë°ì´í„° ì—°ë™ í•„ìš”)
# ì—¬ê¸°ì„œëŠ” engagement_rateë¥¼ ëŒ€ë¦¬ ë³€ìˆ˜ë¡œ ì‚¬ìš©
y = user_features['avg_engagement']

# ë…ë¦½ë³€ìˆ˜
X = user_features[[
    'posts_per_week',
    'total_likes',
    'total_comments',
    'platform_count',
    'hashtag_diversity'
]]

# ì •ê·œí™”
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = sm.add_constant(X_scaled)  # ìƒìˆ˜í•­ ì¶”ê°€

# OLS íšŒê·€
model = sm.OLS(y, X_scaled)
results = model.fit()

# ê²°ê³¼ ì¶œë ¥
print(results.summary())

# í•´ì„
print("\n===== í•´ì„ =====")
print(f"R-squared: {results.rsquared:.3f}")
print(f"Adj. R-squared: {results.rsquared_adj:.3f}")
print(f"F-statistic p-value: {results.f_pvalue:.4f}")

for i, col in enumerate(['const'] + list(X.columns)):
    coef = results.params[i]
    pval = results.pvalues[i]
    sig = '***' if pval < 0.001 else ('**' if pval < 0.01 else ('*' if pval < 0.05 else ''))
    print(f"{col}: Î² = {coef:.4f}, p = {pval:.4f} {sig}")
```

### 4.2 ìœ ì˜ì„± ê²€ì¦

**ê·€ë¬´ê°€ì„¤ (H0)**: SNS í™œë™ ì§€í‘œëŠ” ì„±ê³¼ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤ (Î² = 0)  
**ëŒ€ë¦½ê°€ì„¤ (H1)**: SNS í™œë™ ì§€í‘œëŠ” ì„±ê³¼ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤ (Î² â‰  0)

**ìœ ì˜ìˆ˜ì¤€**: Î± = 0.05

**ê²°ê³¼ í•´ì„ ì˜ˆì‹œ:**
```
posts_per_week: Î² = 0.125, p = 0.0001 ***
â†’ p < 0.05ì´ë¯€ë¡œ ê·€ë¬´ê°€ì„¤ ê¸°ê°
â†’ ì£¼ê°„ ê²Œì‹œ ë¹ˆë„ëŠ” ì°¸ì—¬ìœ¨ì— ìœ ì˜í•œ ì˜í–¥ì„ ë¯¸ì¹¨
â†’ ì£¼ 1íšŒ ê²Œì‹œ ì¦ê°€ ì‹œ ì°¸ì—¬ìœ¨ 12.5%p ì¦ê°€

platform_count: Î² = 0.087, p = 0.023 *
â†’ ë‹¤ì¤‘ í”Œë«í¼ í™œìš© ì‹œ ì°¸ì—¬ìœ¨ ì¦ê°€

hashtag_diversity: Î² = 0.032, p = 0.421
â†’ p > 0.05ì´ë¯€ë¡œ ê·€ë¬´ê°€ì„¤ ì±„íƒ ë¶ˆê°€
â†’ í•´ì‹œíƒœê·¸ ë‹¤ì–‘ì„±ì€ ìœ ì˜í•œ ì˜í–¥ ì—†ìŒ
```

---

# VII. ì‹œê°í™” ë° ëŒ€ì‹œë³´ë“œ

## 1. Power BI ì—°ë™

### 1.1 ì—°ê²° ì„¤ì •

**Power BI Desktop:**
1. "Get Data" â†’ "Azure SQL Database"
2. Server: `your-server.database.windows.net`
3. Database: `PMI_SNS_DB`
4. Authentication: Database (username/password)
5. Select Tables: `dim_Users`, `fact_Posts`, `agg_DailyMetrics`

### 1.2 ë°ì´í„° ëª¨ë¸ë§

**ê´€ê³„ ì„¤ì •:**
```
dim_Users (user_id) 1 -------- âˆ fact_Posts (user_id)
fact_Posts (post_id) 1 -------- âˆ fact_VideoTranscripts (post_id)
dim_Users (user_id) 1 -------- âˆ agg_DailyMetrics (user_id)
```

**ê³„ì‚° í•„ë“œ (DAX):**

```dax
// ì´ ê²Œì‹œë¬¼ ìˆ˜
Total Posts = COUNTROWS(fact_Posts)

// í‰ê·  ì°¸ì—¬ìœ¨
Avg Engagement = AVERAGE(fact_Posts[engagement_rate])

// ì›”ê°„ ê²Œì‹œë¬¼ ì¦ê°€ìœ¨
MoM Growth = 
VAR CurrentMonth = CALCULATE([Total Posts], DATESMTD(fact_Posts[published_date]))
VAR PreviousMonth = CALCULATE([Total Posts], DATEADD(fact_Posts[published_date], -1, MONTH))
RETURN DIVIDE(CurrentMonth - PreviousMonth, PreviousMonth, 0)

// í´ëŸ¬ìŠ¤í„°ë³„ ì‚¬ìš©ì ìˆ˜ (ì‚¬ì „ì— í´ëŸ¬ìŠ¤í„° ì •ë³´ë¥¼ dim_Usersì— ì¶”ê°€)
Users by Cluster = COUNTROWS(FILTER(dim_Users, dim_Users[cluster] = "Super Engagers"))
```

---

## 2. í•µì‹¬ KPI ì •ì˜

### 2.1 KPI ë¦¬ìŠ¤íŠ¸

| KPI | ì •ì˜ | ëª©í‘œê°’ | ëŒ€ì‹œë³´ë“œ ì‹œê°í™” |
|-----|------|--------|----------------|
| **ì¼ì¼ ê²Œì‹œë¬¼ ìˆ˜** | ì¼ì¼ ìˆ˜ì§‘ ê²Œì‹œë¬¼ ìˆ˜ | 500+ | ì„  ê·¸ë˜í”„ (ì¶”ì„¸) |
| **í‰ê·  ì°¸ì—¬ìœ¨** | (ì¢‹ì•„ìš”+ëŒ“ê¸€)/ì¡°íšŒìˆ˜ | 5% | ê²Œì´ì§€ ì°¨íŠ¸ |
| **í”Œë«í¼ ë¹„ì¤‘** | í”Œë«í¼ë³„ ê²Œì‹œë¬¼ % | Balanced | íŒŒì´ ì°¨íŠ¸ |
| **ê³ ì°¸ì—¬ìœ¨ ê²Œì‹œë¬¼ ë¹„ìœ¨** | engagement > 10% | 15% | ë„ë„› ì°¨íŠ¸ |
| **í´ëŸ¬ìŠ¤í„° ë¶„í¬** | ê° í´ëŸ¬ìŠ¤í„° ì‚¬ìš©ì ìˆ˜ | - | ë§‰ëŒ€ ê·¸ë˜í”„ |
| **ê°ì„± ìŠ¤ì½”ì–´** | ê¸ì • ê²Œì‹œë¬¼ % | 80% | ëˆ„ì  ë§‰ëŒ€ |
| **ë¦¬ìŠ¤í¬ ê²Œì‹œë¬¼** | ë¶€ì • ë˜ëŠ” ê³¼ì¥ ì˜ì‹¬ | <5% | ê²½ê³  ì¹´ë“œ |

---

## 3. ëŒ€ì‹œë³´ë“œ ì„¤ê³„

### 3.1 Page 1: Overview (ê°œìš”)

**ë ˆì´ì•„ì›ƒ:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“Š PM-International Korea SNS í™œë™ ëŒ€ì‹œë³´ë“œ           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ì´ ê²Œì‹œë¬¼ â”‚ í‰ê·  ì°¸ì—¬ â”‚ í™œë™ ì‚¬ìš©ìâ”‚ ë¦¬ìŠ¤í¬   â”‚ ìµœì‹  ì—…ë°ì´íŠ¸â”‚
â”‚ 15,234   â”‚  5.8%   â”‚  1,850  â”‚   23    â”‚ 2024-10-27â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  [ì¼ì¼ ê²Œì‹œë¬¼ ì¶”ì„¸ - ì„  ê·¸ë˜í”„]                         â”‚
â”‚                                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ í”Œë«í¼ë³„ ê²Œì‹œë¬¼ (íŒŒì´)  â”‚  í´ëŸ¬ìŠ¤í„° ë¶„í¬ (ë§‰ëŒ€)          â”‚
â”‚                       â”‚                              â”‚
â”‚  Naver: 40%          â”‚  Super Engagers: 15%         â”‚
â”‚  YouTube: 25%        â”‚  Steady: 35%                 â”‚
â”‚  Instagram: 20%      â”‚  Casual: 40%                 â”‚
â”‚  ...                 â”‚  Dormant: 10%                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Page 2: Platform Analysis (í”Œë«í¼ ë¶„ì„)

**ìŠ¬ë¼ì´ì„œ:**
- í”Œë«í¼ ì„ íƒ: [All, Naver, YouTube, Instagram, KakaoStory, Facebook]
- ë‚ ì§œ ë²”ìœ„: [ìµœê·¼ 7ì¼, 30ì¼, 90ì¼, All]

**ì°¨íŠ¸:**
1. í”Œë«í¼ë³„ í‰ê·  ì°¸ì—¬ìœ¨ (ë§‰ëŒ€ ê·¸ë˜í”„)
2. í”Œë«í¼ë³„ ê²Œì‹œë¬¼ ìˆ˜ ì¶”ì´ (ì„  ê·¸ë˜í”„)
3. í”Œë«í¼ë³„ ìƒìœ„ í•´ì‹œíƒœê·¸ (ì›Œë“œ í´ë¼ìš°ë“œ)

### 3.3 Page 3: User Clusters (ì‚¬ìš©ì í´ëŸ¬ìŠ¤í„°)

**í…Œì´ë¸”:**
| í´ëŸ¬ìŠ¤í„° | ì‚¬ìš©ì ìˆ˜ | ì£¼ê°„ ê²Œì‹œ | í‰ê·  ì°¸ì—¬ìœ¨ | ëŒ€í‘œ ì‚¬ìš©ì |
|----------|-----------|-----------|------------|------------|
| Super Engagers | 278 | 8.2 | 12.4% | @user123 |
| Steady | 648 | 3.1 | 6.2% | @user456 |
| ... | ... | ... | ... | ... |

**ì¸ì‚¬ì´íŠ¸ ì¹´ë“œ:**
- "Super EngagersëŠ” Steady ëŒ€ë¹„ ì°¸ì—¬ìœ¨ 2ë°° ë†’ìŒ"
- "Dormant ì‚¬ìš©ì ì¤‘ 35%ê°€ ìµœê·¼ í•œ ë‹¬ê°„ í™œë™ ì—†ìŒ"

### 3.4 Page 4: Risk Monitoring (ë¦¬ìŠ¤í¬ ëª¨ë‹ˆí„°ë§)

**í•„í„°:**
- ê°ì„±: [ë¶€ì •, ì¤‘ë¦½, ê¸ì •]
- ë¦¬ìŠ¤í¬ ë ˆë²¨: [ë‚®ìŒ, ì¤‘ê°„, ë†’ìŒ]

**í…Œì´ë¸”:**
| ê²Œì‹œë¬¼ URL | ì‘ì„±ì | ê°ì„± | ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´ | í‚¤ì›Œë“œ |
|-----------|--------|------|-------------|--------|
| blog.naver.com/... | user789 | ë¶€ì • | 85 | "ì‚¬ê¸°", "í™˜ë¶ˆ" |
| youtube.com/... | user234 | ê¸ì • | 92 | "ì›” 1000ë§Œì›" |

**ì•¡ì…˜:**
- [ê²Œì‹œë¬¼ ë³´ê¸°] ë²„íŠ¼
- [ë‹´ë‹¹ì ì•Œë¦¼] ë²„íŠ¼

---

# VIII. ê¸°ìˆ  ìŠ¤íƒ ë° í™˜ê²½ ì„¤ì •

## 1. í•„ìš” ë„êµ¬ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬

### 1.1 Python ë¼ì´ë¸ŒëŸ¬ë¦¬

```bash
# requirements.txt

# ì›¹ í¬ë¡¤ë§
requests==2.31.0
beautifulsoup4==4.12.2
selenium==4.15.2
webdriver-manager==4.0.1

# ë°ì´í„° ì²˜ë¦¬
pandas==2.1.3
numpy==1.26.2

# ë°ì´í„°ë² ì´ìŠ¤
pyodbc==5.0.1
sqlalchemy==2.0.23

# OCR
easyocr==1.7.1
Pillow==10.1.0

# ë™ì˜ìƒ
yt-dlp==2023.11.16
youtube-transcript-api==0.6.1
openai-whisper==20231117  # Self-hosted STT

# ë¨¸ì‹ ëŸ¬ë‹
scikit-learn==1.3.2
statsmodels==0.14.0

# NLP
transformers==4.35.2
torch==2.1.1
konlpy==0.6.0

# Azure SDK
azure-storage-blob==12.19.0
azure-cognitiveservices-vision-computervision==0.9.0

# ì‹œê°í™”
matplotlib==3.8.2
seaborn==0.13.0

# ìœ í‹¸ë¦¬í‹°
python-dotenv==1.0.0
tqdm==4.66.1
```

**ì„¤ì¹˜:**
```bash
pip install -r requirements.txt
```

### 1.2 ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

**ê°œë°œ í™˜ê²½:**
- OS: Windows 10/11, macOS, Linux (Ubuntu 22.04 ê¶Œì¥)
- Python: 3.9 ì´ìƒ
- RAM: 8GB ì´ìƒ (16GB ê¶Œì¥)
- Disk: 50GB ì´ìƒ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œìš©)

**Azure VM (í”„ë¡œë•ì…˜):**
- Size: Standard_D4s_v3 (4 vCPU, 16GB RAM)
- OS: Ubuntu 22.04 LTS
- Disk: 128GB Premium SSD

---

## 2. ê°œë°œ í™˜ê²½ êµ¬ì¶•

### 2.1 ë¡œì»¬ ê°œë°œ í™˜ê²½

**Step 1: Python ê°€ìƒí™˜ê²½ ìƒì„±**
```bash
# venv ìƒì„±
python -m venv venv

# í™œì„±í™”
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install -r requirements.txt
```

**Step 2: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**
```bash
# .env íŒŒì¼ ìƒì„±
cat > .env << EOF
# Naver API
NAVER_CLIENT_ID=9v7cOolOk2ctSQXc73sd
NAVER_CLIENT_SECRET=9jHcXVNQwZ

# YouTube API
YOUTUBE_API_KEY=YOUR_YOUTUBE_API_KEY

# Instagram/Facebook
FB_APP_ID=YOUR_FB_APP_ID
FB_APP_SECRET=YOUR_FB_APP_SECRET
INSTAGRAM_ACCESS_TOKEN=YOUR_INSTAGRAM_TOKEN

# Azure SQL Database
AZURE_SQL_SERVER=your-server.database.windows.net
AZURE_SQL_DATABASE=PMI_SNS_DB
AZURE_SQL_USERNAME=admin
AZURE_SQL_PASSWORD=your_password

# Azure Computer Vision
AZURE_CV_ENDPOINT=https://your-resource.cognitiveservices.azure.com/
AZURE_CV_KEY=your_subscription_key
EOF
```

**Step 3: Chrome WebDriver ì„¤ì¹˜**
```python
# ìë™ ì„¤ì¹˜ (webdriver-manager)
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)
```

### 2.2 Azure VM ì„¤ì • (í”„ë¡œë•ì…˜)

**Step 1: VM ìƒì„±**
```bash
# Azure CLIë¡œ VM ìƒì„±
az vm create \
  --resource-group PMI-SNS-RG \
  --name pmi-sns-crawler-vm \
  --image UbuntuLTS \
  --size Standard_D4s_v3 \
  --admin-username azureuser \
  --generate-ssh-keys
```

**Step 2: SSH ì ‘ì† ë° í™˜ê²½ êµ¬ì¶•**
```bash
# SSH ì ‘ì†
ssh azureuser@<VM_IP>

# Python 3.10 ì„¤ì¹˜
sudo apt update
sudo apt install python3.10 python3.10-venv python3-pip -y

# Chrome & ChromeDriver ì„¤ì¹˜ (Headless)
wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
sudo apt install ./google-chrome-stable_current_amd64.deb -y

# í”„ë¡œì íŠ¸ í´ë¡ 
git clone https://github.com/your-org/pmi-sns-crawler.git
cd pmi-sns-crawler

# ê°€ìƒí™˜ê²½ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬
python3.10 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# .env íŒŒì¼ ì—…ë¡œë“œ (ë¡œì»¬ì—ì„œ)
# ë¡œì»¬:
scp .env azureuser@<VM_IP>:~/pmi-sns-crawler/
```

**Step 3: Cron ì„¤ì • (ì¼ì¼ ìë™ ì‹¤í–‰)**
```bash
# crontab í¸ì§‘
crontab -e

# ë§¤ì¼ 02:00 (KST = UTC+9, ì¦‰ 17:00 UTC)ì— ì‹¤í–‰
0 17 * * * cd /home/azureuser/pmi-sns-crawler && /home/azureuser/pmi-sns-crawler/venv/bin/python main.py --platform all --days 1 >> /home/azureuser/logs/crawler.log 2>&1
```

---

## 3. ë²„ì „ ê´€ë¦¬ (Git)

### 3.1 Git ì €ì¥ì†Œ êµ¬ì¡°

```
pmi-sns-crawler/
â”œâ”€â”€ .env.example          # í™˜ê²½ ë³€ìˆ˜ í…œí”Œë¦¿
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ main.py               # ë©”ì¸ ì‹¤í–‰ íŒŒì¼
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py       # ì„¤ì • ê´€ë¦¬
â”œâ”€â”€ crawlers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ naver_crawler.py
â”‚   â”œâ”€â”€ youtube_crawler.py
â”‚   â”œâ”€â”€ instagram_crawler.py
â”‚   â”œâ”€â”€ kakao_crawler.py
â”‚   â””â”€â”€ facebook_crawler.py
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ocr_processor.py
â”‚   â””â”€â”€ video_transcriber.py
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ connection.py
â”‚   â””â”€â”€ models.py
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ clustering.py
â”‚   â”œâ”€â”€ sentiment.py
â”‚   â””â”€â”€ regression.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ retry.py
â””â”€â”€ tests/
    â”œâ”€â”€ test_naver.py
    â””â”€â”€ test_ocr.py
```

### 3.2 .gitignore

```
# .gitignore

# í™˜ê²½ ë³€ìˆ˜
.env

# Python
__pycache__/
*.py[cod]
*$py.class
venv/
*.so

# ë°ì´í„°
*.csv
*.json
output/
logs/

# IDE
.vscode/
.idea/
*.swp

# OS
.DS_Store
Thumbs.db
```

---

# IX. í”„ë¡œì íŠ¸ ì¼ì • ë° ë§ˆì¼ìŠ¤í†¤

## 1. 12ì£¼ íƒ€ì„ë¼ì¸

| ì£¼ì°¨ | í™œë™ | ì‚°ì¶œë¬¼ | ë‹´ë‹¹ |
|------|------|--------|------|
| **Week 1-2** | í™˜ê²½ ì„¤ì •, API í…ŒìŠ¤íŠ¸ | ê°œë°œ í™˜ê²½, í…ŒìŠ¤íŠ¸ ë°ì´í„° | ì¸í„´ |
| **Week 3-4** | ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ ê°œë°œ | naver_crawler.py, 1K posts | ì¸í„´ |
| **Week 5-6** | ìœ íŠœë¸Œ, ì¸ìŠ¤íƒ€ê·¸ë¨ í¬ë¡¤ëŸ¬ | youtube_crawler.py, instagram_crawler.py | ì¸í„´ |
| **Week 7** | ì¹´ì¹´ì˜¤ìŠ¤í† ë¦¬, í˜ì´ìŠ¤ë¶ í¬ë¡¤ëŸ¬ | ì „ì²´ í¬ë¡¤ëŸ¬ ì™„ì„± | ì¸í„´ |
| **Week 8** | Azure SQL Database ì„¤ì • | ìŠ¤í‚¤ë§ˆ, ì´ˆê¸° ë°ì´í„° | ì¸í„´ + DevOps |
| **Week 9** | Azure Data Factory íŒŒì´í”„ë¼ì¸ | ETL ìë™í™” | DevOps |
| **Week 10** | ë°ì´í„° ë¶„ì„ (í´ëŸ¬ìŠ¤í„°ë§, íšŒê·€) | ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸, ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ | ì¸í„´ + ë°ì´í„° ë¶„ì„ê°€ |
| **Week 11** | Power BI ëŒ€ì‹œë³´ë“œ ê°œë°œ | ëŒ€ì‹œë³´ë“œ v1.0 | ì¸í„´ + BI ì „ë¬¸ê°€ |
| **Week 12** | ìµœì¢… í…ŒìŠ¤íŠ¸, ë¬¸ì„œí™”, ì¸ìˆ˜ì¸ê³„ | ìš´ì˜ ë§¤ë‰´ì–¼, ìµœì¢… ë³´ê³ ì„œ | ì „ì²´ íŒ€ |

### 1.1 ì£¼ì°¨ë³„ ìƒì„¸ ê³„íš

**Week 1-2: Setup & Learning**
- Day 1-2: í”„ë¡œì íŠ¸ í‚¥ì˜¤í”„, ìš”êµ¬ì‚¬í•­ í™•ì¸
- Day 3-5: ë¡œì»¬ ê°œë°œ í™˜ê²½ êµ¬ì¶•, Python ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
- Day 6-8: Naver/YouTube API í…ŒìŠ¤íŠ¸, ìƒ˜í”Œ í¬ë¡¤ë§
- Day 9-10: ì½”ë“œ ë¦¬ë·°, Git ì €ì¥ì†Œ ì„¤ì •

**Week 3-4: Naver Blog Crawler**
- Day 11-13: Naver Search API + Selenium í†µí•©
- Day 14-16: í•´ì‹œíƒœê·¸ í•„í„°ë§ ë¡œì§ êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
- Day 17-18: OCR (EasyOCR) í†µí•©
- Day 19-20: 1,000ê°œ ìƒ˜í”Œ ìˆ˜ì§‘, ë°ì´í„° ê²€ì¦

**Week 5-6: YouTube & Instagram**
- Day 21-23: YouTube Data API êµ¬í˜„, ìë§‰ ë‹¤ìš´ë¡œë“œ
- Day 24-26: Instagram Graph API êµ¬í˜„, Access Token ê´€ë¦¬
- Day 27-28: ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
- Day 29-30: í†µí•© í…ŒìŠ¤íŠ¸, 5,000ê°œ ë°ì´í„° ìˆ˜ì§‘

**Week 7: Kakao & Facebook**
- Day 31-32: ì¹´ì¹´ì˜¤ìŠ¤í† ë¦¬ ìŠ¤í¬ë˜í•‘ (ë²•ì  ê²€í† )
- Day 33-34: Facebook Graph API êµ¬í˜„
- Day 35: ì „ì²´ í¬ë¡¤ëŸ¬ í†µí•© í…ŒìŠ¤íŠ¸

**Week 8: Database Setup**
- Day 36-37: Azure SQL Database ìƒì„±, ìŠ¤í‚¤ë§ˆ êµ¬í˜„
- Day 38-39: ì´ˆê¸° ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ (CSV â†’ SQL)
- Day 40: ì¸ë±ìŠ¤ ìµœì í™”, ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

**Week 9: Automation Pipeline**
- Day 41-42: Azure Data Factory íŒŒì´í”„ë¼ì¸ ì„¤ê³„
- Day 43-44: ETL Activities êµ¬í˜„ (Copy, Data Flow)
- Day 45: ìŠ¤ì¼€ì¤„ë§ ì„¤ì •, ì—ëŸ¬ í•¸ë“¤ë§

**Week 10: Data Analysis**
- Day 46-47: íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)
- Day 48-49: í´ëŸ¬ìŠ¤í„°ë§ (K-Means), ê°ì„± ë¶„ì„
- Day 50: íšŒê·€ ë¶„ì„, ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ ì‘ì„±

**Week 11: Dashboard**
- Day 51-52: Power BI ì—°ê²°, ë°ì´í„° ëª¨ë¸ë§
- Day 53-54: ëŒ€ì‹œë³´ë“œ í˜ì´ì§€ êµ¬ì¶• (4 pages)
- Day 55: ëŒ€ì‹œë³´ë“œ ë¦¬ë·°, í”¼ë“œë°± ë°˜ì˜

**Week 12: Finalization**
- Day 56-57: ì „ì²´ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
- Day 58-59: ë¬¸ì„œ ì‘ì„± (ìš´ì˜ ë§¤ë‰´ì–¼, íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ)
- Day 60: ìµœì¢… ë°œí‘œ, ì¸ìˆ˜ì¸ê³„

---

## 2. ì£¼ì°¨ë³„ ì‚°ì¶œë¬¼

| ì£¼ì°¨ | ì‚°ì¶œë¬¼ | í˜•ì‹ |
|------|--------|------|
| Week 1-2 | ê°œë°œ í™˜ê²½ êµ¬ì¶• ì™„ë£Œ ë³´ê³ ì„œ | ë¬¸ì„œ (MD) |
| Week 3-4 | naver_crawler.py, ìƒ˜í”Œ ë°ì´í„° 1K | ì½”ë“œ + CSV |
| Week 5-6 | youtube_crawler.py, instagram_crawler.py, ìƒ˜í”Œ 5K | ì½”ë“œ + CSV |
| Week 7 | ì „ì²´ í¬ë¡¤ëŸ¬ í†µí•©, ìƒ˜í”Œ 10K | ì½”ë“œ + CSV |
| Week 8 | Azure SQL Database ìŠ¤í‚¤ë§ˆ | SQL DDL ìŠ¤í¬ë¦½íŠ¸ |
| Week 9 | Azure Data Factory Pipeline JSON | JSON |
| Week 10 | ë°ì´í„° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸, ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ | Jupyter Notebook, PDF |
| Week 11 | Power BI ëŒ€ì‹œë³´ë“œ (.pbix) | PBIX íŒŒì¼ |
| Week 12 | ìµœì¢… ë³´ê³ ì„œ, ìš´ì˜ ë§¤ë‰´ì–¼ | PDF |

---

## 3. ë¦¬ìŠ¤í¬ ê´€ë¦¬

### 3.1 ë¦¬ìŠ¤í¬ ëª©ë¡

| ë¦¬ìŠ¤í¬ | í™•ë¥  | ì˜í–¥ë„ | ëŒ€ì‘ ë°©ì•ˆ |
|--------|------|--------|-----------|
| **API ë³€ê²½** (Naver, Instagram) | ì¤‘ê°„ | ë†’ìŒ | ì£¼ê¸°ì  ëª¨ë‹ˆí„°ë§, ëŒ€ì²´ ë°©ë²• ì¤€ë¹„ |
| **í¬ë¡¤ë§ ì°¨ë‹¨** (IP ë¸”ë¡) | ë‚®ìŒ | ë†’ìŒ | Rate limiting, í”„ë¡ì‹œ ì‚¬ìš© |
| **ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ** | ë†’ìŒ | ì¤‘ê°„ | ìƒ˜í”Œë§ ê²€ì¦, ìë™ í’ˆì§ˆ ì²´í¬ |
| **ì¼ì • ì§€ì—°** (í•™ìŠµ ê³¡ì„ ) | ì¤‘ê°„ | ì¤‘ê°„ | ë²„í¼ ì‹œê°„ í™•ë³´, ìš°ì„ ìˆœìœ„ ì¡°ì • |
| **Azure ë¹„ìš© ì´ˆê³¼** | ë‚®ìŒ | ë‚®ìŒ | ë¹„ìš© ì•Œë¦¼ ì„¤ì •, ë¦¬ì†ŒìŠ¤ ìµœì í™” |

### 3.2 ì»¨í‹°ì „ì‹œ í”Œëœ

**ì‹œë‚˜ë¦¬ì˜¤ 1: Naver API ë³€ê²½ â†’ í¬ë¡¤ë§ ë¶ˆê°€**
- ëŒ€ì‘: Seleniumë§Œìœ¼ë¡œ ì „í™˜ (ëŠë¦¬ì§€ë§Œ ì•ˆì •ì )
- ì˜í–¥: ìˆ˜ì§‘ ì†ë„ 50% ê°ì†Œ
- ë³µêµ¬ ì‹œê°„: 2-3ì¼

**ì‹œë‚˜ë¦¬ì˜¤ 2: Instagram API Rate Limit ì´ˆê³¼**
- ëŒ€ì‘: ì—¬ëŸ¬ Facebook App ì‚¬ìš© (ë‹¤ì¤‘ Access Token)
- ì˜í–¥: êµ¬ì¡° ë³µì¡ë„ ì¦ê°€
- ì¶”ê°€ ì‹œê°„: 1ì¼

**ì‹œë‚˜ë¦¬ì˜¤ 3: OCR ì •í™•ë„ ë‚®ìŒ (< 70%)**
- ëŒ€ì‘: EasyOCR â†’ Azure CVë¡œ ì „í™˜
- ì˜í–¥: ë¹„ìš© ì¦ê°€ ($5/ì›”)
- ë³µêµ¬ ì‹œê°„: ì¦‰ì‹œ

---

# X. ì¸ìˆ˜ì¸ê³„ ë° ë¬¸ì„œí™”

## 1. ì½”ë“œ ë¬¸ì„œí™” ê°€ì´ë“œ

### 1.1 Docstring ê·œì¹™

**í•¨ìˆ˜ Docstring (Google Style):**

```python
def crawl_naver_blog(url, target_hashtags):
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ í¬ë¡¤ë§ ë° í•´ì‹œíƒœê·¸ í•„í„°ë§
    
    Args:
        url (str): ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ URL
        target_hashtags (list): íƒ€ê²Ÿ í•´ì‹œíƒœê·¸ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['#í”¼ì— ì¸í„°ë‚´ì…”ë„'])
    
    Returns:
        dict: {
            'title': str,
            'content_text': str,
            'hashtags': list,
            'images': list,
            'videos': list
        } ë˜ëŠ” None (í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œ)
    
    Raises:
        TimeoutException: í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì´ˆê³¼ (10ì´ˆ)
        NoSuchElementException: í•„ìˆ˜ ìš”ì†Œ ë¯¸ë°œê²¬
    
    Example:
        >>> result = crawl_naver_blog('https://blog.naver.com/...', ['#í”¼ì— ì¸í„°ë‚´ì…”ë„'])
        >>> print(result['title'])
        'FitLine í›„ê¸°'
    """
    # êµ¬í˜„...
```

### 1.2 README.md ì‘ì„±

```markdown
# PM-International Korea SNS Crawler

## í”„ë¡œì íŠ¸ ê°œìš”
PM-International Korea íŒ€íŒŒíŠ¸ë„ˆì˜ SNS í™œë™ ë°ì´í„°ë¥¼ ìë™ ìˆ˜ì§‘í•˜ëŠ” í¬ë¡¤ëŸ¬

## ì£¼ìš” ê¸°ëŠ¥
- 5ê°œ SNS í”Œë«í¼ í¬ë¡¤ë§ (Naver, YouTube, Instagram, KakaoStory, Facebook)
- ì´ë¯¸ì§€ OCR ë° ë™ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œ
- Azure SQL Database ìë™ ì ì¬
- Power BI ëŒ€ì‹œë³´ë“œ ì—°ë™

## ì„¤ì¹˜ ë°©ë²•
\```bash
git clone https://github.com/your-org/pmi-sns-crawler.git
cd pmi-sns-crawler
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
\```

## ì‚¬ìš© ë°©ë²•
\```bash
# .env íŒŒì¼ ì„¤ì •
cp .env.example .env
# .env í¸ì§‘í•˜ì—¬ API í‚¤ ì…ë ¥

# í¬ë¡¤ë§ ì‹¤í–‰
python main.py --platform all --days 1

# íŠ¹ì • í”Œë«í¼ë§Œ
python main.py --platform naver --days 7
\```

## ì„¤ì •
- `config/settings.py`: í¬ë¡¤ë§ ì„¤ì •, íƒ€ê²Ÿ í•´ì‹œíƒœê·¸
- `.env`: API í‚¤, ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…
### ë¬¸ì œ: "ChromeDriver not found"
í•´ê²°: `webdriver-manager`ê°€ ìë™ìœ¼ë¡œ ì„¤ì¹˜í•˜ì§€ë§Œ, ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ í•„ìš” ì‹œ [ë§í¬](...)

### ë¬¸ì œ: "Azure SQL connection failed"
í•´ê²°: ë°©í™”ë²½ ì„¤ì •ì—ì„œ í˜„ì¬ IP ì¶”ê°€ í•„ìš”

## ë¼ì´ì„ ìŠ¤
MIT
```

---

## 2. ìš´ì˜ ë§¤ë‰´ì–¼

### 2.1 ì¼ì¼ ìš´ì˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

**ë§¤ì¼ ì˜¤ì „ 9ì‹œ (KST):**
- [ ] ì „ë‚  í¬ë¡¤ë§ ì‘ì—… ë¡œê·¸ í™•ì¸ (`/logs/crawler_YYYYMMDD.log`)
- [ ] Azure Data Factory íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìƒíƒœ í™•ì¸
- [ ] Power BI ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìµœì‹ í™” í™•ì¸ (ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ë‚ ì§œ)
- [ ] ì—ëŸ¬ ë°œìƒ ì‹œ: Slack/Email ì•Œë¦¼ í™•ì¸ ë° ëŒ€ì‘

**ì£¼ê°„ (ë§¤ì£¼ ì›”ìš”ì¼):**
- [ ] ì§€ë‚œ ì£¼ ìˆ˜ì§‘ í†µê³„ ë¦¬ë·° (ê²Œì‹œë¬¼ ìˆ˜, ì—ëŸ¬ìœ¨)
- [ ] Azure ë¦¬ì†ŒìŠ¤ ë¹„ìš© í™•ì¸
- [ ] ë°ì´í„°ë² ì´ìŠ¤ ìš©ëŸ‰ í™•ì¸ (> 80% ì‹œ Scale-up)

**ì›”ê°„ (ë§¤ì›” 1ì¼):**
- [ ] Access Token ê°±ì‹  (Instagram, Facebook)
- [ ] ì „ì›” ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ ì‘ì„±
- [ ] í¬ë¡¤ëŸ¬ ì½”ë“œ ì—…ë°ì´íŠ¸ (API ë³€ê²½ ëŒ€ì‘)

### 2.2 ì—ëŸ¬ ëŒ€ì‘ ê°€ì´ë“œ

**Error 1: "Naver API quota exceeded"**
```
ì›ì¸: ì¼ì¼ 25,000 calls ì´ˆê³¼
ëŒ€ì‘: 
1. í¬ë¡¤ë§ ë¹ˆë„ ì¡°ì • (ì¼ 1íšŒ â†’ ì£¼ 3íšŒ)
2. í‚¤ì›Œë“œ ìˆ˜ ê°ì†Œ
3. ì¶”ê°€ API í‚¤ ë°œê¸‰ (ë‹¤ë¥¸ ê³„ì •)
```

**Error 2: "Instagram API 429 Too Many Requests"**
```
ì›ì¸: ì‹œê°„ë‹¹ 200 calls ì´ˆê³¼
ëŒ€ì‘:
1. RateLimiter í´ë˜ìŠ¤ ì ìš© (190 calls/hourë¡œ ì œí•œ)
2. ì—¬ëŸ¬ Access Token ì‚¬ìš© (ë‹¤ì¤‘ ê³„ì •)
```

**Error 3: "Azure SQL Database connection timeout"**
```
ì›ì¸: ë°©í™”ë²½, ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ
ëŒ€ì‘:
1. Azure Portal â†’ SQL Database â†’ Networking â†’ Firewall í™•ì¸
2. í˜„ì¬ IP ì¶”ê°€
3. Connection String í™•ì¸
```

---

## 3. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

### 3.1 í¬ë¡¤ë§ ë¬¸ì œ

**ë¬¸ì œ: ë„¤ì´ë²„ ë¸”ë¡œê·¸ iframe ì „í™˜ ì‹¤íŒ¨**
```python
# í•´ê²°ì±…
try:
    iframe = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.ID, 'mainFrame'))
    )
    driver.switch_to.frame(iframe)
except TimeoutException:
    print("iframe not found, trying alternative method...")
    # ëŒ€ì•ˆ: ì§ì ‘ HTML íŒŒì‹±
```

**ë¬¸ì œ: YouTube API quota exceeded**
```python
# í•´ê²°ì±…: ì—¬ëŸ¬ API Key ì‚¬ìš©
API_KEYS = ['KEY1', 'KEY2', 'KEY3']
current_key_index = 0

def get_youtube_client():
    global current_key_index
    key = API_KEYS[current_key_index]
    current_key_index = (current_key_index + 1) % len(API_KEYS)
    return build('youtube', 'v3', developerKey=key)
```

### 3.2 ë°ì´í„°ë² ì´ìŠ¤ ë¬¸ì œ

**ë¬¸ì œ: Deadlock detected**
```sql
-- í•´ê²°ì±…: Transaction Isolation Level ì¡°ì •
SET TRANSACTION ISOLATION LEVEL READ COMMITTED;
```

**ë¬¸ì œ: Slow query (> 10ì´ˆ)**
```sql
-- í•´ê²°ì±…: ì¸ë±ìŠ¤ ì¶”ê°€ ë˜ëŠ” ì¿¼ë¦¬ ìµœì í™”
-- ì‹¤í–‰ ê³„íš í™•ì¸
SET STATISTICS IO ON;
SET STATISTICS TIME ON;

SELECT * FROM fact_Posts WHERE ...;
```

---

# ë¶€ë¡

## A. Python ì½”ë“œ ì „ì²´ (ëª¨ë“ˆë³„)

*ì°¸ê³ : ìœ„ì—ì„œ ì œê³µí•œ ì½”ë“œ ìŠ¤ë‹ˆí«ë“¤ì„ ëª¨ë“ˆë³„ë¡œ ì •ë¦¬í•œ ì™„ì „í•œ ì½”ë“œ*

## B. SQL ìŠ¤í¬ë¦½íŠ¸

*ì°¸ê³ : ìœ„ì—ì„œ ì œê³µí•œ DDL, DML, Stored Procedure ë“±*

## C. Azure ë¦¬ì†ŒìŠ¤ ì„¤ì • ê°€ì´ë“œ

### C.1 Azure SQL Database ìƒì„±

```bash
# Resource Group ìƒì„±
az group create --name PMI-SNS-RG --location koreacentral

# SQL Server ìƒì„±
az sql server create \
  --name pmi-sns-server \
  --resource-group PMI-SNS-RG \
  --location koreacentral \
  --admin-user sqladmin \
  --admin-password 'YourPassword123!'

# SQL Database ìƒì„± (S2: 50 DTU)
az sql db create \
  --resource-group PMI-SNS-RG \
  --server pmi-sns-server \
  --name PMI_SNS_DB \
  --service-objective S2

# ë°©í™”ë²½ ê·œì¹™ (í˜„ì¬ IP í—ˆìš©)
az sql server firewall-rule create \
  --resource-group PMI-SNS-RG \
  --server pmi-sns-server \
  --name AllowMyIP \
  --start-ip-address <YOUR_IP> \
  --end-ip-address <YOUR_IP>
```

### C.2 Azure Blob Storage ìƒì„±

```bash
# Storage Account ìƒì„±
az storage account create \
  --name pmisnsstorage \
  --resource-group PMI-SNS-RG \
  --location koreacentral \
  --sku Standard_LRS

# Container ìƒì„±
az storage container create \
  --name raw \
  --account-name pmisnsstorage

# Access Key í™•ì¸
az storage account keys list \
  --resource-group PMI-SNS-RG \
  --account-name pmisnsstorage
```

## D. API ì¸ì¦ ì„¤ì • ê°€ì´ë“œ

### D.1 Naver API

1. https://developers.naver.com/apps/#/register ì ‘ì†
2. "ì• í”Œë¦¬ì¼€ì´ì…˜ ë“±ë¡" í´ë¦­
3. ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ë¦„: "PMI SNS Crawler"
4. ì‚¬ìš© API: "ê²€ìƒ‰"
5. ë¹„ë¡œê·¸ì¸ ì˜¤í”ˆ API ì„œë¹„ìŠ¤ í™˜ê²½: "WEB ì„¤ì •" (http://localhost)
6. Client ID, Client Secret í™•ì¸ â†’ `.env`ì— ì €ì¥

### D.2 YouTube API

1. https://console.cloud.google.com/ ì ‘ì†
2. í”„ë¡œì íŠ¸ ìƒì„±: "PMI-SNS-Project"
3. "API ë° ì„œë¹„ìŠ¤" â†’ "ë¼ì´ë¸ŒëŸ¬ë¦¬" â†’ "YouTube Data API v3" ê²€ìƒ‰ â†’ ì‚¬ìš© ì„¤ì •
4. "ì‚¬ìš©ì ì¸ì¦ ì •ë³´" â†’ "ì‚¬ìš©ì ì¸ì¦ ì •ë³´ ë§Œë“¤ê¸°" â†’ "API í‚¤"
5. API í‚¤ í™•ì¸ â†’ `.env`ì— ì €ì¥

### D.3 Instagram Graph API

*ì°¸ê³ : ìœ„ì—ì„œ ì œê³µí•œ ìƒì„¸ ê°€ì´ë“œ ì°¸ì¡°*

---

**[ë¬¸ì„œ ë]**

**ë²„ì „ ì´ë ¥:**
- v1.0 (2024-10-01): ì´ˆì•ˆ ì‘ì„±
- v2.0 (2024-10-15): ê¸°ìˆ  ìŠ¤íƒ ì—…ë°ì´íŠ¸, Azure ì—°ë™ ì¶”ê°€
- **v2.2 (2025-10-27): ìµœì¢… ë²„ì „ - ì „ì²´ êµ¬í˜„ ê°€ì´ë“œ ì™„ì„±**

**ì‘ì„±ì**: PMIì½”ë¦¬ì•„ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ íŒ€  
**ê²€í† ì**: í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €, CTO  
**ìŠ¹ì¸ ì¼ì**: 2025ë…„ 10ì›” 27ì¼