#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PM-International Korea ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v6.6 (ìµœì¢… ê°œì„ íŒ)

ì£¼ìš” ê°œì„  ì‚¬í•­:
1. âœ… view_count, like_count, comment_count ì¶”ì¶œ ì¬ë„ì… (ë‹¤ì¤‘ ì„ íƒì ì‹œë„)
2. âœ… í¬ë¡¤ë§ ì•ˆì •ì„± ê°•í™” (3íšŒ ì¬ì‹œë„ + ì§€ìˆ˜ ë°±ì˜¤í”„)
3. âœ… ë°ì´í„° ê²€ì¦ ë¡œì§ ì¶”ê°€
4. âœ… ì¤‘ë³µ ì œê±° ë¡œì§ ê°•í™” (URL + ì œëª© + ì‘ì„±ì)
5. âœ… ë¡œê¹… ê°œì„  (ì§„í–‰ë¥ , ì„±ê³µë¥  í‘œì‹œ)
6. âœ… ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”
7. âœ… referrer_name, hashtags, media_urls ì¶”ì¶œ ë¡œì§ ê°œì„ 
8. âœ… ì‹¤íŒ¨ URL ë³„ë„ ì €ì¥ (failed_urls.json)
9. âœ… ì†ë„ ìµœì í™” (ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”, ëŒ€ê¸° ì‹œê°„ ì¡°ì •)
"""

import os
import re
import json
import time
import random
import logging
from datetime import datetime
from urllib.parse import urlparse, parse_qs, unquote
from typing import List, Dict, Optional, Set, Tuple

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd

# ===========================
# ë¡œê¹… ì„¤ì •
# ===========================

class ColoredFormatter(logging.Formatter):
    """ì»¬ëŸ¬ ë¡œê¹… í¬ë§·í„°"""
    
    COLORS = {
        'DEBUG': '\033[94m',    # íŒŒë€ìƒ‰
        'INFO': '\033[92m',     # ì´ˆë¡ìƒ‰
        'WARNING': '\033[93m',  # ë…¸ë€ìƒ‰
        'ERROR': '\033[91m',    # ë¹¨ê°„ìƒ‰
        'CRITICAL': '\033[95m', # ë³´ë¼ìƒ‰
        'RESET': '\033[0m'
    }
    
    def format(self, record):
        log_color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
        record.levelname = f"{log_color}{record.levelname}{self.COLORS['RESET']}"
        return super().format(record)

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# íŒŒì¼ í•¸ë“¤ëŸ¬
file_handler = logging.FileHandler(
    f'naver_blog_crawler_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log',
    encoding='utf-8'
)
file_handler.setLevel(logging.DEBUG)
file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(file_formatter)

# ì½˜ì†” í•¸ë“¤ëŸ¬
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_formatter = ColoredFormatter('%(levelname)s - %(message)s')
console_handler.setFormatter(console_formatter)

logger.addHandler(file_handler)
logger.addHandler(console_handler)

# ===========================
# ì„¤ì • ì˜ì—­
# ===========================

# Naver Open API ì„¤ì •
NAVER_CLIENT_ID = "9v7cOolOk2ctSQXc73sd"
NAVER_CLIENT_SECRET = "9jHcXVNQwZ"

# ê²€ìƒ‰ í‚¤ì›Œë“œ (PM International ê´€ë ¨)
SEARCH_KEYWORDS = [
    "í”¼ì— ì¸í„°ë‚´ì…”ë„",
    "í”¼ì— ì½”ë¦¬ì•„", 
    "PMì¸í„°ë‚´ì…”ë„",
    "ë…ì¼í”¼ì— ",
    "í•ë¼ì¸",
    "FitLine",
    "ë² ì´ì‹ìŠ¤",
    "í”„ë¡œì…°ì´í”„",
    "ì—‘í‹°ë°”ì´ì¦ˆ",
    "íŒŒì›Œì¹µí…Œì¼",
    "ë¦¬ìŠ¤í† ë ˆì´íŠ¸"
]

# PM ê´€ë ¨ í•„ìˆ˜ í‚¤ì›Œë“œ (ìµœì†Œ 1ê°œ ì´ìƒ í¬í•¨)
PM_REQUIRED_KEYWORDS = [
    "í”¼ì— ", "PM", "pm", "í•ë¼ì¸", "fitline", "FitLine", 
    "íŒ€íŒŒíŠ¸ë„ˆ", "ë…ì¼í”¼ì— ", "í”¼ì— ì½”ë¦¬ì•„", "í”¼ì— ì¸í„°ë‚´ì…”ë„"
]

# ì œì™¸ í‚¤ì›Œë“œ
EXCLUDE_KEYWORDS = [
    "ì±„ìš©", "êµ¬ì¸", "êµ¬ì§", "ì•Œë°”", "ì•„ë¥´ë°”ì´íŠ¸",
    "ì‚¬ê¸°", "í”¼í•´", "í™˜ë¶ˆ", "ì†Œì†¡", "ì‚¬ì¹­"
]

# API ê²€ìƒ‰ ì„¤ì •
DISPLAY_PER_PAGE = 100
MAX_PAGES = 1
TOTAL_TARGET = 100

# Selenium ì„¤ì •
SELENIUM_WAIT_TIME = 10
PAGE_LOAD_WAIT = 2  # 3ì´ˆ â†’ 2ì´ˆë¡œ ë‹¨ì¶• (ì†ë„ ê°œì„ )
PAGE_LOAD_TIMEOUT = 15  # í˜ì´ì§€ ë¡œë”© ìµœëŒ€ ëŒ€ê¸° ì‹œê°„
MAX_RETRY = 3  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

# ìš”ì²­ ê°„ ë”œë ˆì´ (ì´ˆ)
REQUEST_DELAY_MIN = 1.5
REQUEST_DELAY_MAX = 3.5

# User-Agent
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0'
]

# ===========================
# í†µê³„ í´ë˜ìŠ¤
# ===========================

class CrawlingStats:
    """í¬ë¡¤ë§ í†µê³„ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.total_attempts = 0
        self.success_count = 0
        self.fail_count = 0
        self.filtered_out = 0
        self.duplicate_count = 0
        self.start_time = time.time()
    
    def add_attempt(self):
        self.total_attempts += 1
    
    def add_success(self):
        self.success_count += 1
    
    def add_fail(self):
        self.fail_count += 1
    
    def add_filtered(self):
        self.filtered_out += 1
    
    def add_duplicate(self):
        self.duplicate_count += 1
    
    def get_success_rate(self) -> float:
        if self.total_attempts == 0:
            return 0.0
        return (self.success_count / self.total_attempts) * 100
    
    def get_elapsed_time(self) -> str:
        elapsed = time.time() - self.start_time
        minutes = int(elapsed // 60)
        seconds = int(elapsed % 60)
        return f"{minutes}ë¶„ {seconds}ì´ˆ"
    
    def print_stats(self):
        logger.info("=" * 70)
        logger.info("ğŸ“Š í¬ë¡¤ë§ í†µê³„")
        logger.info("=" * 70)
        logger.info(f"ì´ ì‹œë„: {self.total_attempts}ê°œ")
        logger.info(f"âœ… ì„±ê³µ: {self.success_count}ê°œ")
        logger.info(f"âŒ ì‹¤íŒ¨: {self.fail_count}ê°œ")
        logger.info(f"ğŸš« í•„í„°ë§: {self.filtered_out}ê°œ")
        logger.info(f"ğŸ” ì¤‘ë³µ: {self.duplicate_count}ê°œ")
        logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {self.get_success_rate():.1f}%")
        logger.info(f"â±ï¸  ì†Œìš” ì‹œê°„: {self.get_elapsed_time()}")
        logger.info("=" * 70)

# ì „ì—­ í†µê³„ ê°ì²´
stats = CrawlingStats()

# ===========================
# Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™”
# ===========================

def init_selenium_driver() -> webdriver.Chrome:
    """Selenium Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_argument(f'user-agent={random.choice(USER_AGENTS)}')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™” (ì†ë„ í–¥ìƒ)
    prefs = {'profile.managed_default_content_settings.images': 2}
    chrome_options.add_experimental_option('prefs', prefs)
    
    # Homebrew chromedriver ìš°ì„  ì‚¬ìš© (macOS ë³´ì•ˆ ë¬¸ì œ í•´ê²°)
    homebrew_chromedriver = '/opt/homebrew/bin/chromedriver'
    
    if os.path.exists(homebrew_chromedriver):
        service = Service(homebrew_chromedriver)
        driver = webdriver.Chrome(service=service, options=chrome_options)
    else:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
    
    # í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì†ë„ ê°œì„ )
    driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)
    
    driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
        'source': '''
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            })
        '''
    })
    
    logger.info("âœ… Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ")
    return driver

# ===========================
# API ê²€ìƒ‰ í•¨ìˆ˜
# ===========================

def search_naver_blog(keyword: str, display: int = 100, start: int = 1) -> Optional[Dict]:
    """Naver Open Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡œê·¸ ê²€ìƒ‰"""
    url = "https://openapi.naver.com/v1/search/blog.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
        "User-Agent": random.choice(USER_AGENTS)
    }
    params = {
        "query": keyword,
        "display": display,
        "start": start,
        "sort": "sim"
    }
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        logger.error(f"API ìš”ì²­ ì‹¤íŒ¨ - í‚¤ì›Œë“œ: {keyword}, ì—ëŸ¬: {str(e)}")
        return None

# ===========================
# URL íŒŒì‹± ë° ì •ê·œí™”
# ===========================

def extract_blog_info_from_url(url: str) -> Optional[Dict[str, str]]:
    """ë¸”ë¡œê·¸ URLì—ì„œ blog_idì™€ post_id ì¶”ì¶œ"""
    try:
        url = unquote(url)
        parsed = urlparse(url)
        
        # íŒ¨í„´ 1: blog.naver.com/{blog_id}/{post_id}
        match = re.search(r'blog\.naver\.com/([^/]+)/(\d+)', url)
        if match:
            return {'blog_id': match.group(1), 'post_id': match.group(2)}
        
        # íŒ¨í„´ 2: PostView.nhn?blogId={blog_id}&logNo={post_id}
        if 'PostView' in url:
            query_params = parse_qs(parsed.query)
            blog_id = query_params.get('blogId', [None])[0]
            post_id = query_params.get('logNo', [None])[0]
            if blog_id and post_id:
                return {'blog_id': blog_id, 'post_id': post_id}
        
        # íŒ¨í„´ 3: m.blog.naver.com/{blog_id}/{post_id}
        match = re.search(r'm\.blog\.naver\.com/([^/]+)/(\d+)', url)
        if match:
            return {'blog_id': match.group(1), 'post_id': match.group(2)}
        
        return None
        
    except Exception as e:
        logger.error(f"URL íŒŒì‹± ì˜¤ë¥˜: {url}, {str(e)}")
        return None

def normalize_blog_url(blog_id: str, post_id: str) -> str:
    """ì •ê·œí™”ëœ ë¸”ë¡œê·¸ URL ìƒì„±"""
    return f"https://blog.naver.com/{blog_id}/{post_id}"

# ===========================
# ì½˜í…ì¸  í•„í„°ë§
# ===========================

def is_pm_related_content(title: str, content: str) -> bool:
    """ì œëª© ë° ë³¸ë¬¸ì´ PM International ê´€ë ¨ ë‚´ìš©ì¸ì§€ í™•ì¸"""
    combined_text = f"{title} {content}".lower()
    
    # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
    for exclude_kw in EXCLUDE_KEYWORDS:
        if exclude_kw.lower() in combined_text:
            logger.debug(f"ì œì™¸ í‚¤ì›Œë“œ ë°œê²¬: {exclude_kw}")
            return False
    
    # PM í•„ìˆ˜ í‚¤ì›Œë“œ ì²´í¬
    has_required = any(kw.lower() in combined_text for kw in PM_REQUIRED_KEYWORDS)
    
    if not has_required:
        logger.debug("PM í•„ìˆ˜ í‚¤ì›Œë“œ ë¯¸í¬í•¨")
        return False
    
    return True

# ===========================
# ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜ë“¤
# ===========================

def extract_blog_name(driver: webdriver.Chrome) -> str:
    """ë¸”ë¡œê·¸ëª… ì¶”ì¶œ (ë‹¤ì¤‘ ì„ íƒì ì‹œë„)"""
    selectors = [
        "div.blog_title a",
        "div.blog2_series a.link",
        "h1.blog_title",
        "div.tit_area h3.tit",
        "div.blog_title_area a",
        "a.blog_name",
        "div.nick_area a.nick"
    ]
    
    for selector in selectors:
        try:
            element = driver.find_element(By.CSS_SELECTOR, selector)
            blog_name = element.text.strip()
            if blog_name and len(blog_name) > 0:
                logger.debug(f"ë¸”ë¡œê·¸ëª… ì¶”ì¶œ ì„±ê³µ: {blog_name}")
                return blog_name
        except NoSuchElementException:
            continue
    
    # ë©”íƒ€ íƒœê·¸ ì‹œë„
    try:
        og_title = driver.find_element(By.CSS_SELECTOR, 'meta[property="og:title"]')
        content = og_title.get_attribute('content')
        if content and ' : ' in content:
            return content.split(' : ')[0].strip()
        elif content:
            return content.strip()
    except NoSuchElementException:
        pass
    
    # í˜ì´ì§€ íƒ€ì´í‹€
    title = driver.title
    if ' : ' in title:
        return title.split(' : ')[0].strip()
    
    logger.warning("ë¸”ë¡œê·¸ëª… ì¶”ì¶œ ì‹¤íŒ¨")
    return "ì•Œ ìˆ˜ ì—†ìŒ"

def extract_view_count(driver: webdriver.Chrome, soup: BeautifulSoup) -> int:
    """ì¡°íšŒìˆ˜ ì¶”ì¶œ (ë‹¤ì¤‘ ì„ íƒì ì‹œë„)"""
    selectors = [
        'span.se_publishDate.pcol2 em',  # ìŠ¤ë§ˆíŠ¸ì—ë””í„° ONE
        'span.pcol2 em',
        'span.cnt_view',
        'span.view',
        'em.cnt',
        'span.count',
        'div.blog_post_view_info span.num'
    ]
    
    # Selenium ì‹œë„
    for selector in selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            for elem in elements:
                text = elem.text.strip()
                # "ì¡°íšŒ 123" ë˜ëŠ” "123" í˜•íƒœ
                match = re.search(r'(\d[\d,]*)', text)
                if match:
                    count = int(match.group(1).replace(',', ''))
                    logger.debug(f"ì¡°íšŒìˆ˜ ì¶”ì¶œ: {count}")
                    return count
        except Exception:
            continue
    
    # BeautifulSoup ì‹œë„
    for selector in selectors:
        elem = soup.select_one(selector)
        if elem:
            text = elem.get_text(strip=True)
            match = re.search(r'(\d[\d,]*)', text)
            if match:
                count = int(match.group(1).replace(',', ''))
                logger.debug(f"ì¡°íšŒìˆ˜ ì¶”ì¶œ (BS): {count}")
                return count
    
    # í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
    page_text = soup.get_text()
    match = re.search(r'ì¡°íšŒ[:\s]*(\d[\d,]*)', page_text)
    if match:
        count = int(match.group(1).replace(',', ''))
        logger.debug(f"ì¡°íšŒìˆ˜ ì¶”ì¶œ (í…ìŠ¤íŠ¸): {count}")
        return count
    
    logger.debug("ì¡°íšŒìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨")
    return 0

def extract_like_count(driver: webdriver.Chrome, soup: BeautifulSoup) -> int:
    """ì¢‹ì•„ìš”(ê³µê°) ìˆ˜ ì¶”ì¶œ"""
    selectors = [
        'span.u_cnt._count',  # ê³µê° ë²„íŠ¼
        'span.cnt_like',
        'em.u_cnt',
        'span.like_count',
        'a.btn_empathy span.u_cnt',
        'div.end_btn span.u_cnt'
    ]
    
    # Selenium ì‹œë„
    for selector in selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            for elem in elements:
                text = elem.text.strip()
                match = re.search(r'(\d[\d,]*)', text)
                if match:
                    count = int(match.group(1).replace(',', ''))
                    logger.debug(f"ì¢‹ì•„ìš”ìˆ˜ ì¶”ì¶œ: {count}")
                    return count
        except Exception:
            continue
    
    # BeautifulSoup ì‹œë„
    for selector in selectors:
        elem = soup.select_one(selector)
        if elem:
            text = elem.get_text(strip=True)
            match = re.search(r'(\d[\d,]*)', text)
            if match:
                count = int(match.group(1).replace(',', ''))
                logger.debug(f"ì¢‹ì•„ìš”ìˆ˜ ì¶”ì¶œ (BS): {count}")
                return count
    
    # í…ìŠ¤íŠ¸ ê²€ìƒ‰
    page_text = soup.get_text()
    match = re.search(r'ê³µê°[:\s]*(\d[\d,]*)', page_text)
    if match:
        count = int(match.group(1).replace(',', ''))
        logger.debug(f"ì¢‹ì•„ìš”ìˆ˜ ì¶”ì¶œ (í…ìŠ¤íŠ¸): {count}")
        return count
    
    logger.debug("ì¢‹ì•„ìš”ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨")
    return 0

def extract_comment_count(driver: webdriver.Chrome, soup: BeautifulSoup) -> int:
    """ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ"""
    selectors = [
        'span.u_cnt._count.ccmtcnt',  # ëŒ“ê¸€ ë²„íŠ¼
        'span.cnt_cmt',
        'em.u_cmt',
        'span.comment_count',
        'a.btn_comment span.u_cnt',
        'div.end_btn span.u_cnt'
    ]
    
    # Selenium ì‹œë„
    for selector in selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            for elem in elements:
                text = elem.text.strip()
                match = re.search(r'(\d[\d,]*)', text)
                if match:
                    count = int(match.group(1).replace(',', ''))
                    logger.debug(f"ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ: {count}")
                    return count
        except Exception:
            continue
    
    # BeautifulSoup ì‹œë„
    for selector in selectors:
        elem = soup.select_one(selector)
        if elem:
            text = elem.get_text(strip=True)
            match = re.search(r'(\d[\d,]*)', text)
            if match:
                count = int(match.group(1).replace(',', ''))
                logger.debug(f"ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ (BS): {count}")
                return count
    
    # í…ìŠ¤íŠ¸ ê²€ìƒ‰
    page_text = soup.get_text()
    match = re.search(r'ëŒ“ê¸€[:\s]*(\d[\d,]*)', page_text)
    if match:
        count = int(match.group(1).replace(',', ''))
        logger.debug(f"ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ (í…ìŠ¤íŠ¸): {count}")
        return count
    
    logger.debug("ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨")
    return 0

def extract_hashtags(driver: webdriver.Chrome, soup: BeautifulSoup, content: str) -> List[str]:
    """í•´ì‹œíƒœê·¸ ì¶”ì¶œ (3ë‹¨ê³„)"""
    hashtags = set()
    
    # 1. í•˜ë‹¨ íƒœê·¸ ì˜ì—­
    tag_selectors = [
        "div.post_tag a.link",
        "div.blog2_series a.link",
        "div.tag_area a",
        "div.post_bottom_area a.link",
        "a[href*='/PostList.naver?tag=']"
    ]
    
    for selector in tag_selectors:
        try:
            tag_elements = driver.find_elements(By.CSS_SELECTOR, selector)
            for elem in tag_elements:
                tag_text = elem.text.strip()
                if tag_text:
                    hashtags.add(tag_text.lstrip('#'))
        except Exception:
            continue
    
    # 2. ë³¸ë¬¸ ë‚´ #í•´ì‹œíƒœê·¸
    hash_pattern = re.findall(r'#([^\s#]+)', content)
    hashtags.update(hash_pattern)
    
    # 3. ë©”íƒ€ë°ì´í„°
    try:
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords and meta_keywords.get('content'):
            keywords = meta_keywords['content'].split(',')
            hashtags.update([k.strip() for k in keywords if k.strip()])
    except Exception:
        pass
    
    result = sorted(list(hashtags))[:20]
    logger.debug(f"í•´ì‹œíƒœê·¸ ì¶”ì¶œ: {len(result)}ê°œ")
    return result

def extract_media_urls(driver: webdriver.Chrome, soup: BeautifulSoup) -> Tuple[List[str], List[str]]:
    """ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ URL ì¶”ì¶œ"""
    image_urls = []
    video_urls = []
    
    try:
        # ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
        time.sleep(2)
        
        # ì´ë¯¸ì§€ ì¶”ì¶œ
        img_selectors = [
            "div.se-main-container img",
            "div#postViewArea img",
            "div.se-component-content img",
            "div.post_ct img",
            "div.__se_component_area img"
        ]
        
        for selector in img_selectors:
            try:
                img_elements = driver.find_elements(By.CSS_SELECTOR, selector)
                for img in img_elements:
                    src = img.get_attribute('src') or img.get_attribute('data-src')
                    if src and src.startswith('http'):
                        # ì¸ë„¤ì¼ â†’ ì›ë³¸
                        if 'type=w' in src:
                            src = re.sub(r'type=w\d+', 'type=w966', src)
                        image_urls.append(src)
            except Exception:
                continue
        
        # ë¹„ë””ì˜¤ ì¶”ì¶œ
        video_selectors = [
            "div.se-main-container video",
            "div#postViewArea video",
            "div.se-component-content video"
        ]
        
        for selector in video_selectors:
            try:
                video_elements = driver.find_elements(By.CSS_SELECTOR, selector)
                for video in video_elements:
                    src = video.get_attribute('src') or video.get_attribute('data-src')
                    if src and src.startswith('http'):
                        video_urls.append(src)
            except Exception:
                continue
        
        # ì¤‘ë³µ ì œê±°
        image_urls = list(dict.fromkeys(image_urls))[:10]
        video_urls = list(dict.fromkeys(video_urls))[:5]
        
        logger.debug(f"ë¯¸ë””ì–´ ì¶”ì¶œ: ì´ë¯¸ì§€ {len(image_urls)}ê°œ, ë¹„ë””ì˜¤ {len(video_urls)}ê°œ")
        return image_urls, video_urls
        
    except Exception as e:
        logger.error(f"ë¯¸ë””ì–´ URL ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return [], []

def parse_date(date_str: str) -> Optional[str]:
    """ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±"""
    try:
        date_str = re.sub(r'[^\d\-\.\s:]', '', date_str).strip()
        
        patterns = [
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})\s+(\d{1,2}):(\d{1,2})',
            r'(\d{4})-(\d{1,2})-(\d{1,2})\s+(\d{1,2}):(\d{1,2})',
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})',
            r'(\d{4})-(\d{1,2})-(\d{1,2})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, date_str)
            if match:
                groups = match.groups()
                if len(groups) == 5:
                    year, month, day, hour, minute = groups
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)} {hour.zfill(2)}:{minute.zfill(2)}:00"
                else:
                    year, month, day = groups
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)} 00:00:00"
        
        return None
        
    except Exception as e:
        logger.error(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {date_str}, {str(e)}")
        return None

# ===========================
# ë°ì´í„° ê²€ì¦
# ===========================

def validate_post_data(post_data: Dict) -> bool:
    """ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦"""
    required_fields = ['post_id', 'blog_id', 'url', 'title', 'content']
    
    for field in required_fields:
        if not post_data.get(field):
            logger.warning(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
            return False
    
    # ì œëª© ê¸¸ì´ ì²´í¬
    if len(post_data['title']) < 2:
        logger.warning(f"ì œëª©ì´ ë„ˆë¬´ ì§§ìŒ: {post_data['title']}")
        return False
    
    # ë³¸ë¬¸ ê¸¸ì´ ì²´í¬
    if len(post_data['content']) < 50:
        logger.warning(f"ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ: {len(post_data['content'])}ì")
        return False
    
    return True

# ===========================
# ì¬ì‹œë„ ë¡œì§
# ===========================

def retry_with_backoff(func, *args, **kwargs):
    """ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ ì‚¬ìš©í•œ ì¬ì‹œë„"""
    for attempt in range(MAX_RETRY):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            if attempt < MAX_RETRY - 1:
                wait_time = (2 ** attempt) + random.uniform(0, 1)
                logger.warning(f"ì¬ì‹œë„ {attempt + 1}/{MAX_RETRY}: {wait_time:.1f}ì´ˆ ëŒ€ê¸° - {str(e)}")
                time.sleep(wait_time)
            else:
                logger.error(f"ìµœì¢… ì‹¤íŒ¨: {str(e)}")
                raise

# ===========================
# ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜
# ===========================

def crawl_blog_post_selenium(driver: webdriver.Chrome, url: str, blog_id: str, post_id: str, 
                             failed_url_manager=None) -> Optional[Dict]:
    """Seleniumì„ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ í¬ë¡¤ë§ (ì¬ì‹œë„ í¬í•¨)"""
    
    def _crawl():
        logger.debug(f"í¬ë¡¤ë§ ì‹œì‘: {url}")
        driver.get(url)
        time.sleep(PAGE_LOAD_WAIT)
        
        # iframe ì²˜ë¦¬
        try:
            WebDriverWait(driver, SELENIUM_WAIT_TIME).until(
                EC.frame_to_be_available_and_switch_to_it((By.ID, "mainFrame"))
            )
            logger.debug("mainFrame ì „í™˜ ì™„ë£Œ")
        except TimeoutException:
            logger.debug("mainFrame ì—†ìŒ - ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§")
        
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # ë¸”ë¡œê·¸ëª…
        referrer_name = extract_blog_name(driver)
        
        # ì œëª©
        title_selectors = [
            'div.se-title-text',
            'div.pcol1 span.se-fs-',
            'div#viewTypeSelector span.se-fs-',
            'h3.se-title-text',
            'span.pcol1.itemSubjectBoldfont'
        ]
        title = None
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem and title_elem.get_text(strip=True):
                title = title_elem.get_text(strip=True)
                break
        
        if not title:
            logger.warning(f"ì œëª© ì—†ìŒ: {url}")
            title = "ì œëª© ì—†ìŒ"
        
        # ë³¸ë¬¸
        content_selectors = [
            'div.se-main-container',
            'div#postViewArea',
            'div.se-component-content'
        ]
        content = ""
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = content_elem.get_text(separator='\n', strip=True)
                break
        
        if not content:
            logger.warning(f"ë³¸ë¬¸ ì—†ìŒ: {url}")
            return None
        
        # PM í•„í„°ë§
        if not is_pm_related_content(title, content):
            logger.debug(f"PM ë¬´ê´€: {title}")
            stats.add_filtered()
            return None
        
        # ë°œí–‰ì¼
        published_date = None
        date_selectors = [
            'span.se_publishDate',
            'span.se-publishDate',
            'div.se_publishDate',
            'span.post_date',
            'p.date'
        ]
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                published_date = parse_date(date_text)
                break
        
        # ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”ìˆ˜, ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ
        view_count = extract_view_count(driver, soup)
        like_count = extract_like_count(driver, soup)
        comment_count = extract_comment_count(driver, soup)
        
        # í•´ì‹œíƒœê·¸
        hashtags = extract_hashtags(driver, soup, content)
        
        # ë¯¸ë””ì–´ URL
        image_urls, video_urls = extract_media_urls(driver, soup)
        
        # ê²°ê³¼ êµ¬ì„±
        post_data = {
            'post_id': post_id,
            'blog_id': blog_id,
            'url': url,
            'title': title,
            'content': content[:5000],
            'published_date': published_date,
            'referrer_name': referrer_name,
            'view_count': view_count,
            'like_count': like_count,
            'comment_count': comment_count,
            'hashtags': ', '.join(hashtags) if hashtags else '',
            'image_urls': json.dumps(image_urls, ensure_ascii=False) if image_urls else '',
            'video_urls': json.dumps(video_urls, ensure_ascii=False) if video_urls else '',
            'collected_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # ë°ì´í„° ê²€ì¦
        if not validate_post_data(post_data):
            logger.warning(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {title}")
            return None
        
        logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {title[:30]}... (ì¡°íšŒ:{view_count}, ì¢‹ì•„ìš”:{like_count}, ëŒ“ê¸€:{comment_count})")
        return post_data
    
    # ì¬ì‹œë„ ë¡œì§ ì ìš©
    stats.add_attempt()
    try:
        result = retry_with_backoff(_crawl)
        if result:
            stats.add_success()
        return result
    except Exception as e:
        error_msg = str(e)
        logger.error(f"í¬ë¡¤ë§ ìµœì¢… ì‹¤íŒ¨: {url}, {error_msg}")
        stats.add_fail()
        
        # ì‹¤íŒ¨ URL ê¸°ë¡
        if failed_url_manager:
            failed_url_manager.add_failed_url(url, error_msg, blog_id, post_id)
        
        return None
    finally:
        try:
            driver.switch_to.default_content()
        except Exception:
            pass

# ===========================
# ì¤‘ë³µ ì²´í¬
# ===========================

def generate_post_fingerprint(post_data: Dict) -> str:
    """ê²Œì‹œë¬¼ ê³ ìœ  ì§€ë¬¸ ìƒì„± (URL + ì œëª© + ì‘ì„±ì)"""
    return f"{post_data['url']}_{post_data['title']}_{post_data['blog_id']}"

# ===========================
# ì—ëŸ¬ ê²Œì‹œë¬¼ ê´€ë¦¬
# ===========================

class FailedURLManager:
    """ì‹¤íŒ¨í•œ URL ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, filename='failed_urls.json'):
        self.filename = filename
        self.failed_urls = []
    
    def add_failed_url(self, url: str, reason: str, blog_id: str = '', post_id: str = ''):
        """ì‹¤íŒ¨í•œ URL ì¶”ê°€"""
        self.failed_urls.append({
            'url': url,
            'blog_id': blog_id,
            'post_id': post_id,
            'reason': reason,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
    
    def save_to_file(self):
        """ì‹¤íŒ¨í•œ URLì„ íŒŒì¼ì— ì €ì¥"""
        if self.failed_urls:
            with open(self.filename, 'w', encoding='utf-8') as f:
                json.dump(self.failed_urls, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ ì‹¤íŒ¨ URL ì €ì¥: {self.filename} ({len(self.failed_urls)}ê°œ)")
    
    def get_failed_count(self) -> int:
        return len(self.failed_urls)

# ===========================
# ë©”ì¸ í•¨ìˆ˜
# ===========================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("=" * 70)
    logger.info("ğŸš€ PM-International Korea ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v6.6 ì‹œì‘")
    logger.info("=" * 70)
    
    driver = init_selenium_driver()
    failed_url_manager = FailedURLManager()
    
    collected_posts = []
    collected_urls = set()
    collected_fingerprints = set()
    
    try:
        for keyword in SEARCH_KEYWORDS:
            logger.info(f"\nğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰: '{keyword}'")
            
            for page in range(1, MAX_PAGES + 1):
                start = (page - 1) * DISPLAY_PER_PAGE + 1
                logger.info(f"ğŸ“„ í˜ì´ì§€ {page} ê²€ìƒ‰ ì¤‘... (start={start})")
                
                search_result = search_naver_blog(keyword, DISPLAY_PER_PAGE, start)
                
                if not search_result or 'items' not in search_result:
                    logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    break
                
                items = search_result['items']
                logger.info(f"ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼: {len(items)}ê°œ")
                
                if len(items) == 0:
                    break
                
                for idx, item in enumerate(items, 1):
                    if len(collected_posts) >= TOTAL_TARGET:
                        logger.info(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±: {TOTAL_TARGET}ê°œ")
                        break
                    
                    link = item.get('link', '')
                    if not link or link in collected_urls:
                        stats.add_duplicate()
                        continue
                    
                    blog_info = extract_blog_info_from_url(link)
                    if not blog_info:
                        continue
                    
                    blog_id = blog_info['blog_id']
                    post_id = blog_info['post_id']
                    normalized_url = normalize_blog_url(blog_id, post_id)
                    
                    if normalized_url in collected_urls:
                        stats.add_duplicate()
                        continue
                    
                    logger.info(f"[{len(collected_posts)+1}/{TOTAL_TARGET}] í¬ë¡¤ë§ ì¤‘...")
                    
                    post_data = crawl_blog_post_selenium(driver, normalized_url, blog_id, post_id, failed_url_manager)
                    
                    if post_data:
                        fingerprint = generate_post_fingerprint(post_data)
                        if fingerprint not in collected_fingerprints:
                            collected_posts.append(post_data)
                            collected_urls.add(normalized_url)
                            collected_fingerprints.add(fingerprint)
                        else:
                            logger.debug("ì¤‘ë³µ ê²Œì‹œë¬¼ (fingerprint)")
                            stats.add_duplicate()
                    
                    time.sleep(random.uniform(REQUEST_DELAY_MIN, REQUEST_DELAY_MAX))
                
                if len(collected_posts) >= TOTAL_TARGET:
                    break
                
                time.sleep(random.uniform(2, 4))
            
            if len(collected_posts) >= TOTAL_TARGET:
                break
        
        # í†µê³„ ì¶œë ¥
        stats.print_stats()
        
        # ê²°ê³¼ ì €ì¥
        if collected_posts:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'naver_blog_test_{timestamp}.csv'
            
            df = pd.DataFrame(collected_posts)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            logger.info(f"\n{'=' * 70}")
            logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
            logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(collected_posts)}ê°œ")
            logger.info(f"{'=' * 70}")
        else:
            logger.warning("âš ï¸  ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì‹¤íŒ¨ URL ì €ì¥
        failed_url_manager.save_to_file()
        if failed_url_manager.get_failed_count() > 0:
            logger.info(f"âŒ ì‹¤íŒ¨í•œ URL: {failed_url_manager.get_failed_count()}ê°œ (failed_urls.json ì°¸ì¡°)")
    
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  ì‚¬ìš©ì ì¤‘ë‹¨")
        stats.print_stats()
    
    except Exception as e:
        logger.error(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {str(e)}")
        stats.print_stats()
    
    finally:
        driver.quit()
        logger.info("âœ… ë“œë¼ì´ë²„ ì¢…ë£Œ")

if __name__ == "__main__":
    main()
