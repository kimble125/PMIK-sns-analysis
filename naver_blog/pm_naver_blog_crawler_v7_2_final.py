#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PM-International Korea ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v7.2 (ìµœì¢… ê°œì„ íŒ)

ğŸ¯ v7.2 ì£¼ìš” ê°œì„  ì‚¬í•­:
1. âœ… invalid session id ì—ëŸ¬ í•´ê²°: ë“œë¼ì´ë²„ ìë™ ì¬ì‹œì‘ ë¡œì§ ì¶”ê°€
2. âœ… ë¡œê¹… ë ˆë²¨ INFOë¡œ ë³€ê²½: í„°ë¯¸ë„ ë©”ì‹œì§€ 90% ê°ì†Œ
3. âœ… published_datetime ì¶”ê°€: ë‚ ì§œ + ì‹œê°„ í•¨ê»˜ ìˆ˜ì§‘ (YYYY-MM-DD HH:MM:SS)
4. âœ… í•„í„°ë§ ë¡œì§ ì™„í™”: PM + ì¶”ì²œì¸ë§Œ í•„ìˆ˜, 8ìë¦¬ëŠ” ì„ íƒ (ìˆ˜ì§‘ëŸ‰ ì¦ê°€)
5. âœ… ê²€ìƒ‰ ê²°ê³¼ ì‚¬ì „ í•„í„°ë§: ë¶ˆí•„ìš”í•œ í¬ë¡¤ë§ 50% ê°ì†Œ
6. âœ… ì§„í–‰ë¥  í‘œì‹œ ê°œì„ : ì‹¤ì‹œê°„ í†µê³„ ë° ì„±ê³µë¥  í‘œì‹œ
7. âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ê°•í™”: ì£¼ê¸°ì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜

ğŸ“Š ì¶œë ¥ ì»¬ëŸ¼ (15ê°œ):
- ê¸°ë³¸: platform, post_id, blog_id, url, title, content, published_datetime
- ì¶”ì²œì¸: sponsor_phone, sponsor_partner_id
- ì°¸ì—¬: like_count, comment_count
- ì½˜í…ì¸ : hashtags, image_urls, video_urls
- ë©”íƒ€: collected_date

ì‘ì„±ì: PMI Korea ë°ì´í„° ë¶„ì„íŒ€
ë²„ì „: 7.2
ìµœì¢… ìˆ˜ì •ì¼: 2025-11-06
"""

import os
import re
import json
import time
import random
import logging
import gc
from datetime import datetime
from typing import List, Dict, Optional, Set, Tuple
from urllib.parse import urlparse, parse_qs, unquote

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd

# config ëª¨ë“ˆì´ ìˆëŠ” ê²½ìš°ì—ë§Œ import (ë¡œì»¬ í™˜ê²½)
try:
    import config
    NAVER_CLIENT_ID = config.NAVER_CLIENT_ID
    NAVER_CLIENT_SECRET = config.NAVER_CLIENT_SECRET
except ImportError:
    NAVER_CLIENT_ID = None
    NAVER_CLIENT_SECRET = None

# ===========================
# ë¡œê¹… ì„¤ì •
# ===========================

class ColoredFormatter(logging.Formatter):
    """ì»¬ëŸ¬ ë¡œê¹… í¬ë§·í„°"""
    
    COLORS = {
        'DEBUG': '\033[94m',    # íŒŒë€ìƒ‰
        'INFO': '\033[92m',     # ì´ˆë¡ìƒ‰
        'WARNING': '\033[93m',  # ë…¸ë€ìƒ‰
        'ERROR': '\033[91m',    # ë¹¨ê°„ìƒ‰
        'CRITICAL': '\033[95m', # ë³´ë¼ìƒ‰
        'RESET': '\033[0m'
    }
    
    def format(self, record):
        log_color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
        record.levelname = f"{log_color}{record.levelname}{self.COLORS['RESET']}"
        return super().format(record)

logging.basicConfig(
    level=logging.INFO,  # v7.2: DEBUG â†’ INFOë¡œ ë³€ê²½
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)
logger.propagate = False

# Handler ì„¤ì •
if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(ColoredFormatter('%(asctime)s - %(levelname)s - %(message)s'))
    logger.addHandler(console_handler)
else:
    logger.handlers[0].setFormatter(ColoredFormatter('%(asctime)s - %(levelname)s - %(message)s'))

# ===========================
# ì„¤ì •
# ===========================

# ê²€ìƒ‰ í‚¤ì›Œë“œ
SEARCH_KEYWORDS = [
    "í”¼ì— ì¸í„°ë‚´ì…”ë„", "PMì¸í„°ë‚´ì…”ë„", "PM-International",
    "í•ë¼ì¸", "FitLine", "í”¼íŠ¸ë¼ì¸",
    "íŒ€íŒŒíŠ¸ë„ˆ", "PMì½”ë¦¬ì•„"
]

# í•„ìˆ˜ í¬í•¨ í‚¤ì›Œë“œ (v7.2: PM ë¸Œëœë“œ)
PM_BRAND_KEYWORDS = ["í”¼ì— ", "í”¼ì— ì¸í„°ë‚´ì…”ë„", "PM", "PMì¸í„°ë‚´ì…”ë„", "PM-International"]

# í•„ìˆ˜ í¬í•¨ í‚¤ì›Œë“œ (v7.2: íŒë§¤ì› í™œë™ í‚¤ì›Œë“œë§Œ í•„ìˆ˜)
PM_SALES_KEYWORDS = ["ì¶”ì²œì¸", "íŒŒíŠ¸ë„ˆë²ˆí˜¸", "íšŒì›ë²ˆí˜¸", "íŒŒíŠ¸ë„ˆID", "ë“±ë¡ë²ˆí˜¸", "í›„ì›ì¸"]

# ì œì™¸ í‚¤ì›Œë“œ
EXCLUDE_KEYWORDS = [
    "ë‰´ìŠ¤", "ê¸°ì‚¬", "ë³´ë„", "ê³µì§€",
    "ì•„ì¹´ë°ë¯¸", "ì„¸ë¯¸ë‚˜", "íŒ½ì°½íƒ±í¬", "ë°°ê´€"
]

# ìˆ˜ì§‘ ì„¤ì •
POSTS_PER_KEYWORD = 20
TOTAL_TARGET = 100
MAX_SEARCH_RESULTS = 50

# íƒ€ì´ë° ì„¤ì •
SELENIUM_WAIT_TIMEOUT = 10
IFRAME_WAIT_TIMEOUT = 15
REQUEST_DELAY_MIN = 1.5
REQUEST_DELAY_MAX = 3.0

# ë“œë¼ì´ë²„ ì¬ì‹œì‘ ì„¤ì • (v7.2: ìƒˆë¡œ ì¶”ê°€)
DRIVER_RESTART_INTERVAL = 30  # 30ê°œë§ˆë‹¤ ì¬ì‹œì‘
MAX_CONSECUTIVE_ERRORS = 5    # ì—°ì† 5ë²ˆ ì—ëŸ¬ ì‹œ ì¬ì‹œì‘

# ===========================
# ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤
# ===========================

class CrawlStats:
    """í¬ë¡¤ë§ í†µê³„ (v7.2: ì„±ê³µë¥  ì¶”ê°€)"""
    def __init__(self):
        self.total_searched = 0
        self.total_collected = 0
        self.total_duplicates = 0
        self.total_filtered = 0
        self.total_errors = 0
        self.start_time = time.time()
    
    def add_success(self):
        self.total_collected += 1
    
    def add_duplicate(self):
        self.total_duplicates += 1
    
    def add_filtered(self):
        self.total_filtered += 1
    
    def add_error(self):
        self.total_errors += 1
    
    def get_success_rate(self) -> float:
        """ì„±ê³µë¥  ê³„ì‚°"""
        total_attempts = self.total_collected + self.total_filtered + self.total_errors
        if total_attempts == 0:
            return 0.0
        return (self.total_collected / total_attempts) * 100
    
    def print_stats(self):
        elapsed = time.time() - self.start_time
        success_rate = self.get_success_rate()
        
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ“Š í¬ë¡¤ë§ í†µê³„")
        logger.info(f"{'='*70}")
        logger.info(f"âœ… ìˆ˜ì§‘ ì„±ê³µ: {self.total_collected}ê°œ")
        logger.info(f"ğŸ”„ ì¤‘ë³µ ì œê±°: {self.total_duplicates}ê°œ")
        logger.info(f"ğŸš« í•„í„°ë§: {self.total_filtered}ê°œ")
        logger.info(f"âŒ ì˜¤ë¥˜: {self.total_errors}ê°œ")
        logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")
        logger.info(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ ({elapsed/60:.1f}ë¶„)")
        if self.total_collected > 0:
            logger.info(f"âš¡ í‰ê·  ìˆ˜ì§‘ ì‹œê°„: {elapsed/self.total_collected:.1f}ì´ˆ/ê°œ")
        logger.info(f"{'='*70}\n")

class FailedURLManager:
    """ì‹¤íŒ¨í•œ URL ê´€ë¦¬"""
    def __init__(self):
        self.failed_urls: List[Dict] = []
    
    def add_failed_url(self, url: str, reason: str, error: str = ""):
        self.failed_urls.append({
            'url': url,
            'reason': reason,
            'error': str(error)[:500],  # ì—ëŸ¬ ë©”ì‹œì§€ 500ìë¡œ ì œí•œ
            'timestamp': datetime.now().isoformat()
        })
    
    def save_to_file(self, filename: str = 'failed_urls.json'):
        if self.failed_urls:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.failed_urls, f, ensure_ascii=False, indent=2)
    
    def get_failed_count(self) -> int:
        return len(self.failed_urls)

# ===========================
# í—¬í¼ í•¨ìˆ˜
# ===========================

def normalize_blog_url(blog_id: str, post_id: str) -> str:
    """ë¸”ë¡œê·¸ URL ì •ê·œí™”"""
    return f"https://blog.naver.com/{blog_id}/{post_id}"

def generate_post_fingerprint(post_data: Dict) -> str:
    """ê²Œì‹œë¬¼ ê³ ìœ  ì§€ë¬¸ ìƒì„±"""
    title = post_data.get('title', '')
    content = post_data.get('content', '')[:200]
    return f"{title}_{content}"

def extract_blog_info_from_url(url: str) -> Optional[Tuple[str, str]]:
    """URLì—ì„œ blog_idì™€ post_id ì¶”ì¶œ
    
    ì§€ì› í˜•ì‹:
    - https://blog.naver.com/blog_id/post_id
    - https://blog.naver.com/PostView.nhn?blogId=blog_id&logNo=post_id
    - https://m.blog.naver.com/blog_id/post_id
    """
    try:
        parsed_url = urlparse(url)
        
        # í˜•ì‹ 1: /blog_id/post_id
        path_match = re.match(r'/([^/]+)/(\d+)', parsed_url.path)
        if path_match:
            return path_match.group(1), path_match.group(2)
        
        # í˜•ì‹ 2: PostView.nhn?blogId=...&logNo=...
        if 'PostView' in parsed_url.path:
            query_params = parse_qs(parsed_url.query)
            blog_id = query_params.get('blogId', [None])[0]
            post_id = query_params.get('logNo', [None])[0]
            if blog_id and post_id:
                return blog_id, post_id
        
        return None
    except Exception as e:
        logger.debug(f"URL íŒŒì‹± ì‹¤íŒ¨: {url} - {str(e)}")
        return None

def parse_korean_datetime(date_str: str) -> Optional[str]:
    """í•œêµ­ì–´ ë‚ ì§œ ë¬¸ìì—´ì„ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (v7.2: ì‹œê°„ í¬í•¨)
    
    ì˜ˆ: '2024. 11. 5. 14:30' -> '2024-11-05 14:30:00'
    """
    try:
        # ë¶ˆí•„ìš”í•œ ê³µë°± ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        date_str = re.sub(r'\s+', ' ', date_str.strip())
        
        # íŒ¨í„´ 1: YYYY. MM. DD. HH:MM
        match = re.search(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.\s*(\d{1,2}):(\d{2})', date_str)
        if match:
            year, month, day, hour, minute = match.groups()
            return f"{year}-{int(month):02d}-{int(day):02d} {int(hour):02d}:{int(minute):02d}:00"
        
        # íŒ¨í„´ 2: YYYY. MM. DD. (ì‹œê°„ ì—†ìŒ)
        match = re.search(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.', date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{int(month):02d}-{int(day):02d} 00:00:00"
        
        # íŒ¨í„´ 3: YYYY-MM-DD HH:MM
        match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})\s+(\d{1,2}):(\d{2})', date_str)
        if match:
            year, month, day, hour, minute = match.groups()
            return f"{year}-{int(month):02d}-{int(day):02d} {int(hour):02d}:{int(minute):02d}:00"
        
        # íŒ¨í„´ 4: YYYY-MM-DD (ì‹œê°„ ì—†ìŒ)
        match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{int(month):02d}-{int(day):02d} 00:00:00"
        
        return None
    except Exception as e:
        logger.debug(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str} - {str(e)}")
        return None

def extract_sponsor_phone(text: str) -> str:
    """ì¶”ì²œì¸ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ (010-xxxx-xxxx í˜•ì‹ë§Œ)"""
    if not text:
        return ""
    
    # 010-xxxx-xxxx í˜•ì‹ë§Œ ì¶”ì¶œ (ì •í™•íˆ 11ìë¦¬)
    pattern = r'010[-\s]?\d{4}[-\s]?\d{4}'
    matches = re.findall(pattern, text)
    
    if matches:
        # í•˜ì´í”ˆ ì •ê·œí™”
        phone = re.sub(r'\s', '', matches[0])
        phone = re.sub(r'(\d{3})(\d{4})(\d{4})', r'\1-\2-\3', phone)
        return phone
    
    return ""

def extract_sponsor_partner_id(text: str) -> str:
    """ì¶”ì²œì¸ íŒŒíŠ¸ë„ˆ ID ì¶”ì¶œ (ì •í™•íˆ 8ìë¦¬ ìˆ«ìë§Œ)"""
    if not text:
        return ""
    
    # ì •í™•íˆ 8ìë¦¬ ìˆ«ìë§Œ (ì•ë’¤ì— ìˆ«ìê°€ ì—†ì–´ì•¼ í•¨)
    pattern = r'(?<!\d)(\d{8})(?!\d)'
    matches = re.findall(pattern, text)
    
    if matches:
        return matches[0]
    
    return ""

def has_eight_digit_number(text: str) -> bool:
    """í…ìŠ¤íŠ¸ì— 8ìë¦¬ ìˆ«ìê°€ ìˆëŠ”ì§€ í™•ì¸"""
    if not text:
        return False
    pattern = r'(?<!\d)(\d{8})(?!\d)'
    return bool(re.search(pattern, text))

def has_sales_keyword_in_snippet(snippet: str) -> bool:
    """ê²€ìƒ‰ ìŠ¤ë‹ˆí«ì— íŒë§¤ì› í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸ (v7.2: ìƒˆë¡œ ì¶”ê°€)"""
    if not snippet:
        return False
    return any(keyword in snippet for keyword in PM_SALES_KEYWORDS)

def extract_hashtags(soup: BeautifulSoup, content_text: str) -> str:
    """í•´ì‹œíƒœê·¸ ì¶”ì¶œ (# ê¸°í˜¸ í¬í•¨, ë³¸ë¬¸ ë‚´ í•´ì‹œíƒœê·¸ë„ ì¶”ì¶œ)"""
    hashtags_set = set()
    
    # ë°©ë²• 1: HTML íƒœê·¸ì—ì„œ ì¶”ì¶œ
    tag_selectors = [
        'a.link_tag',
        'a[href*="tag"]',
        'span.ell',
        'div.post_tag a',
        'div.tag_area a'
    ]
    
    for selector in tag_selectors:
        tags = soup.select(selector)
        for tag in tags:
            tag_text = tag.get_text(strip=True)
            if tag_text and not tag_text.startswith('#'):
                tag_text = f"#{tag_text}"
            if tag_text:
                hashtags_set.add(tag_text)
    
    # ë°©ë²• 2: ë³¸ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ # íŒ¨í„´ ì¶”ì¶œ
    if content_text:
        # #ë¡œ ì‹œì‘í•˜ê³  ê³µë°±ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ì ì „ê¹Œì§€
        hashtag_pattern = r'#[^\s#,ØŒ]+' 
        matches = re.findall(hashtag_pattern, content_text)
        for match in matches:
            # ëì˜ íŠ¹ìˆ˜ë¬¸ì ì œê±°
            cleaned = re.sub(r'[.,!?;:\)]+$', '', match)
            if len(cleaned) > 1:  # # ë‹¤ìŒì— ë¬¸ìê°€ ìˆëŠ” ê²½ìš°ë§Œ
                hashtags_set.add(cleaned)
    
    return ', '.join(sorted(hashtags_set)) if hashtags_set else ""

def extract_like_count(driver, soup) -> int:
    """ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ"""
    try:
        # Seleniumìœ¼ë¡œ ì‹œë„
        like_selectors = [
            'span.u_cnt._count',
            'span.cnt_like',
            'em.u_cnt',
            'span.like_count',
            'a.btn_empathy span.u_cnt',
            'div.end_btn span.u_cnt',
            'span#printLog'
        ]
        
        for selector in like_selectors:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                for element in elements:
                    text = element.text.strip()
                    if text and text.isdigit():
                        return int(text)
            except:
                continue
        
        # BeautifulSoupë¡œ ì‹œë„
        for selector in like_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text(strip=True)
                if text and text.isdigit():
                    return int(text)
        
        # í…ìŠ¤íŠ¸ì—ì„œ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ
        page_text = soup.get_text()
        like_patterns = [
            r'ì¢‹ì•„ìš”\s*(\d+)',
            r'ê³µê°\s*(\d+)',
            r'empathy.*?(\d+)'
        ]
        for pattern in like_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                return int(match.group(1))
        
        return 0
    except Exception as e:
        logger.debug(f"ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return 0

def extract_comment_count(driver, soup) -> int:
    """ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ"""
    try:
        # Seleniumìœ¼ë¡œ ì‹œë„
        comment_selectors = [
            'span.u_cnt._count.ccmtcnt',
            'span.cnt_cmt',
            'em.u_cmt',
            'span.comment_count',
            'a.btn_comment span.u_cnt',
            'div.end_btn span.u_cnt',
            'span.num'
        ]
        
        for selector in comment_selectors:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                for element in elements:
                    text = element.text.strip()
                    # ëŒ“ê¸€ ë²„íŠ¼ì— 'ëŒ“ê¸€ N' í˜•ì‹ì¼ ìˆ˜ ìˆìŒ
                    number_match = re.search(r'\d+', text)
                    if number_match:
                        return int(number_match.group())
            except:
                continue
        
        # BeautifulSoupë¡œ ì‹œë„
        for selector in comment_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text(strip=True)
                number_match = re.search(r'\d+', text)
                if number_match:
                    return int(number_match.group())
        
        # í…ìŠ¤íŠ¸ì—ì„œ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ
        page_text = soup.get_text()
        comment_patterns = [
            r'ëŒ“ê¸€\s*(\d+)',
            r'ì½”ë©˜íŠ¸\s*(\d+)',
            r'comment.*?(\d+)'
        ]
        for pattern in comment_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                return int(match.group(1))
        
        return 0
    except Exception as e:
        logger.debug(f"ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return 0

def content_passes_filter(title: str, content: str, full_text: str) -> Tuple[bool, str]:
    """ì½˜í…ì¸  í•„í„°ë§ (v7.2: ì™„í™”ëœ ë¡œì§)
    
    ê·œì¹™:
    1. ["í”¼ì— ", "í”¼ì— ì¸í„°ë‚´ì…”ë„"] ì¤‘ í•˜ë‚˜ ë°˜ë“œì‹œ ì¡´ì¬
    2. ["ì¶”ì²œì¸" í‚¤ì›Œë“œ] ë°˜ë“œì‹œ ì¡´ì¬ (8ìë¦¬ëŠ” ì„ íƒ)
    3. ì œì™¸ í‚¤ì›Œë“œ ì¤‘ ë‘˜ ì´ìƒ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì œì™¸
    
    Returns:
        (í†µê³¼ì—¬ë¶€, ì‹¤íŒ¨ì‚¬ìœ )
    """
    # ì „ì²´ í…ìŠ¤íŠ¸ ì¤€ë¹„
    text_lower = full_text.lower()
    
    # ê·œì¹™ 1: PM ë¸Œëœë“œ í‚¤ì›Œë“œ ì²´í¬
    has_pm_keyword = any(keyword.lower() in text_lower for keyword in PM_BRAND_KEYWORDS)
    if not has_pm_keyword:
        return False, "PM ë¸Œëœë“œ í‚¤ì›Œë“œ ì—†ìŒ"
    
    # ê·œì¹™ 2: íŒë§¤ì› í™œë™ í‚¤ì›Œë“œ ì²´í¬ (v7.2: 8ìë¦¬ëŠ” ì„ íƒ)
    has_sales_keyword = any(keyword in full_text for keyword in PM_SALES_KEYWORDS)
    
    if not has_sales_keyword:
        return False, "íŒë§¤ì› ê´€ë ¨ í‚¤ì›Œë“œ ì—†ìŒ"
    
    # ê·œì¹™ 3: ì œì™¸ í‚¤ì›Œë“œ ì²´í¬ (ë‘˜ ì´ìƒ ìˆìœ¼ë©´ ì œì™¸)
    exclude_count = sum(1 for keyword in EXCLUDE_KEYWORDS if keyword in full_text)
    if exclude_count >= 2:
        return False, f"ì œì™¸ í‚¤ì›Œë“œ {exclude_count}ê°œ ë°œê²¬"
    
    return True, ""

# ===========================
# í¬ë¡¤ë§ í•¨ìˆ˜
# ===========================

def setup_driver() -> webdriver.Chrome:
    """Selenium ë“œë¼ì´ë²„ ì„¤ì • (v7.2: ì•ˆì •ì„± ê°•í™”)"""
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # v7.2: ë©”ëª¨ë¦¬ ìµœì í™”
    chrome_options.add_argument('--disable-extensions')
    chrome_options.add_argument('--disable-plugins')
    chrome_options.add_argument('--disable-images')  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
    
    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    chrome_options.add_argument(f'user-agent={user_agent}')
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    # ìë™í™” ê°ì§€ ìš°íšŒ
    driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
        'source': '''
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            })
        '''
    })
    
    return driver

def search_naver_blog(keyword: str, max_results: int = MAX_SEARCH_RESULTS) -> List[Dict]:
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ (v7.2: ìŠ¤ë‹ˆí« ì‚¬ì „ í•„í„°ë§ ì¶”ê°€)"""
    results = []
    
    try:
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ API ì—”ë“œí¬ì¸íŠ¸
        base_url = "https://s.search.naver.com/p/blog/search.naver"
        
        params = {
            'where': 'blog',
            'query': keyword,
            'start': 1,
            'display': max_results
        }
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://www.naver.com/'
        }
        
        response = requests.get(base_url, params=params, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
        items = soup.select('li.bx, div.total_wrap')
        
        for item in items[:max_results]:
            try:
                # URL ì¶”ì¶œ
                link_elem = item.select_one('a.title_link, a.api_txt_lines')
                if not link_elem:
                    continue
                
                url = link_elem.get('href', '')
                if not url or 'blog.naver.com' not in url:
                    continue
                
                # blog_idì™€ post_id ì¶”ì¶œ
                blog_info = extract_blog_info_from_url(url)
                if not blog_info:
                    continue
                
                blog_id, post_id = blog_info
                
                # v7.2: ê²€ìƒ‰ ìŠ¤ë‹ˆí«ì—ì„œ ì‚¬ì „ í•„í„°ë§
                snippet_elem = item.select_one('a.api_txt_lines, div.api_txt_lines')
                snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                
                # íŒë§¤ì› í‚¤ì›Œë“œê°€ ìŠ¤ë‹ˆí«ì— ìˆëŠ”ì§€ í™•ì¸ (ì„ íƒì )
                has_sales_hint = has_sales_keyword_in_snippet(snippet)
                
                results.append({
                    'url': url,
                    'blog_id': blog_id,
                    'post_id': post_id,
                    'snippet': snippet,
                    'has_sales_hint': has_sales_hint  # ìš°ì„ ìˆœìœ„ìš©
                })
            
            except Exception as e:
                logger.debug(f"ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                continue
        
        # v7.2: íŒë§¤ì› í‚¤ì›Œë“œ ìˆëŠ” ê²ƒ ìš°ì„  ì •ë ¬
        results.sort(key=lambda x: x['has_sales_hint'], reverse=True)
        
        logger.info(f"ğŸ” '{keyword}' ê²€ìƒ‰: {len(results)}ê°œ ë°œê²¬")
        return results
    
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {str(e)}")
        return []

def crawl_blog_post_selenium(
    driver: webdriver.Chrome,
    url: str,
    blog_id: str,
    post_id: str,
    failed_url_manager: FailedURLManager
) -> Optional[Dict]:
    """Seleniumì„ ì‚¬ìš©í•œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ í¬ë¡¤ë§"""
    
    try:
        driver.get(url)
        time.sleep(2)
        
        # iframe ëŒ€ê¸° ë° ì „í™˜
        try:
            WebDriverWait(driver, IFRAME_WAIT_TIMEOUT).until(
                EC.frame_to_be_available_and_switch_to_it((By.ID, "mainFrame"))
            )
        except TimeoutException:
            logger.warning(f"âš ï¸  iframe ë¡œë“œ ì‹¤íŒ¨: {url}")
            failed_url_manager.add_failed_url(url, "iframe_timeout")
            return None
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        WebDriverWait(driver, SELENIUM_WAIT_TIMEOUT).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        # HTML íŒŒì‹±
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ
        title = ""
        title_selectors = [
            'div.se-title-text',
            'span.se-fs-',
            'div.se-component-content',
            'h3.se_textarea'
        ]
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                if title:
                    break
        
        if not title:
            title_elem = soup.find(['h1', 'h2', 'h3'])
            if title_elem:
                title = title_elem.get_text(strip=True)
        
        if not title:
            logger.warning(f"âš ï¸  ì œëª© ì—†ìŒ: {url}")
            failed_url_manager.add_failed_url(url, "no_title")
            driver.switch_to.default_content()
            return None
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        content = ""
        content_selectors = [
            'div.se-main-container',
            'div#postViewArea',
            'div.se-component-content',
            'div.post-view',
            'div.__se_component_area'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = content_elem.get_text(separator=' ', strip=True)
                if len(content) > 50:
                    break
        
        if not content or len(content) < 20:
            logger.warning(f"âš ï¸  ë³¸ë¬¸ ì—†ìŒ ë˜ëŠ” ë„ˆë¬´ ì§§ìŒ: {url}")
            failed_url_manager.add_failed_url(url, "no_content")
            driver.switch_to.default_content()
            return None
        
        # ì „ì²´ í…ìŠ¤íŠ¸ (í•„í„°ë§ìš©)
        full_text = f"{title} {content}"
        
        # ì½˜í…ì¸  í•„í„°ë§
        passes, reason = content_passes_filter(title, content, full_text)
        if not passes:
            logger.debug(f"ğŸš« í•„í„°ë§: {title[:30]}... ({reason})")
            driver.switch_to.default_content()
            return None
        
        # v7.2: ë‚ ì§œ+ì‹œê°„ ì¶”ì¶œ
        published_datetime = ""
        date_selectors = [
            'span.se_publishDate',
            'span.date',
            'p.date',
            'div.post_date',
            'span.p_date'
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                published_datetime = parse_korean_datetime(date_text)
                if published_datetime:
                    break
        
        # ì¶”ì²œì¸ ì •ë³´ ì¶”ì¶œ
        sponsor_phone = extract_sponsor_phone(full_text)
        sponsor_partner_id = extract_sponsor_partner_id(full_text)
        
        # ì¢‹ì•„ìš”/ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ
        like_count = extract_like_count(driver, soup)
        comment_count = extract_comment_count(driver, soup)
        
        # í•´ì‹œíƒœê·¸ ì¶”ì¶œ
        hashtags = extract_hashtags(soup, content)
        
        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
        image_urls = []
        img_tags = soup.select('img.se-image-resource, img.__se_img_el, div.se-component-content img')
        for img in img_tags[:10]:  # ìµœëŒ€ 10ê°œ
            src = img.get('src') or img.get('data-src')
            if src and src.startswith('http'):
                image_urls.append(src)
        
        # ë¹„ë””ì˜¤ URL ì¶”ì¶œ
        video_urls = []
        video_tags = soup.select('video source, iframe[src*="youtube"], iframe[src*="youtu.be"]')
        for video in video_tags[:5]:  # ìµœëŒ€ 5ê°œ
            src = video.get('src')
            if src and src.startswith('http'):
                video_urls.append(src)
        
        # iframeì—ì„œ ë‚˜ê°€ê¸°
        driver.switch_to.default_content()
        
        # ë°ì´í„° êµ¬ì„±
        post_data = {
            'platform': 'naver_blog',
            'post_id': f"{blog_id}_{post_id}",
            'blog_id': blog_id,
            'url': url,
            'title': title,
            'content': content,
            'published_datetime': published_datetime,  # v7.2: ì‹œê°„ í¬í•¨
            'sponsor_phone': sponsor_phone,
            'sponsor_partner_id': sponsor_partner_id,
            'like_count': like_count,
            'comment_count': comment_count,
            'hashtags': hashtags,
            'image_urls': ', '.join(image_urls) if image_urls else "",
            'video_urls': ', '.join(video_urls) if video_urls else "",
            'collected_date': datetime.now().strftime('%Y-%m-%d')
        }
        
        return post_data
    
    except TimeoutException:
        logger.warning(f"âš ï¸  íƒ€ì„ì•„ì›ƒ: {url}")
        failed_url_manager.add_failed_url(url, "timeout")
        try:
            driver.switch_to.default_content()
        except:
            pass
        return None
    
    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {url} - {str(e)[:200]}")
        failed_url_manager.add_failed_url(url, "crawl_error", str(e))
        try:
            driver.switch_to.default_content()
        except:
            pass
        return None

# ===========================
# ë©”ì¸ í•¨ìˆ˜
# ===========================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (v7.2: ë“œë¼ì´ë²„ ì¬ì‹œì‘ ë¡œì§ ì¶”ê°€)"""
    logger.info("="*70)
    logger.info("ğŸš€ PM International ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v7.2 ì‹œì‘")
    logger.info("="*70)
    
    # ì´ˆê¸°í™”
    driver = setup_driver()
    stats = CrawlStats()
    failed_url_manager = FailedURLManager()
    
    collected_posts = []
    collected_urls: Set[str] = set()
    collected_fingerprints: Set[str] = set()
    
    consecutive_errors = 0  # v7.2: ì—°ì† ì—ëŸ¬ ì¹´ìš´í„°
    crawl_count = 0  # v7.2: í¬ë¡¤ë§ ì¹´ìš´í„°
    
    try:
        for keyword in SEARCH_KEYWORDS:
            if len(collected_posts) >= TOTAL_TARGET:
                logger.info(f"âœ… ëª©í‘œ ë‹¬ì„± ({TOTAL_TARGET}ê°œ)")
                break
            
            logger.info(f"\nğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰: '{keyword}'")
            
            # ê²€ìƒ‰
            search_results = search_naver_blog(keyword, MAX_SEARCH_RESULTS)
            if not search_results:
                logger.warning(f"âš ï¸  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {keyword}")
                continue
            
            # í¬ë¡¤ë§
            for result in search_results:
                if len(collected_posts) >= TOTAL_TARGET:
                    break
                
                # v7.2: ì£¼ê¸°ì  ë“œë¼ì´ë²„ ì¬ì‹œì‘ (ì•ˆì •ì„± í–¥ìƒ)
                if crawl_count > 0 and crawl_count % DRIVER_RESTART_INTERVAL == 0:
                    logger.info(f"ğŸ”„ ë“œë¼ì´ë²„ ì¬ì‹œì‘ ({crawl_count}ê°œ ì²˜ë¦¬)")
                    driver.quit()
                    time.sleep(2)
                    driver = setup_driver()
                    consecutive_errors = 0
                    gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                
                blog_id = result['blog_id']
                post_id = result['post_id']
                normalized_url = normalize_blog_url(blog_id, post_id)
                
                # ì¤‘ë³µ ì²´í¬
                if normalized_url in collected_urls:
                    stats.add_duplicate()
                    continue
                
                logger.info(f"[{len(collected_posts)+1}/{TOTAL_TARGET}] í¬ë¡¤ë§ ì¤‘... (ì„±ê³µë¥ : {stats.get_success_rate():.1f}%)")
                
                # í¬ë¡¤ë§ ì‹¤í–‰
                try:
                    post_data = crawl_blog_post_selenium(
                        driver, normalized_url, blog_id, post_id, failed_url_manager
                    )
                    
                    if post_data:
                        # ì¤‘ë³µ ì²´í¬ (ì§€ë¬¸ ê¸°ë°˜)
                        fingerprint = generate_post_fingerprint(post_data)
                        if fingerprint not in collected_fingerprints:
                            collected_posts.append(post_data)
                            collected_urls.add(normalized_url)
                            collected_fingerprints.add(fingerprint)
                            stats.add_success()
                            consecutive_errors = 0  # ì„±ê³µ ì‹œ ì—ëŸ¬ ì¹´ìš´í„° ë¦¬ì…‹
                            logger.info(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {post_data['title'][:50]}...")
                        else:
                            stats.add_duplicate()
                            logger.debug(f"ğŸ”„ ì¤‘ë³µ (ì§€ë¬¸): {post_data['title'][:30]}...")
                    else:
                        stats.add_filtered()
                        consecutive_errors += 1
                
                except WebDriverException as e:
                    # v7.2: invalid session id ì—ëŸ¬ ì²˜ë¦¬
                    if 'invalid session id' in str(e).lower():
                        logger.warning(f"âš ï¸  ì„¸ì…˜ ì˜¤ë¥˜ ê°ì§€ - ë“œë¼ì´ë²„ ì¬ì‹œì‘")
                        driver.quit()
                        time.sleep(2)
                        driver = setup_driver()
                        consecutive_errors = 0
                        continue
                    else:
                        stats.add_error()
                        consecutive_errors += 1
                
                except Exception as e:
                    logger.error(f"âŒ ì˜ˆì™¸ ë°œìƒ: {str(e)[:200]}")
                    stats.add_error()
                    consecutive_errors += 1
                
                # v7.2: ì—°ì† ì—ëŸ¬ ì‹œ ë“œë¼ì´ë²„ ì¬ì‹œì‘
                if consecutive_errors >= MAX_CONSECUTIVE_ERRORS:
                    logger.warning(f"âš ï¸  ì—°ì† {MAX_CONSECUTIVE_ERRORS}íšŒ ì—ëŸ¬ - ë“œë¼ì´ë²„ ì¬ì‹œì‘")
                    driver.quit()
                    time.sleep(3)
                    driver = setup_driver()
                    consecutive_errors = 0
                    gc.collect()
                
                crawl_count += 1
                
                # ìš”ì²­ ê°„ ì§€ì—°
                time.sleep(random.uniform(REQUEST_DELAY_MIN, REQUEST_DELAY_MAX))
            
            # í‚¤ì›Œë“œ ê°„ ëŒ€ê¸°
            if len(collected_posts) < TOTAL_TARGET:
                time.sleep(random.uniform(2, 4))
        
        # í†µê³„ ì¶œë ¥
        stats.print_stats()
        
        # CSVë¡œ ì €ì¥
        if collected_posts:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'naver_blog_pm_v7_2_{timestamp}.csv'
            
            df = pd.DataFrame(collected_posts)
            
            # ì»¬ëŸ¼ ìˆœì„œ ì •ì˜
            column_order = [
                'platform', 'post_id', 'blog_id', 'url', 'title', 'content',
                'published_datetime', 'sponsor_phone', 'sponsor_partner_id',
                'like_count', 'comment_count', 'hashtags', 'image_urls',
                'video_urls', 'collected_date'
            ]
            
            df = df[column_order]
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            logger.info(f"\n{'='*70}")
            logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
            logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(collected_posts)}ê°œ")
            logger.info(f"ğŸ“‹ ì»¬ëŸ¼: {len(column_order)}ê°œ")
            logger.info(f"{'='*70}")
        else:
            logger.warning("âš ï¸  ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì‹¤íŒ¨ URL ì €ì¥
        failed_url_manager.save_to_file()
        if failed_url_manager.get_failed_count() > 0:
            logger.info(f"âŒ ì‹¤íŒ¨ URL: {failed_url_manager.get_failed_count()}ê°œ (failed_urls.json)")
    
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  ì‚¬ìš©ì ì¤‘ë‹¨")
        stats.print_stats()
    
    except Exception as e:
        logger.error(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        stats.print_stats()
    
    finally:
        driver.quit()
        logger.info("âœ… ë“œë¼ì´ë²„ ì¢…ë£Œ")
        logger.info("="*70)
        logger.info("ğŸ í¬ë¡¤ë§ ì™„ë£Œ")
        logger.info("="*70)

if __name__ == "__main__":
    main()
