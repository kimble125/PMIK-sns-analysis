#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v6.3 - í…ŒìŠ¤íŠ¸ ë²„ì „ (ì†ŒëŸ‰ ìˆ˜ì§‘)
========================================================

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. âœ… í•´ì‹œíƒœê·¸ ì¶”ì¶œ ê°œì„  (ì •ê·œì‹ + CSS ì„ íƒì ì¡°í•©)
2. âœ… PM í‚¤ì›Œë“œ í•„í„°ë§ ì¶”ê°€ (ë¬´ê´€í•œ ë°ì´í„° ì œê±°)
3. âœ… ì°¸ì—¬ ì§€í‘œ ìˆ˜ì§‘ ê°œì„  (ë‹¤ì¤‘ ì„ íƒì)
4. âœ… ì¶”ì²œì¸ ì •ë³´ íŒ¨í„´ ê°œì„ 
5. âœ… íŒŒíŠ¸ë„ˆ ë²ˆí˜¸ íŒ¨í„´ ìˆ˜ì • (ìˆ«ìë§Œ)
6. âœ… ì¹´ì¹´ì˜¤ ID ì œê±°
7. âœ… Image/Video URLì— Referer í—¤ë” ì¶”ê°€
8. âœ… ì§„í–‰ ìƒí™© ìë™ ì €ì¥

í…ŒìŠ¤íŠ¸ ì„¤ì •:
- í•´ì‹œíƒœê·¸ë‹¹ ìµœëŒ€ 10ê°œì”©ë§Œ ìˆ˜ì§‘ (ì´ ~170ê°œ URL)
- ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 10-15ë¶„

ì‘ì„±ì: PMIì½”ë¦¬ì•„ ë°ì´í„° íŒ€
ë²„ì „: 6.3-test
ìµœì¢… ìˆ˜ì •: 2025-11-03
"""

import os
import time
import json
import re
from datetime import datetime
from typing import List, Dict, Optional

# ì›¹ í¬ë¡¤ë§
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, UnexpectedAlertPresentException
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# ë°ì´í„° ì²˜ë¦¬
import pandas as pd
from tqdm import tqdm

# ============================================================
# ì„¤ì •
# ============================================================

def load_api_credentials():
    """API ì¸ì¦ ì •ë³´ ë¡œë“œ"""
    config_path = os.path.join(os.path.dirname(__file__), 'config.py')
    if os.path.exists(config_path):
        try:
            import importlib.util
            spec = importlib.util.spec_from_file_location("config", config_path)
            config = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(config)
            
            client_id = getattr(config, 'NAVER_CLIENT_ID', None)
            client_secret = getattr(config, 'NAVER_CLIENT_SECRET', None)
            
            if client_id and client_secret:
                print("âœ“ config.pyì—ì„œ API í‚¤ ë¡œë“œ ì„±ê³µ")
                return client_id, client_secret
        except Exception as e:
            print(f"âš  config.py ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    raise ValueError("API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")

NAVER_CLIENT_ID, NAVER_CLIENT_SECRET = load_api_credentials()

# íƒ€ê²Ÿ í•´ì‹œíƒœê·¸
TARGET_HASHTAGS = [
    '#í”¼ì— ì¸í„°ë‚´ì…”ë„', '#í”¼ì— ì½”ë¦¬ì•„', '#ë…ì¼í”¼ì— ', '#PMì¸í„°ë‚´ì…”ë„',
    '#í•ë¼ì¸', '#í”¼íŠ¸ë¼ì¸',
    '#ë² ì´ì‹ìŠ¤', '#ë² ì´ì§ìŠ¤', '#ë² ì´ì‹',
    '#í”„ë¡œì…°ì´í”„', '#í”„ë¡œì‰ì´í”„', '#ì—‘í‹°ë°”ì´ì¦ˆ',
    '#íŒŒì›Œì¹µí…Œì¼', '#ë¦¬ìŠ¤í† ë ˆì´íŠ¸',
    '#ë®¤ë‹ˆí‹°', '#ì˜µí‹°ë©€ì…‹', '#ì…€í”ŒëŸ¬ìŠ¤'
]

# â­ í…ŒìŠ¤íŠ¸ ì„¤ì •
MAX_RESULTS_PER_HASHTAG = 10  # í…ŒìŠ¤íŠ¸: 10ê°œì”©ë§Œ
NUM_WORKERS = 1  # í…ŒìŠ¤íŠ¸: ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤

# PM ê´€ë ¨ í‚¤ì›Œë“œ (í•„í„°ë§ìš©)
PM_KEYWORDS = [
    'í”¼ì— ', 'PM', 'ì¸í„°ë‚´ì…”ë„', 'í•ë¼ì¸', 'í”¼íŠ¸ë¼ì¸',
    'ë² ì´ì‹ìŠ¤', 'ë² ì´ì§ìŠ¤', 'ë² ì´ì‹',
    'í”„ë¡œì…°ì´í”„', 'í”„ë¡œì‰ì´í”„', 'ì—‘í‹°ë°”ì´ì¦ˆ',
    'íŒŒì›Œì¹µí…Œì¼', 'ë¦¬ìŠ¤í† ë ˆì´íŠ¸', 'ë®¤ë‹ˆí‹°', 'ì˜µí‹°ë©€ì…‹', 'ì…€í”ŒëŸ¬ìŠ¤',
    'ë…ì¼í”¼ì— ', 'FitLine', 'fitline'
]

# ì¶œë ¥ ì„¤ì •
OUTPUT_DIR = "output"
OUTPUT_CSV = f"{OUTPUT_DIR}/naver_blog_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
STATS_FILE = f"{OUTPUT_DIR}/test_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

# ============================================================
# Step 1: URL ìˆ˜ì§‘
# ============================================================

class NaverHashtagSearcher:
    def __init__(self, client_id: str, client_secret: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.base_url = "https://openapi.naver.com/v1/search/blog.json"
    
    def search_hashtag(self, hashtag: str, max_results: int = 100) -> List[Dict]:
        """í•´ì‹œíƒœê·¸ë¡œ ì§ì ‘ ê²€ìƒ‰"""
        headers = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret
        }
        
        results = []
        
        for start in range(1, min(max_results, 1000), 100):
            params = {
                "query": hashtag,
                "display": min(100, max_results),
                "start": start,
                "sort": "date"
            }
            
            try:
                response = requests.get(self.base_url, headers=headers, params=params, timeout=10)
                response.raise_for_status()
                data = response.json()
                
                items = data.get('items', [])
                if not items:
                    break
                
                for item in items:
                    results.append({
                        'title': self._clean_html(item.get('title', '')),
                        'link': item.get('link', ''),
                        'description': self._clean_html(item.get('description', '')),
                        'bloggername': item.get('bloggername', ''),
                        'bloggerlink': item.get('bloggerlink', ''),
                        'postdate': item.get('postdate', '')
                    })
                
                if len(results) >= max_results:
                    break
                
                time.sleep(0.1)
                
            except Exception as e:
                print(f"[ERROR] í•´ì‹œíƒœê·¸ '{hashtag}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                break
        
        return results[:max_results]
    
    def _clean_html(self, text: str) -> str:
        return re.sub('<.*?>', '', text)

# ============================================================
# Step 2: ë³¸ë¬¸ í¬ë¡¤ë§ (ê°œì„ ë¨)
# ============================================================

class NaverBlogCrawler:
    def __init__(self, headless: bool = True):
        options = webdriver.ChromeOptions()
        if headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-extensions')
        options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')
        options.add_argument('--log-level=3')
        
        # ChromeDriver ì„¤ì • (Homebrew ë²„ì „ ìš°ì„  ì‚¬ìš©)
        homebrew_chromedriver = '/opt/homebrew/bin/chromedriver'
        
        if os.path.exists(homebrew_chromedriver):
            # Homebrew ì„¤ì¹˜ ë²„ì „ ì‚¬ìš© (ì„œëª…ë˜ì–´ ìˆì–´ macOS ë³´ì•ˆ ë¬¸ì œ ì—†ìŒ)
            service = Service(homebrew_chromedriver)
            self.driver = webdriver.Chrome(service=service, options=options)
        else:
            # Homebrew ë²„ì „ì´ ì—†ìœ¼ë©´ ChromeDriverManager ì‚¬ìš©
            try:
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=options)
            except Exception as e:
                raise Exception(f"ChromeDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\ní„°ë¯¸ë„ì—ì„œ 'brew install --cask chromedriver' ì‹¤í–‰ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
        self.wait = WebDriverWait(self.driver, 10)
        
        # Referer í—¤ë”ë¥¼ ìœ„í•œ ì„¸ì…˜
        self.session = requests.Session()
        self.session.headers.update({
            'Referer': 'https://blog.naver.com/',
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
    
    def crawl_blog_post(self, url: str) -> Optional[Dict]:
        """ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ í¬ë¡¤ë§ (ê°œì„ ë¨)"""
        try:
            self.driver.get(url)
            time.sleep(1)
            
            # Alert ì²˜ë¦¬
            try:
                self.driver.switch_to.alert.dismiss()
                return None
            except:
                pass
            
            # iframe ì „í™˜
            try:
                iframe = self.wait.until(
                    EC.presence_of_element_located((By.ID, "mainFrame"))
                )
                self.driver.switch_to.frame(iframe)
            except TimeoutException:
                return None
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content_text = ''
            hashtags = []
            image_urls = []
            video_urls = []
            
            # ìŠ¤ë§ˆíŠ¸ì—ë””í„° ONE
            try:
                se_container = self.driver.find_element(By.CLASS_NAME, "se-main-container")
                content_text = se_container.text
                
                # ì´ë¯¸ì§€
                images = se_container.find_elements(By.TAG_NAME, "img")
                for img in images:
                    src = img.get_attribute('src')
                    if src and 'http' in src:
                        image_urls.append(src)
                
                # ë¹„ë””ì˜¤
                videos = se_container.find_elements(By.TAG_NAME, "video")
                for video in videos:
                    src = video.get_attribute('src')
                    if src:
                        video_urls.append(src)
                
                # â­ í•´ì‹œíƒœê·¸ ì¶”ì¶œ ê°œì„  (CSS + ì •ê·œì‹)
                # ë°©ë²• 1: CSS ì„ íƒì
                hashtag_elems = se_container.find_elements(By.CSS_SELECTOR, 
                    "a.se_link_hashtag, a[href*='hashtag'], a[class*='hashtag']")
                for elem in hashtag_elems:
                    tag = elem.text.strip()
                    if tag and tag.startswith('#') and tag not in hashtags:
                        hashtags.append(tag)
                
                # ë°©ë²• 2: ì •ê·œì‹ìœ¼ë¡œ ë³¸ë¬¸ì—ì„œ ì¶”ì¶œ (v5.0 ë°©ì‹)
                hashtag_pattern = r'#[ê°€-í£a-zA-Z0-9_]+'
                hashtags_from_text = re.findall(hashtag_pattern, content_text)
                for tag in hashtags_from_text:
                    if tag not in hashtags:
                        hashtags.append(tag)
            
            except:
                # ìŠ¤ë§ˆíŠ¸ì—ë””í„° 2.0
                try:
                    post_area = self.driver.find_element(By.ID, "postViewArea")
                    content_text = post_area.text
                    
                    images = post_area.find_elements(By.TAG_NAME, "img")
                    for img in images:
                        src = img.get_attribute('src')
                        if src and 'http' in src:
                            image_urls.append(src)
                    
                    # í•´ì‹œíƒœê·¸ ì •ê·œì‹ ì¶”ì¶œ
                    hashtag_pattern = r'#[ê°€-í£a-zA-Z0-9_]+'
                    hashtags = re.findall(hashtag_pattern, content_text)
                    
                except:
                    return None
            
            # iframe ë°–ìœ¼ë¡œ ë‚˜ê°€ê¸°
            self.driver.switch_to.default_content()
            
            # â­ ì°¸ì—¬ ì§€í‘œ ì¶”ì¶œ ê°œì„  (ë‹¤ì¤‘ ì„ íƒì ì‹œë„)
            view_count = self._extract_metric([
                "span.se_publishDate em",
                "span.count_view",
                "div.view_count em",
                "span.num"
            ])
            
            like_count = self._extract_metric([
                "em.u_cnt._count",
                "span.like_count em",
                "a.btn_like em"
            ])
            
            comment_count = self._extract_metric([
                "a.btn_comment em.u_cnt",
                "span.comment_count em",
                "a.cmt_count"
            ])
            
            # ì§€í‘œê°€ ì—†ìœ¼ë©´ body í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ (v5.0 ë°©ì‹)
            if not view_count or not like_count or not comment_count:
                body_text = self.driver.find_element(By.TAG_NAME, 'body').text
                if not view_count:
                    match = re.search(r'ì¡°íšŒ[ìˆ˜]?\s*([\d,]+)', body_text)
                    view_count = match.group(1).replace(',', '') if match else ''
                if not like_count:
                    match = re.search(r'ê³µê°\s*(\d+)', body_text)
                    like_count = match.group(1) if match else ''
                if not comment_count:
                    match = re.search(r'ëŒ“ê¸€\s*(\d+)', body_text)
                    comment_count = match.group(1) if match else ''
            
            return {
                'url': url,
                'title': self.driver.title,
                'content_text': content_text,
                'hashtags': hashtags,
                'image_urls': image_urls,
                'video_urls': video_urls,
                'view_count': view_count,
                'like_count': like_count,
                'comment_count': comment_count,
                'author_id': self._extract_author_id(url)
            }
        
        except UnexpectedAlertPresentException:
            try:
                self.driver.switch_to.alert.dismiss()
            except:
                pass
            return None
        
        except Exception as e:
            return None
    
    def _extract_metric(self, selectors: List[str]) -> str:
        """ì—¬ëŸ¬ ì„ íƒìë¥¼ ì‹œë„í•˜ì—¬ ì§€í‘œ ì¶”ì¶œ"""
        for selector in selectors:
            try:
                elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                text = elem.text.strip()
                if text and text.isdigit():
                    return text
            except:
                continue
        return ''
    
    def _extract_author_id(self, url: str) -> str:
        match = re.search(r'blog\.naver\.com/([^/?]+)', url)
        if match:
            return match.group(1)
        return ''
    
    def close(self):
        try:
            self.driver.quit()
        except:
            pass

# ============================================================
# Step 3: PM ê´€ë ¨ í•„í„°ë§
# ============================================================

def is_pm_related(post_data: Dict) -> bool:
    """PM ê´€ë ¨ ê²Œì‹œë¬¼ì¸ì§€ í™•ì¸"""
    if not post_data:
        return False
    
    # ì œëª©, ë³¸ë¬¸, í•´ì‹œíƒœê·¸ í•©ì¹˜ê¸°
    full_text = ' '.join([
        post_data.get('title', ''),
        post_data.get('content_text', ''),
        ' '.join(post_data.get('hashtags', []))
    ]).lower()
    
    # PM í‚¤ì›Œë“œê°€ ìµœì†Œ 1ê°œ ì´ìƒ ìˆì–´ì•¼ í•¨
    for keyword in PM_KEYWORDS:
        if keyword.lower() in full_text:
            return True
    
    return False

# ============================================================
# Step 4: ì¶”ì²œì¸ ì •ë³´ ì¶”ì¶œ (ê°œì„ ë¨)
# ============================================================

class ReferrerExtractor:
    def __init__(self):
        # â­ ê°œì„ ëœ íŒ¨í„´
        self.phone_patterns = [
            r'(?:ì—°ë½ì²˜|ì „í™”|ë¬¸ì˜|â˜|ğŸ“)[:\s]*([0-9]{2,3}[-\s]?[0-9]{3,4}[-\s]?[0-9]{4})',
            r'(01[016789][-\s]?[0-9]{3,4}[-\s]?[0-9]{4})',
        ]
        
        self.name_patterns = [
            r'(?:ì¶”ì²œì¸|ì¶”ì²œ|ì†Œê°œ)[:\s]*\(?([ê°€-í£]{2,4})\)?',  # ê´„í˜¸ ì•ˆ ì´ë¦„
            r'PM\s*(?:íŒŒíŠ¸ë„ˆ|ë§¤ë‹ˆì €)[:\s]*([ê°€-í£]{2,4})',  # PM íŒŒíŠ¸ë„ˆ í™ê¸¸ë™
            r'(?:ì—°ë½ì²˜|ì „í™”)[:\s]*([ê°€-í£]{2,4})\s*[0-9-]',  # ì—°ë½ì²˜: í™ê¸¸ë™ 010-
        ]
        
        # â­ íŒŒíŠ¸ë„ˆ ë²ˆí˜¸ íŒ¨í„´ ìˆ˜ì • (ìˆ«ìë§Œ)
        self.partner_patterns = [
            r'(?:íŒŒíŠ¸ë„ˆ\s*ë²ˆí˜¸|íŒŒíŠ¸ë„ˆ|íšŒì›\s*ë²ˆí˜¸|ë²ˆí˜¸)[:\s]*([0-9]{7,9})',  # 20577576
            r'(?:ì¶”ì²œ|ì†Œê°œ)\s*ë²ˆí˜¸[:\s]*([0-9]{7,9})',
        ]
    
    def extract_phone(self, text: str) -> str:
        if not text:
            return ''
        for pattern in self.phone_patterns:
            match = re.search(pattern, text)
            if match:
                phone = re.sub(r'[^0-9]', '', match.group(0))
                if len(phone) in [10, 11]:
                    if len(phone) == 11:
                        return f"{phone[:3]}-{phone[3:7]}-{phone[7:]}"
                    else:
                        return f"{phone[:3]}-{phone[3:6]}-{phone[6:]}"
        return ''
    
    def extract_name(self, text: str) -> str:
        if not text:
            return ''
        for pattern in self.name_patterns:
            match = re.search(pattern, text)
            if match:
                name = match.group(1)
                if re.match(r'^[ê°€-í£]{2,4}$', name):
                    # ì¼ë°˜ì ì¸ ë‹¨ì–´ ì œì™¸
                    if name not in ['ì½”ë“œ', 'ìœ¼ë¡œ', 'ì—ì„œ', 'ì—ê²Œ', 'ë¶€í„°', 'ê¹Œì§€']:
                        return name
        return ''
    
    def extract_partner_number(self, text: str) -> str:
        if not text:
            return ''
        for pattern in self.partner_patterns:
            match = re.search(pattern, text)
            if match:
                number = match.group(1)
                # 7-9ìë¦¬ ìˆ«ì í™•ì¸
                if number.isdigit() and 7 <= len(number) <= 9:
                    return number
        return ''
    
    def extract_all(self, content_text: str) -> Dict[str, str]:
        return {
            'name': self.extract_name(content_text),
            'phone': self.extract_phone(content_text),
            'partner_number': self.extract_partner_number(content_text)
        }

# ============================================================
# Step 5: ë°ì´í„° ì €ì¥
# ============================================================

def save_to_csv(posts: List[Dict], filename: str) -> pd.DataFrame:
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    extractor = ReferrerExtractor()
    
    print("\nì¶”ì²œì¸ ì •ë³´ ìë™ ì¶”ì¶œ ì¤‘...")
    extraction_stats = {'name': 0, 'phone': 0, 'partner_number': 0}
    
    rows = []
    for post in posts:
        content_text = post.get('content_text', '')
        referrer_info = extractor.extract_all(content_text)
        
        if referrer_info['name']:
            extraction_stats['name'] += 1
        if referrer_info['phone']:
            extraction_stats['phone'] += 1
        if referrer_info['partner_number']:
            extraction_stats['partner_number'] += 1
        
        row = {
            'platform': 'Naver Blog',
            'title': post.get('title', ''),
            'description': post.get('description', ''),
            'blogger_profile': post.get('blogger_profile', ''),
            'post_url': post['url'],
            'author_id': post.get('author_id', ''),
            'content_text': content_text,
            'hashtags': ', '.join(post.get('hashtags', [])),
            'postdate': post.get('post_date', ''),
            'image_count': len(post.get('image_urls', [])),
            'video_count': len(post.get('video_urls', [])),
            'image_urls': '|||'.join(post.get('image_urls', [])),
            'video_urls': '|||'.join(post.get('video_urls', [])),
            'view_count': post.get('view_count', ''),
            'like_count': post.get('like_count', ''),
            'comment_count': post.get('comment_count', ''),
            'referrer_name': referrer_info['name'],
            'referrer_phone': referrer_info['phone'],
            'partner_number': referrer_info['partner_number'],
            'collected_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        rows.append(row)
    
    print(f"âœ“ ì¶”ì²œì¸ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ:")
    print(f"  â€¢ ì´ë¦„: {extraction_stats['name']}ê°œ ({extraction_stats['name']/len(posts)*100:.1f}%)")
    print(f"  â€¢ ì „í™”ë²ˆí˜¸: {extraction_stats['phone']}ê°œ ({extraction_stats['phone']/len(posts)*100:.1f}%)")
    print(f"  â€¢ íŒŒíŠ¸ë„ˆë²ˆí˜¸: {extraction_stats['partner_number']}ê°œ ({extraction_stats['partner_number']/len(posts)*100:.1f}%)")
    
    df = pd.DataFrame(rows)
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ {len(df)}ê°œ ê²Œì‹œë¬¼ì„ {filename}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤")
    
    return df

def save_stats(stats: Dict, filename: str):
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    print(f"âœ“ í†µê³„ ì •ë³´ë¥¼ {filename}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤")

# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================

def main():
    print("=" * 70)
    print(" ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v6.3 - í…ŒìŠ¤íŠ¸ ë²„ì „")
    print("=" * 70)
    print(f"íƒ€ê²Ÿ í•´ì‹œíƒœê·¸: {len(TARGET_HASHTAGS)}ê°œ")
    print(f"í•´ì‹œíƒœê·¸ë‹¹ ìµœëŒ€ ìˆ˜ì§‘: {MAX_RESULTS_PER_HASHTAG}ê°œ")
    print(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: 10-15ë¶„")
    print("=" * 70)
    
    start_time = datetime.now()
    
    overall_stats = {
        'start_time': start_time.isoformat(),
        'version': '6.3-test',
        'config': {
            'target_hashtags': TARGET_HASHTAGS,
            'max_results_per_hashtag': MAX_RESULTS_PER_HASHTAG
        }
    }
    
    # Phase 1: URL ìˆ˜ì§‘
    print("\n[Phase 1] í•´ì‹œíƒœê·¸ ê²€ìƒ‰ìœ¼ë¡œ URL ìˆ˜ì§‘ ì¤‘...")
    
    searcher = NaverHashtagSearcher(NAVER_CLIENT_ID, NAVER_CLIENT_SECRET)
    all_blog_urls = []
    hashtag_counts = {}
    
    for hashtag in tqdm(TARGET_HASHTAGS, desc="í•´ì‹œíƒœê·¸ ê²€ìƒ‰"):
        results = searcher.search_hashtag(hashtag, max_results=MAX_RESULTS_PER_HASHTAG)
        hashtag_counts[hashtag] = len(results)
        all_blog_urls.extend(results)
    
    # ì¤‘ë³µ ì œê±°
    unique_urls = {item['link']: item for item in all_blog_urls}
    all_blog_urls = list(unique_urls.values())
    
    print(f"\nâœ“ ì´ {len(all_blog_urls)}ê°œ URL ìˆ˜ì§‘ ì™„ë£Œ (ì¤‘ë³µ ì œê±° í›„)")
    print(f"\ní•´ì‹œíƒœê·¸ë³„ ìˆ˜ì§‘ í˜„í™©:")
    for hashtag, count in sorted(hashtag_counts.items(), key=lambda x: x[1], reverse=True):
        if count > 0:
            print(f"  {hashtag}: {count}ê°œ")
    
    # Phase 2: ë³¸ë¬¸ í¬ë¡¤ë§
    print(f"\n[Phase 2] ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘ (ì´ {len(all_blog_urls)}ê°œ URL)...")
    
    crawler = NaverBlogCrawler(headless=True)
    crawled_posts = []
    pm_filtered_count = 0
    
    try:
        for blog_info in tqdm(all_blog_urls, desc="í¬ë¡¤ë§"):
            post_data = crawler.crawl_blog_post(blog_info['link'])
            
            if post_data:
                # PM ê´€ë ¨ í•„í„°ë§
                if is_pm_related(post_data):
                    post_data.update({
                        'post_date': blog_info['postdate'],
                        'author_name': blog_info['bloggername'],
                        'description': blog_info.get('description', ''),
                        'blogger_profile': blog_info.get('bloggerlink', '')
                    })
                    crawled_posts.append(post_data)
                else:
                    pm_filtered_count += 1
            
            time.sleep(0.3)
    
    finally:
        crawler.close()
    
    print(f"\nâœ“ {len(crawled_posts)}ê°œ ê²Œì‹œë¬¼ í¬ë¡¤ë§ ì™„ë£Œ")
    print(f"  â€¢ PM ë¬´ê´€ ê²Œì‹œë¬¼ í•„í„°ë§: {pm_filtered_count}ê°œ")
    
    overall_stats['phase1_collected_urls'] = len(all_blog_urls)
    overall_stats['phase2_crawled_posts'] = len(crawled_posts)
    overall_stats['pm_filtered_count'] = pm_filtered_count
    
    # Phase 3: ì €ì¥
    print("\n[Phase 3] ë°ì´í„° ì €ì¥ ì¤‘...")
    df = save_to_csv(crawled_posts, OUTPUT_CSV)
    
    end_time = datetime.now()
    duration = end_time - start_time
    
    overall_stats['end_time'] = end_time.isoformat()
    overall_stats['duration_seconds'] = duration.total_seconds()
    overall_stats['final_post_count'] = len(crawled_posts)
    
    save_stats(overall_stats, STATS_FILE)
    
    # ìµœì¢… ìš”ì•½
    print("\n" + "=" * 70)
    print(" í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 70)
    print(f"ì†Œìš” ì‹œê°„: {duration}")
    print(f"ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼: {len(crawled_posts)}ê°œ")
    print(f"PM ë¬´ê´€ í•„í„°ë§: {pm_filtered_count}ê°œ")
    print(f"ì¶œë ¥ íŒŒì¼: {OUTPUT_CSV}")
    print(f"í†µê³„ íŒŒì¼: {STATS_FILE}")
    print("\nâœ… ê²°ê³¼ í™•ì¸ í›„ ì „ì²´ í¬ë¡¤ë§ ì§„í–‰í•˜ì„¸ìš”!")
    print("=" * 70)

if __name__ == "__main__":
    main()
