#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PM-International Korea ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v6.8 (ìµœì¢… ê°œì„ íŒ)

ğŸ”§ ì£¼ìš” ê°œì„  ì‚¬í•­:
1. âœ… published_date: API postdate í™œìš© (v5 ë°©ì‹ ë³µì›)
2. âœ… blogger_name: sponsor_nameê³¼ í†µí•© (ì¶”ì²œì¸ ì •ë³´ ìš°ì„ , ì—†ìœ¼ë©´ ë¸”ë¡œê±°ëª…)
3. âœ… hashtags: í˜ì´ì§€ ì†ŒìŠ¤ ì „ì²´ ê²€ìƒ‰ìœ¼ë¡œ ê°œì„ 
4. âœ… ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™” (ì†ë„ í–¥ìƒ)
5. âœ… ì¬ì‹œë„ ë¡œì§ ìµœì í™” (3íšŒâ†’2íšŒ, ëŒ€ê¸°ì‹œê°„ ë‹¨ì¶•)
6. âœ… post_id: ê²Œì‹œë¬¼ ê³ ìœ  ID (í‚¤ê°’ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥)
7. âš ï¸  view_count: ë„¤ì´ë²„ ë¸”ë¡œê·¸ì— ì¡°íšŒìˆ˜ ë¯¸í‘œì‹œë¡œ ì»¬ëŸ¼ ì œê±°
8. âš ï¸  image_urls: ë„¤ì´ë²„ Referer ì •ì±…ìœ¼ë¡œ ì™¸ë¶€ ì ‘ê·¼ ì œí•œ (í–¥í›„ OCR ì²˜ë¦¬ í•„ìš”)

ì¶œë ¥ ì»¬ëŸ¼ (16ê°œ):
- ê¸°ë³¸: platform, post_id, blog_id, url, title, content, published_date
- ì‘ì„±ì: blogger_name (ì¶”ì²œì¸ ìš°ì„ , ì—†ìœ¼ë©´ ë¸”ë¡œê±°ëª…)
- ì¶”ì²œì¸: sponsor_phone, sponsor_partner_id
- ì°¸ì—¬: like_count, comment_count (ì¡°íšŒìˆ˜ ì œì™¸)
- ì½˜í…ì¸ : hashtags, image_urls, video_urls
- ë©”íƒ€: collected_date
"""

import os
import re
import json
import time
import random
import logging
from datetime import datetime
from urllib.parse import urlparse, parse_qs, unquote
from typing import List, Dict, Optional, Set, Tuple

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd
import config

# ===========================
# ë¡œê¹… ì„¤ì •
# ===========================

class ColoredFormatter(logging.Formatter):
    """ì»¬ëŸ¬ ë¡œê¹… í¬ë§·í„°"""
    
    COLORS = {
        'DEBUG': '\033[94m',    # íŒŒë€ìƒ‰
        'INFO': '\033[92m',     # ì´ˆë¡ìƒ‰
        'WARNING': '\033[93m',  # ë…¸ë€ìƒ‰
        'ERROR': '\033[91m',    # ë¹¨ê°„ìƒ‰
        'CRITICAL': '\033[95m', # ë³´ë¼ìƒ‰
        'RESET': '\033[0m'
    }
    
    def format(self, record):
        log_color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
        record.levelname = f"{log_color}{record.levelname}{self.COLORS['RESET']}"
        return super().format(record)

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# íŒŒì¼ í•¸ë“¤ëŸ¬
file_handler = logging.FileHandler(
    f'naver_blog_crawler_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log',
    encoding='utf-8'
)
file_handler.setLevel(logging.DEBUG)
file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(file_formatter)

# ì½˜ì†” í•¸ë“¤ëŸ¬
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_formatter = ColoredFormatter('%(levelname)s - %(message)s')
console_handler.setFormatter(console_formatter)

logger.addHandler(file_handler)
logger.addHandler(console_handler)

# ===========================
# ì„¤ì • ì˜ì—­
# ===========================

# Naver Open API ì„¤ì • (from config.py)
NAVER_CLIENT_ID = config.NAVER_CLIENT_ID
NAVER_CLIENT_SECRET = config.NAVER_CLIENT_SECRET

# ê²€ìƒ‰ í‚¤ì›Œë“œ (PM International ê´€ë ¨)
SEARCH_KEYWORDS = [
    "í”¼ì— ì¸í„°ë‚´ì…”ë„",
    "í”¼ì— ì½”ë¦¬ì•„", 
    "PMì¸í„°ë‚´ì…”ë„",
    "ë…ì¼í”¼ì— ",
    "í•ë¼ì¸",
    "FitLine",
    "ë² ì´ì‹ìŠ¤",
    "í”„ë¡œì…°ì´í”„",
    "ì—‘í‹°ë°”ì´ì¦ˆ",
    "íŒŒì›Œì¹µí…Œì¼",
    "ë¦¬ìŠ¤í† ë ˆì´íŠ¸"
]

# PM ê´€ë ¨ í•„ìˆ˜ í‚¤ì›Œë“œ (ìµœì†Œ 1ê°œ ì´ìƒ í¬í•¨)
PM_REQUIRED_KEYWORDS = [
    "í”¼ì— ", "PM", "pm", "í•ë¼ì¸", "fitline", "FitLine", 
    "íŒ€íŒŒíŠ¸ë„ˆ", "ë…ì¼í”¼ì— ", "í”¼ì— ì½”ë¦¬ì•„", "í”¼ì— ì¸í„°ë‚´ì…”ë„"
]

# ì œì™¸ í‚¤ì›Œë“œ
EXCLUDE_KEYWORDS = [
    "ì±„ìš©", "êµ¬ì¸", "êµ¬ì§", "ì•Œë°”", "ì•„ë¥´ë°”ì´íŠ¸",
    "ì‚¬ê¸°", "í”¼í•´", "í™˜ë¶ˆ", "ì†Œì†¡", "ì‚¬ì¹­"
]

# API ê²€ìƒ‰ ì„¤ì •
DISPLAY_PER_PAGE = 100
MAX_PAGES = 1
TOTAL_TARGET = 100

# Selenium ì„¤ì •
SELENIUM_WAIT_TIME = 10
PAGE_LOAD_WAIT = 2  # 3ì´ˆ â†’ 2ì´ˆë¡œ ë‹¨ì¶• (ì†ë„ ê°œì„ )
PAGE_LOAD_TIMEOUT = 15  # í˜ì´ì§€ ë¡œë”© ìµœëŒ€ ëŒ€ê¸° ì‹œê°„
MAX_RETRY = 2  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (1íšŒ ì‹œë„ + 1íšŒ ì¬ì‹œë„)

# ìš”ì²­ ê°„ ë”œë ˆì´ (ì´ˆ)
REQUEST_DELAY_MIN = 1.5
REQUEST_DELAY_MAX = 3.5

# User-Agent
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0'
]

# ===========================
# í†µê³„ í´ë˜ìŠ¤
# ===========================

class CrawlingStats:
    """í¬ë¡¤ë§ í†µê³„ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.total_attempts = 0
        self.success_count = 0
        self.fail_count = 0
        self.filtered_out = 0
        self.duplicate_count = 0
        self.start_time = time.time()
    
    def add_attempt(self):
        self.total_attempts += 1
    
    def add_success(self):
        self.success_count += 1
    
    def add_fail(self):
        self.fail_count += 1
    
    def add_filtered(self):
        self.filtered_out += 1
    
    def add_duplicate(self):
        self.duplicate_count += 1
    
    def get_success_rate(self) -> float:
        if self.total_attempts == 0:
            return 0.0
        return (self.success_count / self.total_attempts) * 100
    
    def get_elapsed_time(self) -> str:
        elapsed = time.time() - self.start_time
        minutes = int(elapsed // 60)
        seconds = int(elapsed % 60)
        return f"{minutes}ë¶„ {seconds}ì´ˆ"
    
    def print_stats(self):
        logger.info("=" * 70)
        logger.info("ğŸ“Š í¬ë¡¤ë§ í†µê³„")
        logger.info("=" * 70)
        logger.info(f"ì´ ì‹œë„: {self.total_attempts}ê°œ")
        logger.info(f"âœ… ì„±ê³µ: {self.success_count}ê°œ")
        logger.info(f"âŒ ì‹¤íŒ¨: {self.fail_count}ê°œ")
        logger.info(f"ğŸš« í•„í„°ë§: {self.filtered_out}ê°œ")
        logger.info(f"ğŸ” ì¤‘ë³µ: {self.duplicate_count}ê°œ")
        logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {self.get_success_rate():.1f}%")
        logger.info(f"â±ï¸  ì†Œìš” ì‹œê°„: {self.get_elapsed_time()}")
        logger.info("=" * 70)

# ì „ì—­ í†µê³„ ê°ì²´
stats = CrawlingStats()

# ===========================
# Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™”
# ===========================

def init_selenium_driver() -> webdriver.Chrome:
    """Selenium Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_argument(f'user-agent={random.choice(USER_AGENTS)}')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™” (ì†ë„ í–¥ìƒ)
    prefs = {'profile.managed_default_content_settings.images': 2}
    chrome_options.add_experimental_option('prefs', prefs)
    
    # Homebrew chromedriver ìš°ì„  ì‚¬ìš© (macOS ë³´ì•ˆ ë¬¸ì œ í•´ê²°)
    homebrew_chromedriver = '/opt/homebrew/bin/chromedriver'
    
    if os.path.exists(homebrew_chromedriver):
        service = Service(homebrew_chromedriver)
        driver = webdriver.Chrome(service=service, options=chrome_options)
    else:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
    
    # í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì†ë„ ê°œì„ )
    driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)
    
    driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
        'source': '''
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            })
        '''
    })
    
    logger.info("âœ… Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ")
    return driver

# ===========================
# API ê²€ìƒ‰ í•¨ìˆ˜
# ===========================

def search_naver_blog(keyword: str, display: int = 100, start: int = 1) -> Optional[Dict]:
    """Naver Open Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡œê·¸ ê²€ìƒ‰"""
    url = "https://openapi.naver.com/v1/search/blog.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
        "User-Agent": random.choice(USER_AGENTS)
    }
    params = {
        "query": keyword,
        "display": display,
        "start": start,
        "sort": "sim"
    }
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        logger.error(f"API ìš”ì²­ ì‹¤íŒ¨ - í‚¤ì›Œë“œ: {keyword}, ì—ëŸ¬: {str(e)}")
        return None

# ===========================
# URL íŒŒì‹± ë° ì •ê·œí™”
# ===========================

def extract_blog_info_from_url(url: str) -> Optional[Dict[str, str]]:
    """ë¸”ë¡œê·¸ URLì—ì„œ blog_idì™€ post_id ì¶”ì¶œ"""
    try:
        url = unquote(url)
        parsed = urlparse(url)
        
        # íŒ¨í„´ 1: blog.naver.com/{blog_id}/{post_id}
        match = re.search(r'blog\.naver\.com/([^/]+)/(\d+)', url)
        if match:
            return {'blog_id': match.group(1), 'post_id': match.group(2)}
        
        # íŒ¨í„´ 2: PostView.nhn?blogId={blog_id}&logNo={post_id}
        if 'PostView' in url:
            query_params = parse_qs(parsed.query)
            blog_id = query_params.get('blogId', [None])[0]
            post_id = query_params.get('logNo', [None])[0]
            if blog_id and post_id:
                return {'blog_id': blog_id, 'post_id': post_id}
        
        # íŒ¨í„´ 3: m.blog.naver.com/{blog_id}/{post_id}
        match = re.search(r'm\.blog\.naver\.com/([^/]+)/(\d+)', url)
        if match:
            return {'blog_id': match.group(1), 'post_id': match.group(2)}
        
        return None
        
    except Exception as e:
        logger.error(f"URL íŒŒì‹± ì˜¤ë¥˜: {url}, {str(e)}")
        return None

def normalize_blog_url(blog_id: str, post_id: str) -> str:
    """ì •ê·œí™”ëœ ë¸”ë¡œê·¸ URL ìƒì„±"""
    return f"https://blog.naver.com/{blog_id}/{post_id}"

# ===========================
# ì½˜í…ì¸  í•„í„°ë§
# ===========================

def is_pm_related_content(title: str, content: str) -> bool:
    """ì œëª© ë° ë³¸ë¬¸ì´ PM International ê´€ë ¨ ë‚´ìš©ì¸ì§€ í™•ì¸"""
    combined_text = f"{title} {content}".lower()
    
    # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
    for exclude_kw in EXCLUDE_KEYWORDS:
        if exclude_kw.lower() in combined_text:
            logger.debug(f"ì œì™¸ í‚¤ì›Œë“œ ë°œê²¬: {exclude_kw}")
            return False
    
    # PM í•„ìˆ˜ í‚¤ì›Œë“œ ì²´í¬
    has_required = any(kw.lower() in combined_text for kw in PM_REQUIRED_KEYWORDS)
    
    if not has_required:
        logger.debug("PM í•„ìˆ˜ í‚¤ì›Œë“œ ë¯¸í¬í•¨")
        return False
    
    return True

# ===========================
# ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜ë“¤
# ===========================

def extract_blogger_name(driver: webdriver.Chrome) -> str:
    """ë¸”ë¡œê·¸ ìš´ì˜ì ì´ë¦„ ì¶”ì¶œ (2-4ì í•œê¸€ ì´ë¦„ë§Œ)"""
    selectors = [
        "div.blog_title a",
        "div.blog2_series a.link",
        "h1.blog_title",
        "div.tit_area h3.tit",
        "div.blog_title_area a",
        "a.blog_name",
        "div.nick_area a.nick",
        "div.blog_info a.blog_id",
        "span.nick",
        "a.NPI=a:blog.title"
    ]
    
    # ì œì™¸í•  ì¼ë°˜ ë‹¨ì–´ë“¤
    exclude_words = ['ë¸”ë¡œê·¸', 'ë§ˆì¼€íŒ…', 'ë‰´ìŠ¤', 'ê³µì‹', 'ì‚¬ì—…', 'ì½”ë¦¬ì•„', 'ì¸í„°ë‚´ì…”ë„', 
                     'ì½”ë“œ', 'ìœ¼ë¡œ', 'ì—ì„œ', 'ì—ê²Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ë¬¸ì˜', 'ì—°ë½', 'ì ']
    
    for selector in selectors:
        try:
            element = driver.find_element(By.CSS_SELECTOR, selector)
            blog_name = element.text.strip()
            
            # 2-4ì í•œê¸€ ì´ë¦„ ì¶”ì¶œ ì‹œë„
            if blog_name:
                # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ê° ë‹¨ì–´ ê²€ì‚¬
                words = blog_name.split()
                for word in words:
                    # 2-4ì í•œê¸€ë§Œ (ìˆ«ì, ì˜ë¬¸, íŠ¹ìˆ˜ë¬¸ì ì œì™¸)
                    if re.match(r'^[ê°€-í£]{2,4}$', word):
                        # ì œì™¸ ë‹¨ì–´ê°€ ì•„ë‹Œ ê²½ìš°
                        if not any(ex in word for ex in exclude_words):
                            logger.debug(f"ë¸”ë¡œê±° ì´ë¦„ ì¶”ì¶œ: {word}")
                            return word
        except NoSuchElementException:
            continue
    
    # ë©”íƒ€ íƒœê·¸ ì‹œë„
    try:
        og_title = driver.find_element(By.CSS_SELECTOR, 'meta[property="og:title"]')
        content = og_title.get_attribute('content')
        if content:
            # 2-4ì í•œê¸€ ì¶”ì¶œ
            names = re.findall(r'[ê°€-í£]{2,4}', content)
            for name in names:
                if not any(ex in name for ex in exclude_words):
                    logger.debug(f"ë¸”ë¡œê±° ì´ë¦„ ì¶”ì¶œ (ë©”íƒ€): {name}")
                    return name
    except NoSuchElementException:
        pass
    
    # í˜ì´ì§€ íƒ€ì´í‹€
    title = driver.title
    if title:
        names = re.findall(r'[ê°€-í£]{2,4}', title)
        for name in names:
            if not any(ex in name for ex in exclude_words):
                logger.debug(f"ë¸”ë¡œê±° ì´ë¦„ ì¶”ì¶œ (íƒ€ì´í‹€): {name}")
                return name
    
    logger.debug("ë¸”ë¡œê±° ì´ë¦„ ì¶”ì¶œ ì‹¤íŒ¨")
    return ""

def extract_like_count(driver: webdriver.Chrome, soup: BeautifulSoup) -> int:
    """ì¢‹ì•„ìš”(ê³µê°) ìˆ˜ ì¶”ì¶œ"""
    selectors = [
        'span.u_cnt._count',  # ê³µê° ë²„íŠ¼
        'span.cnt_like',
        'em.u_cnt',
        'span.like_count',
        'a.btn_empathy span.u_cnt',
        'div.end_btn span.u_cnt'
    ]
    
    # Selenium ì‹œë„
    for selector in selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            for elem in elements:
                text = elem.text.strip()
                match = re.search(r'(\d[\d,]*)', text)
                if match:
                    count = int(match.group(1).replace(',', ''))
                    logger.debug(f"ì¢‹ì•„ìš”ìˆ˜ ì¶”ì¶œ: {count}")
                    return count
        except Exception:
            continue
    
    # BeautifulSoup ì‹œë„
    for selector in selectors:
        elem = soup.select_one(selector)
        if elem:
            text = elem.get_text(strip=True)
            match = re.search(r'(\d[\d,]*)', text)
            if match:
                count = int(match.group(1).replace(',', ''))
                logger.debug(f"ì¢‹ì•„ìš”ìˆ˜ ì¶”ì¶œ (BS): {count}")
                return count
    
    # í…ìŠ¤íŠ¸ ê²€ìƒ‰
    page_text = soup.get_text()
    match = re.search(r'ê³µê°[:\s]*(\d[\d,]*)', page_text)
    if match:
        count = int(match.group(1).replace(',', ''))
        logger.debug(f"ì¢‹ì•„ìš”ìˆ˜ ì¶”ì¶œ (í…ìŠ¤íŠ¸): {count}")
        return count
    
    logger.debug("ì¢‹ì•„ìš”ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨")
    return 0

def extract_comment_count(driver: webdriver.Chrome, soup: BeautifulSoup) -> int:
    """ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ"""
    selectors = [
        'span.u_cnt_comment',
        'span.cnt_cmt',
        'em.comment_count',
        'span.comment_count',
        'a.btn_cmt span.u_cnt'
    ]
    
    # Selenium ì‹œë„
    for selector in selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            for elem in elements:
                text = elem.text.strip()
                match = re.search(r'(\d[\d,]*)', text)
                if match:
                    count = int(match.group(1).replace(',', ''))
                    logger.debug(f"ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ: {count}")
                    return count
        except Exception:
            continue
    
    # BeautifulSoup ì‹œë„
    for selector in selectors:
        elem = soup.select_one(selector)
        if elem:
            text = elem.get_text(strip=True)
            match = re.search(r'(\d[\d,]*)', text)
            if match:
                count = int(match.group(1).replace(',', ''))
                logger.debug(f"ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ (BS): {count}")
                return count
    
    # í…ìŠ¤íŠ¸ ê²€ìƒ‰
    page_text = soup.get_text()
    match = re.search(r'ëŒ“ê¸€[:\s]*(\d[\d,]*)', page_text)
    if match:
        count = int(match.group(1).replace(',', ''))
        logger.debug(f"ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ (í…ìŠ¤íŠ¸): {count}")
        return count
    
    logger.debug("ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨")
    return 0

def extract_sponsor_info(content: str) -> Dict[str, str]:
    """ì¶”ì²œì¸ ì •ë³´ ì¶”ì¶œ (ì´ë¦„, ì „í™”ë²ˆí˜¸, íŒŒíŠ¸ë„ˆë²ˆí˜¸)"""
    name_patterns = [
        r'(?:ì¶”ì²œì¸|ì¶”ì²œ|ì†Œê°œ|ë¬¸ì˜)[:\s]*\(?([ê°€-í£]{2,4})\)?',
        r'PM\s*(?:íŒŒíŠ¸ë„ˆ|ë§¤ë‹ˆì €)[:\s]*([ê°€-í£]{2,4})',
        r'(?:ì—°ë½ì²˜|ì „í™”)[:\s]*([ê°€-í£]{2,4})\s*[0-9-]'
    ]
    phone_patterns = [
        r'(?:ì—°ë½ì²˜|ì „í™”|ë¬¸ì˜|â˜|ğŸ“)[:\s]*([0-9]{2,3}[-\s]?[0-9]{3,4}[-\s]?[0-9]{4})',
        r'(01[016789][-\s]?[0-9]{3,4}[-\s]?[0-9]{4})'
    ]
    partner_patterns = [
        r'(?:íŒŒíŠ¸ë„ˆ\s*ë²ˆí˜¸|íŒŒíŠ¸ë„ˆ|íšŒì›\s*ë²ˆí˜¸|ë²ˆí˜¸)[:\s]*([0-9]{7,9})',
        r'(?:ì¶”ì²œ|ì†Œê°œ)\s*ë²ˆí˜¸[:\s]*([0-9]{7,9})'
    ]
    
    result = {'name': '', 'phone': '', 'partner_id': ''}
    
    for pattern in name_patterns:
        match = re.search(pattern, content)
        if match:
            name = match.group(1)
            if re.match(r'^[ê°€-í£]{2,4}$', name) and name not in ['ì½”ë“œ', 'ìœ¼ë¡œ', 'ì—ì„œ', 'ì—ê²Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ë¬¸ì˜', 'ì—°ë½']:
                result['name'] = name
                break
    
    for pattern in phone_patterns:
        match = re.search(pattern, content)
        if match:
            phone = re.sub(r'[^0-9]', '', match.group(0))
            if len(phone) in [10, 11]:
                result['phone'] = f"{phone[:3]}-{phone[3:7]}-{phone[7:]}" if len(phone) == 11 else f"{phone[:3]}-{phone[3:6]}-{phone[6:]}"
                break
    
    for pattern in partner_patterns:
        match = re.search(pattern, content)
        if match:
            number = match.group(1)
            if number.isdigit() and 7 <= len(number) <= 9:
                result['partner_id'] = number
                break
    
    return result

def extract_hashtags(driver: webdriver.Chrome, soup: BeautifulSoup, content: str) -> List[str]:
    """í•´ì‹œíƒœê·¸ ì¶”ì¶œ - ìš°ì„ ìˆœìœ„: íƒœê·¸ì˜ì—­ > ë³¸ë¬¸ > ë©”íƒ€ë°ì´í„°"""
    hashtags = set()
    
    # 1. í•˜ë‹¨ íƒœê·¸ ì˜ì—­ (ê°€ì¥ ì‹ ë¢°ë„ ë†’ìŒ)
    tag_selectors = [
        "div.post_tag a.link",
        "div.blog2_series a.link",
        "div.tag_area a",
        "div.post_bottom_area a.link",
        "a[href*='/PostList.naver?tag=']"
    ]
    
    for selector in tag_selectors:
        try:
            tag_elements = driver.find_elements(By.CSS_SELECTOR, selector)
            for elem in tag_elements:
                tag_text = elem.text.strip()
                if tag_text:
                    if not tag_text.startswith('#'):
                        tag_text = '#' + tag_text
                    hashtags.add(tag_text)
        except Exception:
            continue
    
    # 2. ë³¸ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ í•´ì‹œíƒœê·¸ ì¶”ì¶œ (íƒœê·¸ ì˜ì—­ì—ì„œ ëª» ì°¾ì€ ê²½ìš°ë§Œ)
    if len(hashtags) < 5:  # íƒœê·¸ê°€ 5ê°œ ë¯¸ë§Œì´ë©´ ë³¸ë¬¸ì—ì„œ ì¶”ê°€ ê²€ìƒ‰
        # ë³¸ë¬¸ì—ì„œ # íŒ¨í„´ ì¶”ì¶œ
        hash_in_content = re.findall(r'#([ê°€-í£a-zA-Z0-9_]+)', content)
        for tag in hash_in_content:
            if len(tag) >= 2 and len(tag) <= 20:  # 2~20ì ì‚¬ì´
                hashtags.add('#' + tag)
    
    # 3. ë©”íƒ€ë°ì´í„° (íƒœê·¸ê°€ ê±°ì˜ ì—†ëŠ” ê²½ìš°ë§Œ)
    if len(hashtags) < 3:
        try:
            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
            if meta_keywords and meta_keywords.get('content'):
                keywords = meta_keywords['content'].split(',')
                for k in keywords:
                    k = k.strip()
                    if k and 2 <= len(k) <= 20:
                        if not k.startswith('#'):
                            k = '#' + k
                        hashtags.add(k)
        except Exception:
            pass
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬ (ì œí•œ ì—†ìŒ)
    result = sorted(list(hashtags))
    if result:
        logger.debug(f"í•´ì‹œíƒœê·¸ ì¶”ì¶œ: {len(result)}ê°œ - {result[:3]}...")
    else:
        logger.debug("í•´ì‹œíƒœê·¸ ì—†ìŒ")
    return result

def extract_media_urls(driver: webdriver.Chrome, soup: BeautifulSoup) -> Tuple[List[str], List[str]]:
    """ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ URL ì¶”ì¶œ
    
    âš ï¸ ì£¼ì˜: ìˆ˜ì§‘ëœ ì´ë¯¸ì§€ URLì€ ë„¤ì´ë²„ì˜ ë³´ì•ˆ ì •ì±…ìƒ ì™¸ë¶€ì—ì„œ ì§ì ‘ ì ‘ê·¼ ì‹œ 404 ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ì´ëŠ” í¬ë¡¤ëŸ¬ì˜ ë¬¸ì œê°€ ì•„ë‹ˆë¼ ë„¤ì´ë²„ê°€ Referer í—¤ë”ë¥¼ ì²´í¬í•˜ê±°ë‚˜ ì„¸ì…˜ í† í°ì„ ìš”êµ¬í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
    ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´ ë¸Œë¼ìš°ì € ì„¸ì…˜ì„ ìœ ì§€í•˜ê±°ë‚˜ ì ì ˆí•œ í—¤ë”ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    image_urls = []
    video_urls = []
    
    try:
        # ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
        time.sleep(2)
        
        # ì´ë¯¸ì§€ ì¶”ì¶œ
        img_selectors = [
            "div.se-main-container img",
            "div#postViewArea img",
            "div.se-component-content img",
            "div.post_ct img",
            "div.__se_component_area img"
        ]
        
        for selector in img_selectors:
            try:
                img_elements = driver.find_elements(By.CSS_SELECTOR, selector)
                for img in img_elements:
                    src = img.get_attribute('src') or img.get_attribute('data-src')
                    if src and src.startswith('http'):
                        # ì¸ë„¤ì¼ â†’ ì›ë³¸
                        if 'type=w' in src:
                            src = re.sub(r'type=w\d+', 'type=w966', src)
                        image_urls.append(src)
            except Exception:
                continue
        
        # ë¹„ë””ì˜¤ ì¶”ì¶œ
        video_selectors = [
            "div.se-main-container video",
            "div#postViewArea video",
            "div.se-component-content video"
        ]
        
        for selector in video_selectors:
            try:
                video_elements = driver.find_elements(By.CSS_SELECTOR, selector)
                for video in video_elements:
                    src = video.get_attribute('src') or video.get_attribute('data-src')
                    if src and src.startswith('http'):
                        video_urls.append(src)
            except Exception:
                continue
        
        # ì¤‘ë³µ ì œê±°
        image_urls = list(dict.fromkeys(image_urls))[:10]
        video_urls = list(dict.fromkeys(video_urls))[:5]
        
        logger.debug(f"ë¯¸ë””ì–´ ì¶”ì¶œ: ì´ë¯¸ì§€ {len(image_urls)}ê°œ, ë¹„ë””ì˜¤ {len(video_urls)}ê°œ")
        return image_urls, video_urls
        
    except Exception as e:
        logger.error(f"ë¯¸ë””ì–´ URL ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return [], []

def parse_date(date_str: str) -> Optional[str]:
    """ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±"""
    try:
        date_str = re.sub(r'[^\d\-\.\s:]', '', date_str).strip()
        
        patterns = [
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})\s+(\d{1,2}):(\d{1,2})',
            r'(\d{4})-(\d{1,2})-(\d{1,2})\s+(\d{1,2}):(\d{1,2})',
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})',
            r'(\d{4})-(\d{1,2})-(\d{1,2})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, date_str)
            if match:
                groups = match.groups()
                if len(groups) == 5:
                    year, month, day, hour, minute = groups
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)} {hour.zfill(2)}:{minute.zfill(2)}:00"
                else:
                    year, month, day = groups
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)} 00:00:00"
        
        return None
        
    except Exception as e:
        logger.error(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {date_str}, {str(e)}")
        return None

# ===========================
# ë°ì´í„° ê²€ì¦
# ===========================

def validate_post_data(post_data: Dict) -> bool:
    """ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦"""
    required_fields = ['post_id', 'blog_id', 'url', 'title', 'content']
    
    for field in required_fields:
        if not post_data.get(field):
            logger.warning(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
            return False
    
    # ì œëª© ê¸¸ì´ ì²´í¬
    if len(post_data['title']) < 2:
        logger.warning(f"ì œëª©ì´ ë„ˆë¬´ ì§§ìŒ: {post_data['title']}")
        return False
    
    # ë³¸ë¬¸ ê¸¸ì´ ì²´í¬
    if len(post_data['content']) < 50:
        logger.warning(f"ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ: {len(post_data['content'])}ì")
        return False
    
    return True

# ===========================
# ì¬ì‹œë„ ë¡œì§
# ===========================

def retry_with_backoff(func, *args, **kwargs):
    """ì¬ì‹œë„ ë¡œì§ (ë¹ ë¥¸ ì‹¤íŒ¨)"""
    for attempt in range(MAX_RETRY):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            if attempt < MAX_RETRY - 1:
                wait_time = 1.0 + random.uniform(0, 0.5)  # 1~1.5ì´ˆë§Œ ëŒ€ê¸°
                logger.warning(f"ì¬ì‹œë„ {attempt + 1}/{MAX_RETRY}: {wait_time:.1f}ì´ˆ ëŒ€ê¸° - {str(e)[:50]}")
                time.sleep(wait_time)
            else:
                logger.error(f"ìµœì¢… ì‹¤íŒ¨: {str(e)[:50]}")
                raise

# ===========================
# ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜
# ===========================

def crawl_blog_post_selenium(driver: webdriver.Chrome, url: str, blog_id: str, post_id: str, 
                             api_data: Dict = None, failed_url_manager=None) -> Optional[Dict]:
    """Seleniumì„ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ í¬ë¡¤ë§ (ì¬ì‹œë„ í¬í•¨) - ê°œì„ ë¨
    
    Args:
        api_data: API ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° (postdate, bloggername ë“± í¬í•¨)
    """
    
    def _crawl():
        logger.debug(f"í¬ë¡¤ë§ ì‹œì‘: {url}")
        driver.get(url)
        time.sleep(PAGE_LOAD_WAIT)
        
        # iframe ì²˜ë¦¬
        try:
            WebDriverWait(driver, SELENIUM_WAIT_TIME).until(
                EC.frame_to_be_available_and_switch_to_it((By.ID, "mainFrame"))
            )
            logger.debug("mainFrame ì „í™˜ ì™„ë£Œ")
        except TimeoutException:
            logger.debug("mainFrame ì—†ìŒ - ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§")
        
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # ì œëª©
        title_selectors = [
            'div.se-title-text',
            'div.pcol1 span.se-fs-',
            'div#viewTypeSelector span.se-fs-',
            'h3.se-title-text',
            'span.pcol1.itemSubjectBoldfont'
        ]
        title = None
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem and title_elem.get_text(strip=True):
                title = title_elem.get_text(strip=True)
                break
        
        if not title:
            logger.warning(f"ì œëª© ì—†ìŒ: {url}")
            title = "ì œëª© ì—†ìŒ"
        
        # ë³¸ë¬¸ (contentë¥¼ ë¨¼ì € ì¶”ì¶œí•´ì•¼ í•¨)
        content_selectors = [
            'div.se-main-container',
            'div#postViewArea',
            'div.se-component-content'
        ]
        content = ""
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = content_elem.get_text(separator='\n', strip=True)
                break
        
        if not content:
            logger.warning(f"ë³¸ë¬¸ ì—†ìŒ: {url}")
            return None
        
        # ë¸”ë¡œê±° ì´ë¦„ (API ë°ì´í„° ìš°ì„ , ì—†ìœ¼ë©´ í˜ì´ì§€ì—ì„œ ì¶”ì¶œ)
        blogger_name = ''
        if api_data and api_data.get('bloggername'):
            blogger_name = api_data['bloggername']
        else:
            blogger_name = extract_blogger_name(driver)
        
        # ì¶”ì²œì¸ ì •ë³´ ì¶”ì¶œ (ë³¸ë¬¸ì—ì„œ)
        sponsor_info = extract_sponsor_info(content)
        # sponsor_nameì´ ì—†ìœ¼ë©´ blogger_name ì‚¬ìš©
        if not sponsor_info['name'] and blogger_name:
            sponsor_info['name'] = blogger_name
        
        # PM í•„í„°ë§
        if not is_pm_related_content(title, content):
            logger.debug(f"PM ë¬´ê´€: {title}")
            stats.add_filtered()
            return None
        
        # ë°œí–‰ì¼: API ë°ì´í„° ìš°ì„  ì‚¬ìš© (v5 ë°©ì‹)
        published_date = ''
        if api_data and api_data.get('postdate'):
            # API postdate í˜•ì‹: "20231219" -> "2023-12-19"
            postdate = api_data['postdate']
            if len(postdate) == 8:
                published_date = f"{postdate[:4]}-{postdate[4:6]}-{postdate[6:8]}"
                logger.debug(f"ë°œí–‰ì¼ (API): {published_date}")
        
        # API ë°ì´í„°ê°€ ì—†ìœ¼ë©´ í˜ì´ì§€ì—ì„œ ì¶”ì¶œ ì‹œë„
        if not published_date:
            published_date = extract_published_date(driver, soup)
            if published_date:
                logger.debug(f"ë°œí–‰ì¼ (í˜ì´ì§€): {published_date}")
        
        # ì¢‹ì•„ìš”ìˆ˜, ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ (ì¡°íšŒìˆ˜ëŠ” ë„¤ì´ë²„ ë¸”ë¡œê·¸ì— í‘œì‹œ ì•ˆë¨)
        like_count = extract_like_count(driver, soup)
        comment_count = extract_comment_count(driver, soup)
        
        # í•´ì‹œíƒœê·¸
        hashtags = extract_hashtags(driver, soup, content)
        
        # ë¯¸ë””ì–´ URL
        image_urls, video_urls = extract_media_urls(driver, soup)
        
        # ê²°ê³¼ êµ¬ì„±
        post_data = {
            'platform': 'Naver Blog',
            'post_id': post_id,  # ê²Œì‹œë¬¼ ê³ ìœ  ID (í‚¤ê°’)
            'blog_id': blog_id,
            'url': url,
            'title': title,
            'content': content[:5000],
            'published_date': published_date,
            'blogger_name': sponsor_info['name'] if sponsor_info['name'] else blogger_name,  # í†µí•©
            'sponsor_phone': sponsor_info['phone'],
            'sponsor_partner_id': sponsor_info['partner_id'],
            'like_count': like_count,
            'comment_count': comment_count,
            'hashtags': ', '.join(hashtags) if hashtags else '',
            'image_urls': '|'.join(image_urls) if image_urls else '',
            'video_urls': '|'.join(video_urls) if video_urls else '',
            'collected_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # ë°ì´í„° ê²€ì¦
        if not validate_post_data(post_data):
            logger.warning(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {title}")
            return None
        
        logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {title[:30]}... (ì¢‹ì•„ìš”:{like_count}, ëŒ“ê¸€:{comment_count}, í•´ì‹œíƒœê·¸:{len(hashtags)}ê°œ)")
        return post_data
    
    # ì¬ì‹œë„ ë¡œì§ ì ìš©
    stats.add_attempt()
    try:
        result = retry_with_backoff(_crawl)
        if result:
            stats.add_success()
        return result
    except Exception as e:
        error_msg = str(e)
        logger.error(f"í¬ë¡¤ë§ ìµœì¢… ì‹¤íŒ¨: {url}, {error_msg}")
        stats.add_fail()
        
        # ì‹¤íŒ¨ URL ê¸°ë¡
        if failed_url_manager:
            failed_url_manager.add_failed_url(url, error_msg, blog_id, post_id)
        
        return None
    finally:
        try:
            driver.switch_to.default_content()
        except Exception:
            pass

# ===========================
# ì¤‘ë³µ ì²´í¬
# ===========================

def generate_post_fingerprint(post_data: Dict) -> str:
    """ê²Œì‹œë¬¼ ê³ ìœ  ì§€ë¬¸ ìƒì„± (URL + ì œëª© + ì‘ì„±ì)"""
    return f"{post_data['url']}_{post_data['title']}_{post_data['blog_id']}"

# ===========================
# ì—ëŸ¬ ê²Œì‹œë¬¼ ê´€ë¦¬
# ===========================

class FailedURLManager:
    """ì‹¤íŒ¨í•œ URL ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, filename='failed_urls.json'):
        self.filename = filename
        self.failed_urls = []
    
    def add_failed_url(self, url: str, reason: str, blog_id: str = '', post_id: str = ''):
        """ì‹¤íŒ¨í•œ URL ì¶”ê°€"""
        self.failed_urls.append({
            'url': url,
            'blog_id': blog_id,
            'post_id': post_id,
            'reason': reason,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
    
    def save_to_file(self):
        """ì‹¤íŒ¨í•œ URLì„ íŒŒì¼ì— ì €ì¥"""
        if self.failed_urls:
            with open(self.filename, 'w', encoding='utf-8') as f:
                json.dump(self.failed_urls, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ ì‹¤íŒ¨ URL ì €ì¥: {self.filename} ({len(self.failed_urls)}ê°œ)")
    
    def get_failed_count(self) -> int:
        return len(self.failed_urls)

# ===========================
# ë©”ì¸ í•¨ìˆ˜
# ===========================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("=" * 70)
    logger.info("ğŸš€ PM-International Korea ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v6.7 ì‹œì‘")
    logger.info("=" * 70)
    
    driver = init_selenium_driver()
    failed_url_manager = FailedURLManager()
    
    collected_posts = []
    collected_urls = set()
    collected_fingerprints = set()
    
    try:
        for keyword in SEARCH_KEYWORDS:
            logger.info(f"\nğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰: '{keyword}'")
            
            for page in range(1, MAX_PAGES + 1):
                start = (page - 1) * DISPLAY_PER_PAGE + 1
                logger.info(f"ğŸ“„ í˜ì´ì§€ {page} ê²€ìƒ‰ ì¤‘... (start={start})")
                
                search_result = search_naver_blog(keyword, DISPLAY_PER_PAGE, start)
                
                if not search_result or 'items' not in search_result:
                    logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    break
                
                items = search_result['items']
                logger.info(f"ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼: {len(items)}ê°œ")
                
                if len(items) == 0:
                    break
                
                for idx, item in enumerate(items, 1):
                    if len(collected_posts) >= TOTAL_TARGET:
                        logger.info(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±: {TOTAL_TARGET}ê°œ")
                        break
                    
                    link = item.get('link', '')
                    if not link or link in collected_urls:
                        stats.add_duplicate()
                        continue
                    
                    blog_info = extract_blog_info_from_url(link)
                    if not blog_info:
                        continue
                    
                    blog_id = blog_info['blog_id']
                    post_id = blog_info['post_id']
                    normalized_url = normalize_blog_url(blog_id, post_id)
                    
                    if normalized_url in collected_urls:
                        stats.add_duplicate()
                        continue
                    
                    logger.info(f"[{len(collected_posts)+1}/{TOTAL_TARGET}] í¬ë¡¤ë§ ì¤‘...")
                    
                    # API ë°ì´í„° ì „ë‹¬ (postdate, bloggername í¬í•¨)
                    api_item_data = {
                        'postdate': item.get('postdate', ''),
                        'bloggername': item.get('bloggername', '')
                    }
                    
                    post_data = crawl_blog_post_selenium(driver, normalized_url, blog_id, post_id, api_item_data, failed_url_manager)
                    
                    if post_data:
                        fingerprint = generate_post_fingerprint(post_data)
                        if fingerprint not in collected_fingerprints:
                            collected_posts.append(post_data)
                            collected_urls.add(normalized_url)
                            collected_fingerprints.add(fingerprint)
                        else:
                            logger.debug("ì¤‘ë³µ ê²Œì‹œë¬¼ (fingerprint)")
                            stats.add_duplicate()
                    
                    time.sleep(random.uniform(REQUEST_DELAY_MIN, REQUEST_DELAY_MAX))
                
                if len(collected_posts) >= TOTAL_TARGET:
                    break
                
                time.sleep(random.uniform(2, 4))
            
            if len(collected_posts) >= TOTAL_TARGET:
                break
        
        # í†µê³„ ì¶œë ¥
        stats.print_stats()
        
        # ê²°ê³¼ ì €ì¥
        if collected_posts:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'naver_blog_fixed_{timestamp}.csv'
            
            df = pd.DataFrame(collected_posts)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            logger.info(f"\n{'=' * 70}")
            logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
            logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(collected_posts)}ê°œ")
            logger.info(f"{'=' * 70}")
        else:
            logger.warning("âš ï¸  ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì‹¤íŒ¨ URL ì €ì¥
        failed_url_manager.save_to_file()
        if failed_url_manager.get_failed_count() > 0:
            logger.info(f"âŒ ì‹¤íŒ¨í•œ URL: {failed_url_manager.get_failed_count()}ê°œ (failed_urls.json ì°¸ì¡°)")
    
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  ì‚¬ìš©ì ì¤‘ë‹¨")
        stats.print_stats()
    
    except Exception as e:
        logger.error(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {str(e)}")
        stats.print_stats()
    
    finally:
        driver.quit()
        logger.info("âœ… ë“œë¼ì´ë²„ ì¢…ë£Œ")

if __name__ == "__main__":
    main()
