#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PM-International Korea ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v8.1 (1ë§Œê°œ ì´ìƒ ëŒ€ìš©ëŸ‰ ìˆ˜ì§‘)

ğŸš€ v8.1 ì‹ ê·œ ê°œì„  ì‚¬í•­:
1. ğŸ¯ 1ë§Œê°œ ì´ìƒ ëŒ€ìš©ëŸ‰ ìˆ˜ì§‘ ìµœì í™”
   - ì—°ë„ë³„ í‚¤ì›Œë“œ ì¡°í•© ì „ëµ (2023~2025)
   - í‚¤ì›Œë“œë‹¹ 1000ê°œ ìˆ˜ì§‘ (API ìµœëŒ€ì¹˜)
   - ì˜ˆìƒ ìˆ˜ì§‘ëŸ‰: 15,000~18,000ê°œ
2. ğŸ” ê¸°ê°„ ë‹¤ì–‘ì„± í™•ë³´
   - sort="date" (ìµœì‹ ìˆœ ì •ë ¬)
   - 3ë…„ì¹˜ ë°ì´í„° ìˆ˜ì§‘ìœ¼ë¡œ ì‹œê³„ì—´ ë¶„ì„ ê°€ëŠ¥
3. ğŸ›¡ï¸ í•„í„°ë§ ê°•í™”
   - ë§¤íŠ¸ë¦¬ìŠ¤ ì—…ì²´ 'í”¼ì— ì½”ë¦¬ì•„' ì°¨ë‹¨
   - ë¸”ë™ë¦¬ìŠ¤íŠ¸ + ì œì™¸ í‚¤ì›Œë“œ 2ì¤‘ í•„í„°
4. ğŸ“Š íš¨ìœ¨ì  í‚¤ì›Œë“œ ì„ íƒ
   - v7.7 í†µê³„ ê¸°ë°˜ ì„±ê³µë¥  ë†’ì€ í‚¤ì›Œë“œë§Œ ì„ íƒ
   - ì‹¤íŒ¨ í‚¤ì›Œë“œ 7ê°œ ì œì™¸ (ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)

ğŸ”§ v7.7 ìœ ì§€ ì‚¬í•­:
- í‚¤ì›Œë“œë³„ ìƒì„¸ í†µê³„ (ì„±ê³µë¥ , í•„í„°ë§ë¥ , ì¤‘ë³µë¥ )
- 5ë‹¨ê³„ í•„í„°ë§ ì‹œìŠ¤í…œ
- ë“œë¼ì´ë²„ ì¬ì‹œì‘ ì„ê³„ê°’ 50íšŒ
- published_datetime ì•ˆì •ì  ìˆ˜ì§‘

ğŸ“Š ì¶œë ¥ ì»¬ëŸ¼ (15ê°œ):
- ê¸°ë³¸: platform, post_id, blog_id, url, title, content, published_datetime
- ì¶”ì²œì¸: sponsor_phone, sponsor_partner_id
- ì°¸ì—¬: like_count, comment_count
- ì½˜í…ì¸ : hashtags, image_urls, video_urls
- ë©”íƒ€: collected_date

ì‘ì„±ì: PMI Korea ë°ì´í„° ë¶„ì„íŒ€
ë²„ì „: 8.1
ìµœì¢… ìˆ˜ì •ì¼: 2025-11-08
"""

import os
import re
import json
import time
import random
import logging
import gc
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs, unquote
from typing import List, Dict, Optional, Set, Tuple

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd

# ===========================
# ë¡œê¹… ì„¤ì •
# ===========================

class ColoredFormatter(logging.Formatter):
    """ì»¬ëŸ¬ ë¡œê¹… í¬ë§·í„°"""
    
    COLORS = {
        'DEBUG': '\033[94m',
        'INFO': '\033[92m',
        'WARNING': '\033[93m',
        'ERROR': '\033[91m',
        'CRITICAL': '\033[95m',
        'RESET': '\033[0m'
    }
    
    def format(self, record):
        log_color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
        record.levelname = f"{log_color}{record.levelname}{self.COLORS['RESET']}"
        return super().format(record)

logging.basicConfig(
    level=logging.INFO,  # INFOë¡œ ë³µì› (DEBUGëŠ” ê°œë°œìš©ë§Œ)
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê·¸ ì–µì œ (ì„±ëŠ¥ ìµœì í™”)
logging.getLogger('selenium').setLevel(logging.WARNING)
logging.getLogger('urllib3').setLevel(logging.WARNING)
logging.getLogger('requests').setLevel(logging.WARNING)

for handler in logger.handlers:
    handler.setFormatter(ColoredFormatter(
        '%(asctime)s [%(levelname)s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    ))

# ===========================
# ì„¤ì •ê°’
# ===========================

# Naver Open API ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜)
try:
    import config
    NAVER_CLIENT_ID = config.NAVER_CLIENT_ID
    NAVER_CLIENT_SECRET = config.NAVER_CLIENT_SECRET
except ImportError:
    NAVER_CLIENT_ID = os.getenv('NAVER_CLIENT_ID', '')
    NAVER_CLIENT_SECRET = os.getenv('NAVER_CLIENT_SECRET', '')

# User-Agent ëª©ë¡ (v7.4: ë¡œí…Œì´ì…˜ ì§€ì›)
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0'
]

# v8.1: ê²€ìƒ‰ ì„¤ì • (1ë§Œê°œ ì´ìƒ ëŒ€ìš©ëŸ‰ ìˆ˜ì§‘)
# ì—°ë„ë³„ í‚¤ì›Œë“œ ì¡°í•© ì „ëµ (2023~2025)
# v7.7 í†µê³„ ê¸°ë°˜: ì„±ê³µë¥  ë†’ì€ í‚¤ì›Œë“œë§Œ ì„ íƒ

# ì£¼ìš” í‚¤ì›Œë“œ (ì„±ê³µë¥  50%+ í‚¤ì›Œë“œ Ã— 3ë…„)
PRIMARY_KEYWORDS = [
    # í”¼ì— ì¸í„°ë‚´ì…”ë„ (ì„±ê³µë¥  62.5%)
    {"keyword": "í”¼ì— ì¸í„°ë‚´ì…”ë„ 2025", "target": 800},
    {"keyword": "í”¼ì— ì¸í„°ë‚´ì…”ë„ 2024", "target": 800},
    {"keyword": "í”¼ì— ì¸í„°ë‚´ì…”ë„ 2023", "target": 800},
    # ë…ì¼í”¼ì—  (ì„±ê³µë¥  65.2%)
    {"keyword": "ë…ì¼í”¼ì—  2025", "target": 800},
    {"keyword": "ë…ì¼í”¼ì—  2024", "target": 800},
    {"keyword": "ë…ì¼í”¼ì—  2023", "target": 800},
    # PMì¸í„°ë‚´ì…”ë„ (ì„±ê³µë¥  65.9%)
    {"keyword": "PMì¸í„°ë‚´ì…”ë„ 2025", "target": 800},
    {"keyword": "PMì¸í„°ë‚´ì…”ë„ 2024", "target": 800},
    {"keyword": "PMì¸í„°ë‚´ì…”ë„ 2023", "target": 800},
    # í”¼ì— ì½”ë¦¬ì•„ (ì„±ê³µë¥  50.0%, ë§¤íŠ¸ë¦¬ìŠ¤ ì—…ì²´ í•„í„°ë§ í•„ìš”)
    {"keyword": "í”¼ì— ì½”ë¦¬ì•„ 2025", "target": 800},
    {"keyword": "í”¼ì— ì½”ë¦¬ì•„ 2024", "target": 800},
    {"keyword": "í”¼ì— ì½”ë¦¬ì•„ 2023", "target": 800},
]

# ì œí’ˆ í‚¤ì›Œë“œ (ì„±ê³µë¥  40%+ í‚¤ì›Œë“œ Ã— 3ë…„)
SECONDARY_KEYWORDS = [
    # í”¼íŠ¸ë¼ì¸ (ì„±ê³µë¥  61.2%)
    {"keyword": "í”¼íŠ¸ë¼ì¸ 2025", "target": 800},
    {"keyword": "í”¼íŠ¸ë¼ì¸ 2024", "target": 800},
    {"keyword": "í”¼íŠ¸ë¼ì¸ 2023", "target": 800},
    # íƒ‘ì‰ì´í”„ (ì„±ê³µë¥  60.0%)
    {"keyword": "íƒ‘ì‰ì´í”„ 2025", "target": 800},
    {"keyword": "íƒ‘ì‰ì´í”„ 2024", "target": 800},
    {"keyword": "íƒ‘ì‰ì´í”„ 2023", "target": 800},
    # í”„ë¡œì‰ì´í”„ (ì„±ê³µë¥  62.5%)
    {"keyword": "í”„ë¡œì‰ì´í”„ 2025", "target": 800},
    {"keyword": "í”„ë¡œì‰ì´í”„ 2024", "target": 800},
    {"keyword": "í”„ë¡œì‰ì´í”„ 2023", "target": 800},
    # ë””ë“œë§í¬ (ì„±ê³µë¥  62.5%)
    {"keyword": "ë””ë“œë§í¬ 2025", "target": 800},
    {"keyword": "ë””ë“œë§í¬ 2024", "target": 800},
    {"keyword": "ë””ë“œë§í¬ 2023", "target": 800},
    # ë®¤ë…¸ê² (ì„±ê³µë¥  57.7%)
    {"keyword": "ë®¤ë…¸ê² 2025", "target": 800},
    {"keyword": "ë®¤ë…¸ê² 2024", "target": 800},
    {"keyword": "ë®¤ë…¸ê² 2023", "target": 800},
    # ì—‘í‹°ë°”ì´ì¦ˆ (ì„±ê³µë¥  43.5%)
    {"keyword": "ì—‘í‹°ë°”ì´ì¦ˆ 2025", "target": 800},
    {"keyword": "ì—‘í‹°ë°”ì´ì¦ˆ 2024", "target": 800},
    {"keyword": "ì—‘í‹°ë°”ì´ì¦ˆ 2023", "target": 800},
    # íŒŒì›Œì¹µí…Œì¼ (ì„±ê³µë¥  42.9%)
    {"keyword": "íŒŒì›Œì¹µí…Œì¼ 2025", "target": 800},
    {"keyword": "íŒŒì›Œì¹µí…Œì¼ 2024", "target": 800},
    {"keyword": "íŒŒì›Œì¹µí…Œì¼ 2023", "target": 800},
]

# v8.1: ì œì™¸ í‚¤ì›Œë“œ (v7.7 í†µê³„ ê¸°ë°˜ - ê²€ìƒ‰ ê²°ê³¼ ì—†ê±°ë‚˜ ê·¹ì†ŒëŸ‰)
# PMIK (ì„±ê³µë¥  23%), ë¦¬ìŠ¤í† ë ˆì´íŠ¸ (ì¤‘ë³µë¥  29.4%), ì˜µí‹°ë©€ì…‹, ì œë„ˆë ˆì´ì…˜50,
# ê²”ë§í•, ì ¤ìŠˆì¸ , ì•¡í‹°ë°”ì´ì¦ˆì„¸ëŸ¼, ì˜ì¼€ì–´3ì¢…, í”¼íŠ¸ë¼ì¸ìŠ¤í‚¨, í•ë¼ì¸

# ì „ì²´ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
ALL_KEYWORDS = PRIMARY_KEYWORDS + SECONDARY_KEYWORDS

MAX_SEARCH_RESULTS = 1000  # v8.1: API ìµœëŒ€ì¹˜ (í‚¤ì›Œë“œë‹¹ 1000ê°œ)
TOTAL_TARGET = 15000  # v8.1: 1.5ë§Œê°œ ëª©í‘œ (ì¤‘ë³µ ì œê±° í›„)
NUM_WORKERS = 1  # v8.1: ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ (ì•ˆì •ì„± ìš°ì„ )

# í•„í„°ë§ í‚¤ì›Œë“œ
PM_BRAND_KEYWORDS = [
    "í”¼ì— ", "í”¼ì— ì¸í„°ë‚´ì…”ë„", "PM International", "PMInternational",
    "PM", "FitLine", "í•ë¼ì¸", "í”¼íŠ¸ë¼ì¸"
]

PM_SALES_KEYWORDS = [
    "ì¶”ì²œì¸", "ì¶”ì²œì¸ì½”ë“œ", "ì¶”ì²œì¸ ì½”ë“œ", "ì¶”ì²œì¸ë²ˆí˜¸", "ì¶”ì²œì¸ ë²ˆí˜¸",
    "íŒŒíŠ¸ë„ˆ", "íŒŒíŠ¸ë„ˆì½”ë“œ", "íŒŒíŠ¸ë„ˆ ì½”ë“œ", "íŒŒíŠ¸ë„ˆë²ˆí˜¸", "íŒŒíŠ¸ë„ˆ ë²ˆí˜¸",
    "ë“±ë¡", "ê°€ì…", "ë¬¸ì˜"
]

# v8.1: ì œì™¸ í‚¤ì›Œë“œ ê°•í™” (ë§¤íŠ¸ë¦¬ìŠ¤ ì—…ì²´ í•„í„°ë§)
EXCLUDE_KEYWORDS = [
    "ë‰´ìŠ¤", "ê¸°ì‚¬", "ë³´ë„", "ê³µì§€", "ì•„ì¹´ë°ë¯¸", "ì„¸ë¯¸ë‚˜", 
    "íŒ½ì°½íƒ±í¬", "ë°°ê´€", "ê¸°ì",
    "ë§¤íŠ¸ë¦¬ìŠ¤", "ì¹¨ëŒ€", "í˜¼ìˆ˜ê°€êµ¬", "ì‹ í˜¼ê°€êµ¬"  # v8.1: ë§¤íŠ¸ë¦¬ìŠ¤ ì—…ì²´ ì°¨ë‹¨
]

# v8.1: ì–¸ë¡ /ë‰´ìŠ¤ ë¸”ë¡œê·¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸ (ë§¤íŠ¸ë¦¬ìŠ¤ ì—…ì²´ ì¶”ê°€)
EXCLUDED_BLOG_IDS = [
    "ysc14",  # ë§ˆì¼€íŒ… ë‰´ìŠ¤ ë¸”ë¡œê·¸
    "embarkonsleep",  # v8.1: ë§¤íŠ¸ë¦¬ìŠ¤ ì—…ì²´ 'í”¼ì— ì½”ë¦¬ì•„'
]

# v7.4: ì–¸ë¡  ìŠ¤íƒ€ì¼ ì œëª© íŒ¨í„´
MEDIA_TITLE_PATTERNS = [
    r'"[^"]+",\s*"',  # "OOO íšŒì¥", "..." ê°™ì€ ì¸ìš©ë¬¸
    r'ê¸°ì\s+',
    r'ì·¨ì¬\s+',
]

# í¬ë¡¤ë§ ì„¤ì •
PAGE_LOAD_TIMEOUT = 10  # v7.6: íƒ€ì„ì•„ì›ƒ ë‹¨ì¶• (15â†’10)
REQUEST_DELAY_MIN = 1.5  # v7.7: ë©€í‹°í”„ë¡œì„¸ì‹± ê³ ë ¤ (ì›Œì»¤ë‹¹ 1.5ì´ˆ)
REQUEST_DELAY_MAX = 2.5  # v7.7: ë©€í‹°í”„ë¡œì„¸ì‹± ê³ ë ¤ (ì›Œì»¤ë‹¹ 2.5ì´ˆ)
MAX_CONSECUTIVE_ERRORS = 50  # v7.6: ëŒ€ìš©ëŸ‰ í¬ë¡¤ë§ ìµœì í™” (5â†’50)

# ===========================
# v7.3: ì ì‘í˜• ì†ë„ ì¡°ì ˆ
# ===========================

class AdaptiveDelay:
    """ì„±ê³µ/ì‹¤íŒ¨ì— ë”°ë¼ ëŒ€ê¸° ì‹œê°„ì„ ë™ì ìœ¼ë¡œ ì¡°ì ˆ"""
    
    def __init__(self, initial_min=2.0, initial_max=4.0):
        self.delay_min = initial_min
        self.delay_max = initial_max
        self.success_count = 0
        self.fail_count = 0
    
    def on_success(self):
        """ì„±ê³µ ì‹œ ëŒ€ê¸° ì‹œê°„ ë‹¨ì¶• (ìµœì†Œ 1ì´ˆê¹Œì§€)"""
        self.success_count += 1
        if self.success_count >= 3:
            self.delay_min = max(1.0, self.delay_min - 0.2)
            self.delay_max = max(2.0, self.delay_max - 0.3)
            self.success_count = 0
    
    def on_fail(self):
        """ì‹¤íŒ¨ ì‹œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€ (ìµœëŒ€ 10ì´ˆê¹Œì§€)"""
        self.fail_count += 1
        if self.fail_count >= 2:
            self.delay_min = min(5.0, self.delay_min + 0.5)
            self.delay_max = min(10.0, self.delay_max + 1.0)
            self.fail_count = 0
    
    def get_delay(self) -> float:
        """í˜„ì¬ ëŒ€ê¸° ì‹œê°„ ë²”ìœ„ì—ì„œ ëœë¤ ê°’ ë°˜í™˜"""
        return random.uniform(self.delay_min, self.delay_max)

# ===========================
# ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤
# ===========================

class KeywordStats:
    """v7.7: í‚¤ì›Œë“œë³„ í†µê³„"""
    
    def __init__(self, keyword: str, target: int):
        self.keyword = keyword
        self.target = target
        self.searched = 0
        self.collected = 0
        self.filtered = 0
        self.duplicates = 0
        self.errors = 0
    
    def get_success_rate(self) -> float:
        if self.searched == 0:
            return 0.0
        return (self.collected / self.searched) * 100
    
    def get_filter_rate(self) -> float:
        if self.searched == 0:
            return 0.0
        return (self.filtered / self.searched) * 100
    
    def get_duplicate_rate(self) -> float:
        if self.searched == 0:
            return 0.0
        return (self.duplicates / self.searched) * 100
    
    def print_summary(self):
        """í‚¤ì›Œë“œ í†µê³„ ì¶œë ¥"""
        progress = f"{self.collected}/{self.target}"
        success_rate = self.get_success_rate()
        filter_rate = self.get_filter_rate()
        dup_rate = self.get_duplicate_rate()
        
        logger.info(f"  [{self.keyword:15s}] {progress:8s} | "
                   f"ì„±ê³µë¥ : {success_rate:5.1f}% | "
                   f"í•„í„°: {filter_rate:5.1f}% | "
                   f"ì¤‘ë³µ: {dup_rate:5.1f}%")

class CrawlStats:
    """í¬ë¡¤ë§ í†µê³„"""
    
    def __init__(self):
        self.total_attempts = 0
        self.success = 0
        self.filtered = 0
        self.duplicates = 0
        self.errors = 0
        self.start_time = time.time()
        self.keyword_stats = {}  # v7.7: í‚¤ì›Œë“œë³„ í†µê³„
    
    def init_keyword(self, keyword: str, target: int):
        """í‚¤ì›Œë“œ í†µê³„ ì´ˆê¸°í™”"""
        self.keyword_stats[keyword] = KeywordStats(keyword, target)
    
    def add_success(self, keyword: str = None):
        self.success += 1
        self.total_attempts += 1
        if keyword and keyword in self.keyword_stats:
            self.keyword_stats[keyword].collected += 1
    
    def add_filtered(self, keyword: str = None):
        self.filtered += 1
        self.total_attempts += 1
        if keyword and keyword in self.keyword_stats:
            self.keyword_stats[keyword].filtered += 1
    
    def add_duplicate(self, keyword: str = None):
        self.duplicates += 1
        self.total_attempts += 1
        if keyword and keyword in self.keyword_stats:
            self.keyword_stats[keyword].duplicates += 1
    
    def add_error(self, keyword: str = None):
        self.errors += 1
        self.total_attempts += 1
        if keyword and keyword in self.keyword_stats:
            self.keyword_stats[keyword].errors += 1
    
    def add_searched(self, keyword: str):
        """ê²€ìƒ‰ ì‹œë„ ì¹´ìš´íŠ¸"""
        if keyword in self.keyword_stats:
            self.keyword_stats[keyword].searched += 1
    
    def print_keyword_stats(self):
        """í‚¤ì›Œë“œë³„ í†µê³„ ì¶œë ¥"""
        logger.info(f"\n{'='*70}")
        logger.info("ğŸ“Š í‚¤ì›Œë“œë³„ ìˆ˜ì§‘ í˜„í™©")
        logger.info(f"{'='*70}")
        
        # ì£¼ìš” í‚¤ì›Œë“œ
        logger.info("\nğŸ¯ ì£¼ìš” í‚¤ì›Œë“œ (ëª©í‘œ: 60ê°œ)")
        for kw_info in PRIMARY_KEYWORDS:
            keyword = kw_info["keyword"]
            if keyword in self.keyword_stats:
                self.keyword_stats[keyword].print_summary()
        
        # ë‚˜ë¨¸ì§€ í‚¤ì›Œë“œ
        logger.info("\nğŸ“Œ ë‚˜ë¨¸ì§€ í‚¤ì›Œë“œ (ëª©í‘œ: 30ê°œ)")
        for kw_info in SECONDARY_KEYWORDS:
            keyword = kw_info["keyword"]
            if keyword in self.keyword_stats:
                self.keyword_stats[keyword].print_summary()
    
    def print_stats(self):
        elapsed = time.time() - self.start_time
        logger.info(f"\n{'='*70}")
        logger.info("ğŸ“Š ì „ì²´ í¬ë¡¤ë§ í†µê³„")
        logger.info(f"{'='*70}")
        logger.info(f"ì´ ì‹œë„: {self.total_attempts}")
        logger.info(f"âœ… ì„±ê³µ: {self.success}")
        logger.info(f"ğŸ” í•„í„°ë§: {self.filtered}")
        logger.info(f"ğŸ”„ ì¤‘ë³µ: {self.duplicates}")
        logger.info(f"âŒ ì—ëŸ¬: {self.errors}")
        logger.info(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ ({elapsed/60:.1f}ë¶„)")
        if self.success > 0:
            logger.info(f"âš¡ í‰ê·  ì†ë„: {elapsed/self.success:.1f}ì´ˆ/ê°œ")
        logger.info(f"{'='*70}")

class FailedURLManager:
    """ì‹¤íŒ¨ URL ê´€ë¦¬"""
    
    def __init__(self, filename='failed_urls.json'):
        self.filename = filename
        self.failed_urls = {}
        self.load_from_file()
    
    def add_failed(self, url: str, reason: str):
        if url not in self.failed_urls:
            self.failed_urls[url] = {
                'reason': reason,
                'count': 1,
                'last_attempt': datetime.now().isoformat()
            }
        else:
            self.failed_urls[url]['count'] += 1
            self.failed_urls[url]['last_attempt'] = datetime.now().isoformat()
    
    def load_from_file(self):
        if os.path.exists(self.filename):
            try:
                with open(self.filename, 'r', encoding='utf-8') as f:
                    self.failed_urls = json.load(f)
            except:
                pass
    
    def save_to_file(self):
        if self.failed_urls:
            with open(self.filename, 'w', encoding='utf-8') as f:
                json.dump(self.failed_urls, f, ensure_ascii=False, indent=2)
    
    def get_failed_count(self) -> int:
        return len(self.failed_urls)

# ===========================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ===========================

def normalize_blog_url(blog_id: str, post_id: str) -> str:
    """ë¸”ë¡œê·¸ URL ì •ê·œí™”"""
    return f"https://blog.naver.com/{blog_id}/{post_id}"

def generate_post_fingerprint(post_data: Dict) -> str:
    """ê²Œì‹œë¬¼ ê³ ìœ  ì§€ë¬¸ ìƒì„± (ì¤‘ë³µ ë°©ì§€)"""
    title = post_data.get('title', '')
    content = post_data.get('content', '')[:200]
    return f"{title}_{content}"

def extract_blog_info_from_url(url: str) -> Optional[Dict[str, str]]:
    """URLì—ì„œ blog_idì™€ post_id ì¶”ì¶œ"""
    try:
        parsed = urlparse(url)
        
        # ë°©ë²• 1: /blog_id/post_id í˜•ì‹
        path_parts = [p for p in parsed.path.split('/') if p]
        if len(path_parts) >= 2:
            return {'blog_id': path_parts[0], 'post_id': path_parts[1]}
        
        # ë°©ë²• 2: ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°
        query_params = parse_qs(parsed.query)
        if 'blogId' in query_params and 'logNo' in query_params:
            return {
                'blog_id': query_params['blogId'][0],
                'post_id': query_params['logNo'][0]
            }
        
        return None
    except:
        return None

def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ì œ"""
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[\r\n\t]', ' ', text)
    return text.strip()

# ===========================
# v7.4: í•„í„°ë§ í•¨ìˆ˜ë“¤
# ===========================

def is_excluded_blog(blog_id: str) -> bool:
    """ì œì™¸ ëŒ€ìƒ ë¸”ë¡œê·¸ì¸ì§€ í™•ì¸ (v7.4)"""
    return blog_id in EXCLUDED_BLOG_IDS

def is_media_style_title(title: str) -> bool:
    """ì–¸ë¡  ìŠ¤íƒ€ì¼ ì œëª©ì¸ì§€ í™•ì¸ (v7.4)"""
    for pattern in MEDIA_TITLE_PATTERNS:
        if re.search(pattern, title):
            return True
    return False

# ===========================
# ë‚ ì§œ+ì‹œê°„ ì¶”ì¶œ í•¨ìˆ˜
# ===========================

def parse_published_date(date_text: str) -> str:
    """v7.6: v7.1ì˜ ê²€ì¦ëœ ë‚ ì§œ íŒŒì‹± í•¨ìˆ˜ (ì‹œê°„ ì œì™¸, ë‚ ì§œë§Œ)"""
    if not date_text:
        return ""
    
    try:
        # ë¶ˆí•„ìš”í•œ ê³µë°± ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        date_text = re.sub(r'\s+', ' ', date_text.strip())
        
        # íŒ¨í„´ 1: YYYY. MM. DD. HH:MM (ì‹œê°„ í¬í•¨ í˜•ì‹)
        match = re.search(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.\s*\d{1,2}:\d{2}', date_text)
        if match:
            year, month, day = match.groups()
            return f"{year}-{int(month):02d}-{int(day):02d}"
        
        # íŒ¨í„´ 2: YYYY. MM. DD. (ì  í¬í•¨ í˜•ì‹)
        match = re.search(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.', date_text)
        if match:
            year, month, day = match.groups()
            return f"{year}-{int(month):02d}-{int(day):02d}"
        
        # íŒ¨í„´ 3: YYYY-MM-DD (í•˜ì´í”ˆ í˜•ì‹)
        match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', date_text)
        if match:
            year, month, day = match.groups()
            return f"{year}-{int(month):02d}-{int(day):02d}"
        
        return ""
    except Exception as e:
        logger.debug(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_text} - {str(e)}")
        return ""


# ===========================
# ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜
# ===========================

def extract_sponsor_phone(text: str) -> str:
    """ì¶”ì²œì¸ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ"""
    if not text:
        return ""
    
    # 010-xxxx-xxxx í˜•ì‹ë§Œ ìˆ˜ì§‘
    phone_patterns = [
        r'010[-\s]?\d{4}[-\s]?\d{4}',
        r'ì¶”ì²œì¸.*?010[-\s]?\d{4}[-\s]?\d{4}',
        r'ë¬¸ì˜.*?010[-\s]?\d{4}[-\s]?\d{4}',
        r'ì—°ë½ì²˜.*?010[-\s]?\d{4}[-\s]?\d{4}',
    ]
    
    for pattern in phone_patterns:
        match = re.search(pattern, text)
        if match:
            phone = match.group(0)
            # ìˆ«ìë§Œ ì¶”ì¶œ í›„ 010ìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
            digits = re.sub(r'\D', '', phone)
            if digits.startswith('010') and len(digits) == 11:
                return f"{digits[:3]}-{digits[3:7]}-{digits[7:]}"
    
    return ""

def extract_sponsor_partner_id(text: str) -> str:
    """ì¶”ì²œì¸ íŒŒíŠ¸ë„ˆ ID ì¶”ì¶œ (ì •í™•íˆ 8ìë¦¬ ìˆ«ìë§Œ)"""
    if not text:
        return ""
    
    # ì •í™•íˆ 8ìë¦¬ ìˆ«ìë§Œ ì¶”ì¶œ
    partner_patterns = [
        r'ì¶”ì²œì¸\s*(?:ì½”ë“œ|ë²ˆí˜¸|ID)?\s*[:ï¼š]?\s*(\d{8})\b',
        r'íŒŒíŠ¸ë„ˆ\s*(?:ì½”ë“œ|ë²ˆí˜¸|ID)?\s*[:ï¼š]?\s*(\d{8})\b',
        r'ë“±ë¡\s*(?:ì½”ë“œ|ë²ˆí˜¸|ID)?\s*[:ï¼š]?\s*(\d{8})\b',
        r'\b(\d{8})\b',
    ]
    
    for pattern in partner_patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            # ì •í™•íˆ 8ìë¦¬ì¸ì§€ í™•ì¸
            if len(match) == 8:
                return match
    
    return ""

def extract_hashtags(soup: BeautifulSoup, content_text: str) -> str:
    """í•´ì‹œíƒœê·¸ ì¶”ì¶œ (v7.5: ë©”íƒ€ íƒœê·¸ í•„í„°ë§ ê°•í™”)"""
    hashtags = set()
    
    # ì œì™¸í•  ë©”íƒ€ íƒœê·¸ ë¦¬ìŠ¤íŠ¸ (placeholder íƒœê·¸ë“¤)
    meta_tags = {'#íƒœê·¸', '#tag', '#í•´ì‹œíƒœê·¸', '#hashtag', '#tags'}
    
    # ë°©ë²• 1: ê²Œì‹œë¬¼ í•˜ë‹¨ì˜ íƒœê·¸ ì˜ì—­ì—ì„œ ì¶”ì¶œ
    tag_elements = soup.select('a.link_tag, a[href*="tag"], .se_tag a, .post_tag a')
    for elem in tag_elements:
        tag_text = elem.get_text(strip=True)
        if tag_text:
            if not tag_text.startswith('#'):
                tag_text = '#' + tag_text
            # ë©”íƒ€ íƒœê·¸ í•„í„°ë§
            if tag_text.lower() not in meta_tags:
                hashtags.add(tag_text)
    
    # ë°©ë²• 2: ë³¸ë¬¸ì—ì„œ #íƒœê·¸ ì¶”ì¶œ
    hashtag_pattern = r'#([ê°€-í£a-zA-Z0-9_]+)'
    matches = re.findall(hashtag_pattern, content_text)
    for match in matches:
        tag_text = '#' + match
        # ë©”íƒ€ íƒœê·¸ í•„í„°ë§
        if tag_text.lower() not in meta_tags:
            hashtags.add(tag_text)
    
    return ', '.join(sorted(list(hashtags))) if hashtags else ""

def extract_image_urls(soup: BeautifulSoup) -> str:
    """ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
    image_urls = set()
    
    # ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ì„ íƒì
    img_elements = soup.select('img[src], img[data-src], .se-image-resource')
    
    for img in img_elements:
        src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
        if src and ('blogfiles.naver.net' in src or 'pstatic.net' in src):
            # ì¸ë„¤ì¼ì´ ì•„ë‹Œ ì›ë³¸ ì´ë¯¸ì§€ URLë¡œ ë³€í™˜
            src = re.sub(r'\?type=\w\d+', '', src)
            image_urls.add(src)
    
    return ', '.join(list(image_urls)[:10]) if image_urls else ""

def extract_video_urls(soup: BeautifulSoup) -> str:
    """ë¹„ë””ì˜¤ URL ì¶”ì¶œ (v7.5: ë„¤ì´ë²„ ì˜ìƒ í¬í•¨)"""
    video_urls = set()
    
    # ë¹„ë””ì˜¤ ë° iframe ì„ íƒì (YouTube + ë„¤ì´ë²„ ì˜ìƒ)
    video_selectors = [
        'video source',
        'video[src]',
        'iframe[src*="youtube"]',
        'iframe[src*="youtu.be"]',
        'iframe[src*="vimeo"]',
        'iframe[src*="tv.naver"]',  # ë„¤ì´ë²„ TV
        'iframe[src*="naver.com/video"]',  # ë„¤ì´ë²„ ë™ì˜ìƒ
        'iframe[src*="blog.naver.com/PostView"]',  # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë‚´ì¥ ë™ì˜ìƒ
        '.se-video iframe',  # ìŠ¤ë§ˆíŠ¸ì—ë””í„° ë¹„ë””ì˜¤
        '.se-component-content[data-type="video"] iframe'
    ]
    
    for selector in video_selectors:
        elements = soup.select(selector)
        for elem in elements:
            src = elem.get('src') or elem.get('data-src')
            if src:
                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    src = 'https://blog.naver.com' + src
                video_urls.add(src)
    
    return ', '.join(list(video_urls)[:10]) if video_urls else ""

def extract_like_count(driver: webdriver.Chrome, soup: BeautifulSoup) -> int:
    """ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ"""
    try:
        # ë°©ë²• 1: Seleniumìœ¼ë¡œ ì¶”ì¶œ
        like_selectors = [
            '.btn_empathy .count',
            '.area_like .count',
            'em.u_cnt._count',
            '.btn_like .count'
        ]
        
        for selector in like_selectors:
            try:
                elem = driver.find_element(By.CSS_SELECTOR, selector)
                like_text = elem.text.strip()
                like_count = int(re.sub(r'\D', '', like_text))
                if like_count > 0:
                    return like_count
            except:
                continue
        
        # ë°©ë²• 2: BeautifulSoupìœ¼ë¡œ ì¶”ì¶œ
        page_text = soup.get_text()
        like_patterns = [
            r'ê³µê°\s*(\d+)',
            r'ì¢‹ì•„ìš”\s*(\d+)',
            r'empathy.*?(\d+)'
        ]
        for pattern in like_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                return int(match.group(1))
        
        return 0
    except Exception as e:
        logger.debug(f"ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return 0

def extract_comment_count(driver: webdriver.Chrome, soup: BeautifulSoup) -> int:
    """ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ (v7.5: ì •í™•ë„ ê°œì„ )"""
    try:
        # ë°©ë²• 1: ëŒ“ê¸€ ì˜ì—­ íŠ¹ì • ì„ íƒì (ê°€ì¥ ì •í™•)
        comment_specific_selectors = [
            '.btn_comment em.u_cnt',  # ëŒ“ê¸€ ë²„íŠ¼ì˜ ì¹´ìš´íŠ¸ë§Œ
            'a.btn_comment .count',
            '.comment_count',
            '.cmt_count',
            'span[class*="comment"] em.u_cnt'
        ]
        
        for selector in comment_specific_selectors:
            try:
                elem = driver.find_element(By.CSS_SELECTOR, selector)
                comment_text = elem.text.strip()
                # ìˆ«ìë§Œ ì¶”ì¶œ
                numbers = re.findall(r'\d+', comment_text)
                if numbers:
                    count = int(numbers[0])
                    # ë¹„ì •ìƒì ìœ¼ë¡œ í° ìˆ«ì í•„í„°ë§ (ì—°ë„ ë“±)
                    if count < 10000:
                        return count
            except:
                continue
        
        # ë°©ë²• 2: ëŒ“ê¸€ ëª©ë¡ì—ì„œ ì§ì ‘ ì¹´ìš´íŠ¸
        try:
            comment_list = driver.find_elements(By.CSS_SELECTOR, '.se-comment-item, .comment_list .comment_item, #comment_list .comment_item')
            if comment_list:
                return len(comment_list)
        except:
            pass
        
        # ë°©ë²• 3: BeautifulSoupìœ¼ë¡œ ëŒ“ê¸€ ì˜ì—­ì—ì„œë§Œ ì¶”ì¶œ (ì¢ì€ ë²”ìœ„)
        comment_area = soup.select_one('.se-comment-area, .comment_area, #comment, .comment-area')
        if comment_area:
            # "ëŒ“ê¸€ Nê°œ" íŒ¨í„´
            text = comment_area.get_text()
            match = re.search(r'ëŒ“ê¸€\s*(\d+)', text)
            if match:
                count = int(match.group(1))
                if count < 10000:
                    return count
        
        # ë°©ë²• 4: ì „ì²´ í˜ì´ì§€ì—ì„œ ê²€ìƒ‰ (ìµœí›„ ìˆ˜ë‹¨)
        page_text = soup.get_text()
        # ì¢ì€ íŒ¨í„´ ë¨¼ì € ì‹œë„
        patterns = [
            r'ëŒ“ê¸€\s*(\d{1,3})\s*ê°œ',  # "ëŒ“ê¸€ Nê°œ" (ìµœëŒ€ 3ìë¦¬)
            r'ëŒ“ê¸€\s*(\d{1,3})(?!\d)',  # "ëŒ“ê¸€ N" (ë’¤ì— ìˆ«ì ì—†ìŒ)
        ]
        for pattern in patterns:
            match = re.search(pattern, page_text)
            if match:
                count = int(match.group(1))
                # ì—°ë„ë‚˜ í° ìˆ«ì í•„í„°ë§
                if count < 1000:
                    return count
        
        return 0
    except Exception as e:
        logger.debug(f"ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return 0

# ===========================
# v7.4: ë‹¤ì¸µ í•„í„°ë§ í•¨ìˆ˜
# ===========================

def content_passes_filter(title: str, content: str, full_text: str, 
                          blog_id: str, sponsor_partner_id: str) -> Tuple[bool, str]:
    """ì½˜í…ì¸  í•„í„°ë§ (v7.4: ë‹¤ì¸µ í•„í„°ë§)
    
    [ë‹¨ê³„ 1] ë¸”ë™ë¦¬ìŠ¤íŠ¸ blog_id ì²´í¬
    [ë‹¨ê³„ 2] ì–¸ë¡  ìŠ¤íƒ€ì¼ ì œëª© ì²´í¬
    [ë‹¨ê³„ 3] PM ë¸Œëœë“œ í‚¤ì›Œë“œ ì²´í¬
    [ë‹¨ê³„ 4] íŒë§¤ì› í™œë™ í‚¤ì›Œë“œ ì²´í¬
    [ë‹¨ê³„ 5] ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
    
    Returns:
        (í†µê³¼ì—¬ë¶€, ì‹¤íŒ¨ì‚¬ìœ )
    """
    
    # [ë‹¨ê³„ 1] ë¸”ë™ë¦¬ìŠ¤íŠ¸ blog_id ì²´í¬
    if is_excluded_blog(blog_id):
        return False, f"ì œì™¸ ëŒ€ìƒ ë¸”ë¡œê·¸: {blog_id}"
    
    # [ë‹¨ê³„ 2] ì–¸ë¡  ìŠ¤íƒ€ì¼ ì œëª© ì²´í¬
    if is_media_style_title(title):
        return False, "ì–¸ë¡  ìŠ¤íƒ€ì¼ ì œëª©"
    
    # [ë‹¨ê³„ 3] PM ë¸Œëœë“œ í‚¤ì›Œë“œ ì²´í¬
    text_lower = full_text.lower()
    has_pm_keyword = any(keyword.lower() in text_lower for keyword in PM_BRAND_KEYWORDS)
    if not has_pm_keyword:
        return False, "PM ë¸Œëœë“œ í‚¤ì›Œë“œ ì—†ìŒ"
    
    # [ë‹¨ê³„ 4] íŒë§¤ì› í™œë™ í‚¤ì›Œë“œ ì²´í¬
    has_sales_keyword = any(keyword in full_text for keyword in PM_SALES_KEYWORDS)
    has_8digit = bool(sponsor_partner_id)
    
    if not (has_sales_keyword or has_8digit):
        return False, "íŒë§¤ì› ê´€ë ¨ í‚¤ì›Œë“œ ì—†ìŒ"
    
    # [ë‹¨ê³„ 5] ì œì™¸ í‚¤ì›Œë“œ ì²´í¬ (2ê°œ ì´ìƒ ì‹œ ì œì™¸)
    exclude_count = sum(1 for keyword in EXCLUDE_KEYWORDS if keyword in full_text)
    if exclude_count >= 2:
        return False, f"ì œì™¸ í‚¤ì›Œë“œ {exclude_count}ê°œ ë°œê²¬"
    
    return True, ""

# ===========================
# Selenium ë“œë¼ì´ë²„ ì„¤ì •
# ===========================

def setup_driver() -> webdriver.Chrome:
    """Selenium ë“œë¼ì´ë²„ ì„¤ì •"""
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # ë©”ëª¨ë¦¬ ìµœì í™”
    chrome_options.add_argument('--disable-extensions')
    chrome_options.add_argument('--disable-plugins')
    chrome_options.add_argument('--disable-images')
    chrome_options.add_argument('--blink-settings=imagesEnabled=false')
    
    # v7.4: User-Agent ëœë¤ ì„ íƒ
    user_agent = random.choice(USER_AGENTS)
    chrome_options.add_argument(f'user-agent={user_agent}')
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)
    
    # ìë™í™” ê°ì§€ ìš°íšŒ
    driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
        'source': '''
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            })
        '''
    })
    
    return driver

# ===========================
# ê²€ìƒ‰ í•¨ìˆ˜
# ===========================

def search_naver_blog_api(keyword: str, display: int = 100, start: int = 1) -> Optional[Dict]:
    """Naver Open Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡œê·¸ ê²€ìƒ‰"""
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        logger.warning("âš ï¸  Naver API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë˜í•‘ ë°©ì‹ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
        return None
    
    # displayëŠ” ìµœëŒ€ 100ê°œë¡œ ì œí•œ
    display = min(display, 100)
    
    url = "https://openapi.naver.com/v1/search/blog.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
        "User-Agent": random.choice(USER_AGENTS)
    }
    params = {
        "query": keyword,
        "display": display,
        "start": start,
        "sort": "date"  # v8.1: ìµœì‹ ìˆœ ì •ë ¬ (ê¸°ê°„ ë‹¤ì–‘ì„± í™•ë³´)
    }
    
    logger.debug(f"API ìš”ì²­: {url}")
    logger.debug(f"í—¤ë”: Client-Id={NAVER_CLIENT_ID[:10]}...")
    logger.debug(f"íŒŒë¼ë¯¸í„°: {params}")
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        logger.error(f"API ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {str(e)}")
        return None

def parse_search_results(search_data: Dict) -> List[Dict]:
    """API ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±"""
    results = []
    
    if not search_data or 'items' not in search_data:
        return results
    
    for item in search_data['items']:
        try:
            url = item.get('link', '')
            if not url or 'blog.naver.com' not in url:
                continue
            
            blog_info = extract_blog_info_from_url(url)
            if not blog_info:
                continue
            
            results.append({
                'url': url,
                'blog_id': blog_info['blog_id'],
                'post_id': blog_info['post_id'],
                'postdate': item.get('postdate', ''),
                'bloggername': item.get('bloggername', '')
            })
        except Exception as e:
            continue
    
    logger.info(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±: {len(results)}ê°œ")
    return results

def search_naver_blog_scraping(keyword: str, max_results: int = MAX_SEARCH_RESULTS) -> List[Dict]:
    """ì›¹ ìŠ¤í¬ë˜í•‘ì„ ì‚¬ìš©í•œ ë¸”ë¡œê·¸ ê²€ìƒ‰ (í´ë°±ìš©)"""
    results = []
    page = 1
    
    while len(results) < max_results:
        try:
            start = (page - 1) * 10 + 1
            search_url = f"https://search.naver.com/search.naver?where=blog&query={keyword}&start={start}"
            
            headers = {
                'User-Agent': random.choice(USER_AGENTS)
            }
            
            response = requests.get(search_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„)
            blog_items = soup.select('.view_wrap, .total_wrap, .lst_total, .api_ani_send')
            
            # ì„ íƒìê°€ ì‘ë™í•˜ì§€ ì•Šìœ¼ë©´ ì§ì ‘ ë§í¬ ì°¾ê¸°
            if not blog_items:
                blog_links = soup.select('a[href*="blog.naver.com"]')
                logger.debug(f"ì§ì ‘ ë§í¬ ê²€ìƒ‰: {len(blog_links)}ê°œ ë°œê²¬")
                
                for link in blog_links:
                    url = link.get('href', '')
                    title = link.get_text(strip=True) or link.get('title', '')
                    
                    if 'blog.naver.com' in url and title:
                        blog_info = extract_blog_info_from_url(url)
                        if blog_info:
                            results.append({
                                'title': title,
                                'url': url,
                                'blog_id': blog_info['blog_id'],
                                'post_id': blog_info['post_id']
                            })
                        
                        if len(results) >= max_results:
                            break
                
                if results:
                    continue
                else:
                    logger.debug(f"í˜ì´ì§€ {page}: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    break
            
            for item in blog_items:
                title_elem = item.select_one('.title_link, .api_txt_lines')
                url_elem = item.select_one('a.title_link, a.api_txt_lines')
                
                if title_elem and url_elem:
                    title = title_elem.get_text(strip=True)
                    url = url_elem.get('href', '')
                    
                    if 'blog.naver.com' in url:
                        blog_info = extract_blog_info_from_url(url)
                        if blog_info:
                            results.append({
                                'title': title,
                                'url': url,
                                'blog_id': blog_info['blog_id'],
                                'post_id': blog_info['post_id']
                            })
                        
                        if len(results) >= max_results:
                            break
            
            page += 1
            time.sleep(random.uniform(0.5, 1.0))
            
        except Exception as e:
            logger.error(f"ìŠ¤í¬ë˜í•‘ ê²€ìƒ‰ ì˜¤ë¥˜ (í‚¤ì›Œë“œ: {keyword}, í˜ì´ì§€: {page}): {str(e)}")
            break
    
    logger.info(f"ğŸ” '{keyword}' ìŠ¤í¬ë˜í•‘ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
    return results

def search_naver_blog(keyword: str, max_results: int = MAX_SEARCH_RESULTS) -> List[Dict]:
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ (API ìš°ì„ , ìŠ¤í¬ë˜í•‘ í´ë°±)"""
    # API ë°©ì‹ ì‹œë„
    search_data = search_naver_blog_api(keyword, max_results)
    if search_data:
        results = parse_search_results(search_data)
        if results:
            logger.info(f"ğŸ” '{keyword}' API ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
            return results
    
    # ìŠ¤í¬ë˜í•‘ ë°©ì‹ í´ë°±
    logger.warning(f"âš ï¸  '{keyword}' API ì‹¤íŒ¨ - ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ í´ë°±")
    return search_naver_blog_scraping(keyword, max_results)

# ===========================
# í¬ë¡¤ë§ í•¨ìˆ˜
# ===========================

def crawl_blog_post_selenium(driver: webdriver.Chrome, url: str, blog_id: str, 
                            post_id: str, failed_url_manager: FailedURLManager) -> Optional[Dict]:
    """Seleniumì„ ì‚¬ìš©í•œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ í¬ë¡¤ë§ (v7.4)"""
    try:
        logger.debug(f"í¬ë¡¤ë§ ì‹œì‘: {url}")
        driver.get(url)
        
        # iframe ëŒ€ê¸° ë° ì „í™˜ (v7.6: íƒ€ì„ì•„ì›ƒ ë‹¨ì¶• 10ì´ˆâ†’3ì´ˆ)
        try:
            WebDriverWait(driver, 3).until(
                EC.presence_of_element_located((By.ID, 'mainFrame'))
            )
            driver.switch_to.frame('mainFrame')
        except TimeoutException:
            logger.debug("iframe ì—†ìŒ - ë³¸ë¬¸ ì§ì ‘ í¬ë¡¤ë§")
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (v7.6: 2ì´ˆâ†’1ì´ˆ)
        time.sleep(1)
        
        # HTML íŒŒì‹±
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ
        title = ""
        title_selectors = [
            '.se-title-text', '.pcol1', '.se_title', 
            '.post-view .tit', '.tit_h3', 'h3.se_title'
        ]
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = clean_text(title_elem.get_text())
                break
        
        if not title:
            failed_url_manager.add_failed(url, "ì œëª© ì—†ìŒ")
            return None
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        content = ""
        content_selectors = [
            '.se-main-container', '.post-view', '.se_component_wrap',
            '#postViewArea', '.post_ct'
        ]
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = clean_text(content_elem.get_text())
                break
        
        if not content:
            failed_url_manager.add_failed(url, "ë³¸ë¬¸ ì—†ìŒ")
            return None
        
        # ë°œí–‰ ë‚ ì§œ+ì‹œê°„ ì¶”ì¶œ
        published_datetime = ""
        date_selectors = [
            '.se_publishDate', '.post-view .date', '.se_date',
            '.post_info .date', 'span.se_publishDate', '.blog2_series .date',
            '.blog-category .date', '.post_date', 'p.date', 'span.date',
            '.post-meta .date', '.entry-date'
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                published_datetime = parse_published_date(date_text)
                if published_datetime:
                    break
        
        # ì „ì²´ í…ìŠ¤íŠ¸ (í•„í„°ë§ìš©)
        full_text = f"{title} {content}"
        
        # ì¶”ì²œì¸ ì •ë³´ ì¶”ì¶œ
        sponsor_phone = extract_sponsor_phone(full_text)
        sponsor_partner_id = extract_sponsor_partner_id(full_text)
        
        # v7.4: ë‹¤ì¸µ í•„í„°ë§ ê²€ì‚¬
        passes, reason = content_passes_filter(title, content, full_text, blog_id, sponsor_partner_id)
        if not passes:
            logger.debug(f"í•„í„°ë§ë¨: {reason} - {title[:50]}")
            failed_url_manager.add_failed(url, f"í•„í„°ë§: {reason}")
            return None
        
        # í•´ì‹œíƒœê·¸ ì¶”ì¶œ (v7.4: ê°œì„ ëœ ë°©ì‹)
        hashtags = extract_hashtags(soup, content)
        
        # ì´ë¯¸ì§€/ë¹„ë””ì˜¤ URL ì¶”ì¶œ
        image_urls = extract_image_urls(soup)
        video_urls = extract_video_urls(soup)
        
        # ì¢‹ì•„ìš”/ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ
        like_count = extract_like_count(driver, soup)
        comment_count = extract_comment_count(driver, soup)
        
        # v7.4: ë°ì´í„° êµ¬ì„± (post_id í˜•ì‹ ë³€ê²½)
        post_data = {
            'platform': 'naver_blog',
            'post_id': post_id,  # v7.4: blog_id ì¤‘ë³µ ì œê±°
            'blog_id': blog_id,
            'url': url,
            'title': title,
            'content': content,
            'published_datetime': published_datetime,
            'sponsor_phone': sponsor_phone,
            'sponsor_partner_id': sponsor_partner_id,
            'like_count': like_count,
            'comment_count': comment_count,
            'hashtags': hashtags,
            'image_urls': image_urls,
            'video_urls': video_urls,
            'collected_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        driver.switch_to.default_content()
        return post_data
        
    except TimeoutException:
        logger.debug(f"íƒ€ì„ì•„ì›ƒ: {url}")
        failed_url_manager.add_failed(url, "íƒ€ì„ì•„ì›ƒ")
        return None
    except Exception as e:
        logger.debug(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
        failed_url_manager.add_failed(url, f"ì˜¤ë¥˜: {str(e)}")
        return None
    finally:
        try:
            driver.switch_to.default_content()
        except:
            pass

# ===========================
# ë©”ì¸ í•¨ìˆ˜
# ===========================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (v8.1)"""
    logger.info("="*70)
    logger.info(f"ğŸš€ PM International ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v8.1 ì‹œì‘")
    logger.info(f"ğŸ¯ ëª©í‘œ: 15,000ê°œ ì´ìƒ ëŒ€ìš©ëŸ‰ ìˆ˜ì§‘")
    logger.info(f"ğŸ“… ê¸°ê°„: 2023~2025ë…„ (3ë…„ì¹˜ ë°ì´í„°)")
    logger.info(f"ğŸ” í‚¤ì›Œë“œ: {len(ALL_KEYWORDS)}ê°œ (ì—°ë„ë³„ ì¡°í•©)")
    logger.info(f"ğŸ”‹ ë§¥ë¶ ì‚¬ìš©ì ì£¼ì˜: ì „ì›ì„ ì—°ê²°í•˜ê³  ì ˆì „ ëª¨ë“œë¥¼ í•´ì œí•´ì£¼ì„¸ìš”!")
    logger.info(f"âš¡ v8.1: í‚¤ì›Œë“œë‹¹ 1000ê°œ, sort=date, ë§¤íŠ¸ë¦¬ìŠ¤ ì—…ì²´ í•„í„°ë§")
    logger.info("="*70)
    
    driver = setup_driver()
    stats = CrawlStats()
    failed_url_manager = FailedURLManager()
    adaptive = AdaptiveDelay(initial_min=REQUEST_DELAY_MIN, initial_max=REQUEST_DELAY_MAX)
    
    # v7.7: í‚¤ì›Œë“œë³„ í†µê³„ ì´ˆê¸°í™”
    for kw_info in ALL_KEYWORDS:
        stats.init_keyword(kw_info["keyword"], kw_info["target"])
    
    collected_posts = []
    collected_urls = set()
    collected_fingerprints = set()
    consecutive_errors = 0
    crawl_count = 0
    keyword_collected = {}  # í‚¤ì›Œë“œë³„ ìˆ˜ì§‘ ê°œìˆ˜
    
    try:
        # v7.7: í‚¤ì›Œë“œë³„ í¬ë¡¤ë§ (ëª©í‘œ ê°œìˆ˜ ì œí•œ)
        for kw_info in ALL_KEYWORDS:
            keyword = kw_info["keyword"]
            target = kw_info["target"]
            keyword_collected[keyword] = 0
            
            if len(collected_posts) >= TOTAL_TARGET:
                break
            
            logger.info(f"\n{'='*70}")
            logger.info(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰: {keyword} (ëª©í‘œ: {target}ê°œ)")
            logger.info(f"{'='*70}")
            
            search_results = search_naver_blog(keyword, MAX_SEARCH_RESULTS)
            
            if not search_results:
                logger.warning(f"'{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                continue
            
            for result in search_results:
                # ì „ì²´ ëª©í‘œ ë‹¬ì„± ì²´í¬
                if len(collected_posts) >= TOTAL_TARGET:
                    break
                
                # í‚¤ì›Œë“œë³„ ëª©í‘œ ë‹¬ì„± ì²´í¬
                if keyword_collected[keyword] >= target:
                    logger.info(f"âœ… '{keyword}' ëª©í‘œ ë‹¬ì„±: {keyword_collected[keyword]}/{target}")
                    break
                
                blog_id = result['blog_id']
                post_id = result['post_id']
                normalized_url = normalize_blog_url(blog_id, post_id)
                
                # ê²€ìƒ‰ ì‹œë„ ì¹´ìš´íŠ¸
                stats.add_searched(keyword)
                
                # ì¤‘ë³µ ì²´í¬
                if normalized_url in collected_urls:
                    stats.add_duplicate(keyword)
                    continue
                
                logger.info(f"[ì „ì²´: {len(collected_posts)+1}/{TOTAL_TARGET}] "
                           f"[{keyword}: {keyword_collected[keyword]+1}/{target}] í¬ë¡¤ë§ ì¤‘...")
                
                # í¬ë¡¤ë§ ì‹¤í–‰
                post_data = crawl_blog_post_selenium(
                    driver, normalized_url, blog_id, post_id, failed_url_manager
                )
                
                if post_data:
                    # ì¤‘ë³µ ì²´í¬ (ì§€ë¬¸ ê¸°ë°˜)
                    fingerprint = generate_post_fingerprint(post_data)
                    if fingerprint not in collected_fingerprints:
                        collected_posts.append(post_data)
                        collected_urls.add(normalized_url)
                        collected_fingerprints.add(fingerprint)
                        keyword_collected[keyword] += 1
                        stats.add_success(keyword)
                        consecutive_errors = 0
                        adaptive.on_success()
                        logger.info(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {post_data['title'][:50]}")
                    else:
                        stats.add_duplicate(keyword)
                else:
                    stats.add_filtered(keyword)
                    consecutive_errors += 1
                    adaptive.on_fail()
                
                # ì—°ì† ì—ëŸ¬ ì‹œ ë“œë¼ì´ë²„ ì¬ì‹œì‘
                if consecutive_errors >= MAX_CONSECUTIVE_ERRORS:
                    logger.warning(f"âš ï¸  ì—°ì† {MAX_CONSECUTIVE_ERRORS}íšŒ ì—ëŸ¬ - ë“œë¼ì´ë²„ ì¬ì‹œì‘ (v7.6: ëŒ€ìš©ëŸ‰ ìµœì í™”)")
                    driver.quit()
                    time.sleep(3)
                    driver = setup_driver()
                    consecutive_errors = 0
                    gc.collect()
                
                crawl_count += 1
                
                # ì ì‘í˜• ëŒ€ê¸° ì‹œê°„
                delay = adaptive.get_delay()
                time.sleep(delay)
                
                # v7.7: ì£¼ê¸°ì  í†µê³„ ì¶œë ¥ (50ê°œë§ˆë‹¤)
                if crawl_count % 50 == 0:
                    stats.print_keyword_stats()
                    gc.collect()
            
            # í‚¤ì›Œë“œ ì™„ë£Œ í›„ ì§§ì€ ëŒ€ê¸°
            if len(collected_posts) < TOTAL_TARGET:
                time.sleep(random.uniform(1, 2))
        
        # v7.7: ìµœì¢… í†µê³„ ì¶œë ¥
        stats.print_keyword_stats()
        stats.print_stats()
        
        # CSV ì €ì¥
        if collected_posts:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'naver_blog_pm_v8_1_{timestamp}.csv'
            
            df = pd.DataFrame(collected_posts)
            
            # ì»¬ëŸ¼ ìˆœì„œ ëª…ì‹œ
            column_order = [
                'platform', 'post_id', 'blog_id', 'url', 'title', 'content',
                'published_datetime', 'sponsor_phone', 'sponsor_partner_id',
                'like_count', 'comment_count', 'hashtags', 'image_urls',
                'video_urls', 'collected_date'
            ]
            
            df = df[column_order]
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            logger.info(f"\n{'='*70}")
            logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
            logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(collected_posts)}ê°œ")
            logger.info(f"ğŸ“‹ ì»¬ëŸ¼: {len(column_order)}ê°œ")
            logger.info(f"{'='*70}")
        else:
            logger.warning("âš ï¸  ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        failed_url_manager.save_to_file()
        if failed_url_manager.get_failed_count() > 0:
            logger.info(f"âŒ ì‹¤íŒ¨ URL: {failed_url_manager.get_failed_count()}ê°œ (failed_urls.json)")
    
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  ì‚¬ìš©ì ì¤‘ë‹¨")
        stats.print_stats()
    
    except Exception as e:
        logger.error(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        stats.print_stats()
    
    finally:
        driver.quit()
        logger.info("âœ… ë“œë¼ì´ë²„ ì¢…ë£Œ")
        logger.info("="*70)
        logger.info("ğŸ í¬ë¡¤ë§ ì™„ë£Œ")
        logger.info("="*70)

if __name__ == "__main__":
    main()
