#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PM-International Korea ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v7.0 (ìµœì¢… ì™„ì„±íŒ)

ğŸ¯ ì£¼ìš” ê°œì„  ì‚¬í•­:
1. âœ… hashtags: ì´ì „ ë²„ì „ì˜ ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ë¡œì§ìœ¼ë¡œ ë³µì› (# ê¸°í˜¸ ìœ ì§€)
2. âœ… blogger_name ì‚­ì œ â†’ sponsor_nameë§Œ ì‚¬ìš©
3. âœ… view_count: ì™„ì „ ì‚­ì œ (ìˆ˜ì§‘ ë¶ˆê°€ëŠ¥)
4. âœ… sponsor_phone, sponsor_partner_id: íŒ¨í„´ ëŒ€í­ ê°•í™”
5. âœ… ì•ˆì •ì„± ë° ì†ë„ ìµœì í™”

ğŸ“Š ì¶œë ¥ ì»¬ëŸ¼ (16ê°œ):
- ê¸°ë³¸: platform, post_id, blog_id, url, title, content, published_date
- ì¶”ì²œì¸: sponsor_name, sponsor_phone, sponsor_partner_id
- ì°¸ì—¬: like_count, comment_count
- ì½˜í…ì¸ : hashtags, image_urls, video_urls
- ë©”íƒ€: collected_date

ì‘ì„±ì: PMI Korea ë°ì´í„° ë¶„ì„íŒ€
ë²„ì „: 7.0
ìµœì¢… ìˆ˜ì •ì¼: 2025-11-05
"""

import os
import re
import json
import time
import random
import logging
from datetime import datetime
from urllib.parse import urlparse, parse_qs, unquote
from typing import List, Dict, Optional, Set, Tuple

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd

# ===========================
# ë¡œê¹… ì„¤ì •
# ===========================

class ColoredFormatter(logging.Formatter):
    """ì»¬ëŸ¬ ë¡œê¹… í¬ë§·í„°"""
    
    COLORS = {
        'DEBUG': '\033[94m',    # íŒŒë€ìƒ‰
        'INFO': '\033[92m',     # ì´ˆë¡ìƒ‰
        'WARNING': '\033[93m',  # ë…¸ë€ìƒ‰
        'ERROR': '\033[91m',    # ë¹¨ê°„ìƒ‰
        'CRITICAL': '\033[95m', # ë³´ë¼ìƒ‰
        'RESET': '\033[0m'
    }
    
    def format(self, record):
        log_color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
        record.levelname = f"{log_color}{record.levelname}{self.COLORS['RESET']}"
        return super().format(record)

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# íŒŒì¼ í•¸ë“¤ëŸ¬
file_handler = logging.FileHandler(
    f'naver_blog_crawler_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log',
    encoding='utf-8'
)
file_handler.setLevel(logging.DEBUG)
file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(file_formatter)

# ì½˜ì†” í•¸ë“¤ëŸ¬
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_formatter = ColoredFormatter('%(levelname)s - %(message)s')
console_handler.setFormatter(console_formatter)

logger.addHandler(file_handler)
logger.addHandler(console_handler)

# ===========================
# ì„¤ì • ì˜ì—­
# ===========================

# Naver Open API ì„¤ì •
NAVER_CLIENT_ID = "9v7cOolOk2ctSQXc73sd"
NAVER_CLIENT_SECRET = "9jHcXVNQwZ"

# ê²€ìƒ‰ í‚¤ì›Œë“œ (PM International ê´€ë ¨)
SEARCH_KEYWORDS = [
    "í”¼ì— ì¸í„°ë‚´ì…”ë„",
    "í”¼ì— ì½”ë¦¬ì•„", 
    "PMì¸í„°ë‚´ì…”ë„",
    "ë…ì¼í”¼ì— ",
    "í•ë¼ì¸",
    "í”¼íŠ¸ë¼ì¸",
    "FitLine",
    "ë² ì´ì‹ìŠ¤",
    "ë² ì´ì§ìŠ¤",
    "í”„ë¡œì…°ì´í”„",
    "í”„ë¡œì‰ì´í”„",
    "ì—‘í‹°ë°”ì´ì¦ˆ",
    "íŒŒì›Œì¹µí…Œì¼",
    "ë¦¬ìŠ¤í† ë ˆì´íŠ¸",
    "íƒ‘ì‰ì´í”„"
]

# PM ê´€ë ¨ í•„ìˆ˜ í‚¤ì›Œë“œ (ìµœì†Œ 1ê°œ ì´ìƒ í¬í•¨)
PM_REQUIRED_KEYWORDS = [
    "í”¼ì— ", "PM", "í”¼ì— ì¸í„°ë‚´ì…”ë„", "í”¼ì— ì½”ë¦¬ì•„", "pmì¸í„°ë‚´ì…”ë„",
    "í•ë¼ì¸", "í”¼íŠ¸ë¼ì¸", "fitline", "FitLine",
    "íƒ‘ì‰ì´í”„", "TopShape", "topshape",
    "ë…ì¼í”¼ì— ", "pmkorea", "pmì½”ë¦¬ì•„"
]

# í¬ë¡¤ë§ ì„¤ì •
MAX_RESULTS_PER_KEYWORD = 20  # í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
TOTAL_TARGET = 100             # ì „ì²´ ëª©í‘œ ìˆ˜ì§‘ ê°œìˆ˜
REQUEST_DELAY_MIN = 2.0        # ìµœì†Œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
REQUEST_DELAY_MAX = 4.0        # ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
MAX_RETRIES = 3                # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
PAGE_LOAD_TIMEOUT = 15         # í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ (ì´ˆ)
ELEMENT_WAIT_TIMEOUT = 10      # ìš”ì†Œ ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ (ì´ˆ)

# ===========================
# Selenium ë“œë¼ì´ë²„ ì„¤ì •
# ===========================

def create_driver() -> webdriver.Chrome:
    """Selenium Chrome ë“œë¼ì´ë²„ ìƒì„±"""
    try:
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™” (ì†ë„ í–¥ìƒ)
        prefs = {
            'profile.managed_default_content_settings.images': 2,
            'profile.default_content_setting_values.notifications': 2
        }
        options.add_experimental_option('prefs', prefs)
        options.add_experimental_option('excludeSwitches', ['enable-logging', 'enable-automation'])
        options.add_experimental_option('useAutomationExtension', False)
        
        # Homebrew chromedriver ìš°ì„  ì‚¬ìš© (macOS Gatekeeper ë¬¸ì œ í•´ê²°)
        homebrew_chromedriver = '/opt/homebrew/bin/chromedriver'
        
        if os.path.exists(homebrew_chromedriver):
            service = Service(homebrew_chromedriver)
            logger.info("âœ… Homebrew ChromeDriver ì‚¬ìš©")
        else:
            service = Service(ChromeDriverManager().install())
            logger.info("âœ… webdriver-manager ChromeDriver ì‚¬ìš©")
        
        driver = webdriver.Chrome(service=service, options=options)
        driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)
        
        logger.info("âœ… Selenium ë“œë¼ì´ë²„ ìƒì„± ì™„ë£Œ")
        return driver
        
    except Exception as e:
        logger.error(f"âŒ ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise

# ===========================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ===========================

def normalize_blog_url(blog_id: str, post_id: str) -> str:
    """ë¸”ë¡œê·¸ URL ì •ê·œí™”"""
    return f"https://blog.naver.com/{blog_id}/{post_id}"

def extract_blog_info(url: str) -> Optional[Dict[str, str]]:
    """
    URLì—ì„œ blog_idì™€ post_id ì¶”ì¶œ
    
    Args:
        url: ë„¤ì´ë²„ ë¸”ë¡œê·¸ URL
        
    Returns:
        {'blog_id': str, 'post_id': str} ë˜ëŠ” None
    """
    try:
        # ì •ê·œ URL: https://blog.naver.com/blog_id/post_id
        pattern1 = r'blog\.naver\.com/([^/]+)/(\d+)'
        match = re.search(pattern1, url)
        
        if match:
            return {
                'blog_id': match.group(1),
                'post_id': match.group(2)
            }
        
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ë°©ì‹
        parsed = urlparse(url)
        if 'blog.naver.com' in parsed.netloc:
            query = parse_qs(parsed.query)
            if 'blogId' in query and 'logNo' in query:
                return {
                    'blog_id': query['blogId'][0],
                    'post_id': query['logNo'][0]
                }
        
        return None
        
    except Exception as e:
        logger.debug(f"URL íŒŒì‹± ì‹¤íŒ¨: {url}, {str(e)}")
        return None

def parse_date(date_str: str) -> Optional[str]:
    """
    ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹± ë° í‘œì¤€í™”
    
    Args:
        date_str: ë‚ ì§œ ë¬¸ìì—´
        
    Returns:
        'YYYY-MM-DD HH:MM:SS' í˜•ì‹ ë˜ëŠ” None
    """
    try:
        date_str = date_str.strip()
        
        # íŒ¨í„´ 1: "2024. 11. 5. 14:30"
        pattern1 = r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.\s*(\d{1,2}):(\d{2})'
        match = re.search(pattern1, date_str)
        if match:
            year, month, day, hour, minute = match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)} {hour.zfill(2)}:{minute}:00"
        
        # íŒ¨í„´ 2: "2024.11.05"
        pattern2 = r'(\d{4})\.(\d{1,2})\.(\d{1,2})'
        match = re.search(pattern2, date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)} 00:00:00"
        
        # íŒ¨í„´ 3: "11. 5." (ì˜¬í•´)
        pattern3 = r'(\d{1,2})\.\s*(\d{1,2})\.'
        match = re.search(pattern3, date_str)
        if match:
            month, day = match.groups()
            year = datetime.now().year
            return f"{year}-{month.zfill(2)}-{day.zfill(2)} 00:00:00"
        
        return None
        
    except Exception as e:
        logger.debug(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str}, {str(e)}")
        return None

def extract_sponsor_info(text: str) -> Dict[str, Optional[str]]:
    """
    ì¶”ì²œì¸ ì •ë³´ ì¶”ì¶œ (sponsor_name, sponsor_phone, sponsor_partner_id)
    
    Args:
        text: ê²Œì‹œë¬¼ ë³¸ë¬¸
        
    Returns:
        {'sponsor_name': str, 'sponsor_phone': str, 'sponsor_partner_id': str}
    """
    result = {
        'sponsor_name': None,
        'sponsor_phone': None,
        'sponsor_partner_id': None
    }
    
    if not text:
        return result
    
    # sponsor_name ì¶”ì¶œ íŒ¨í„´ (ê°•í™”)
    name_patterns = [
        r'(?:ì¶”ì²œì¸|í›„ì›ì¸|ìŠ¤í°ì„œ|ì¶”ì²œ|referrer|sponsor)[\s:ï¼š]*([ê°€-í£]{2,4})',
        r'(?:ë¬¸ì˜|ì—°ë½|ìƒë‹´)[\s:ï¼š]*([ê°€-í£]{2,4})',
        r'([ê°€-í£]{2,4})[\s]*(?:íŒ€íŒŒíŠ¸ë„ˆ|íŒŒíŠ¸ë„ˆ|ë‹˜)',
    ]
    
    for pattern in name_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            result['sponsor_name'] = match.group(1).strip()
            break
    
    # sponsor_phone ì¶”ì¶œ íŒ¨í„´ (ëŒ€í­ ê°•í™”)
    phone_patterns = [
        r'(?:010|011|016|017|018|019)[-\s]*\d{3,4}[-\s]*\d{4}',  # 010-1234-5678
        r'(?:010|011|016|017|018|019)\d{7,8}',                    # 01012345678
        r'\d{2,3}[-\s]*\d{3,4}[-\s]*\d{4}',                       # 02-123-4567
    ]
    
    for pattern in phone_patterns:
        match = re.search(pattern, text)
        if match:
            phone = match.group(0)
            # ì „í™”ë²ˆí˜¸ ì •ê·œí™” (í•˜ì´í”ˆ ì¶”ê°€)
            phone = re.sub(r'[^\d]', '', phone)
            if len(phone) == 11:
                result['sponsor_phone'] = f"{phone[:3]}-{phone[3:7]}-{phone[7:]}"
            elif len(phone) == 10:
                if phone.startswith('02'):
                    result['sponsor_phone'] = f"{phone[:2]}-{phone[2:6]}-{phone[6:]}"
                else:
                    result['sponsor_phone'] = f"{phone[:3]}-{phone[3:6]}-{phone[6:]}"
            else:
                result['sponsor_phone'] = phone
            break
    
    # sponsor_partner_id ì¶”ì¶œ íŒ¨í„´ (ê°•í™”)
    partner_id_patterns = [
        r'(?:íŒŒíŠ¸ë„ˆ\s*ID|ì¶”ì²œì¸\s*ID|ID)[\s:ï¼š]*([A-Za-z0-9]+)',
        r'(?:ì¶”ì²œì¸\s*ë²ˆí˜¸|íŒŒíŠ¸ë„ˆ\s*ë²ˆí˜¸)[\s:ï¼š]*(\d+)',
        r'ID[\s:ï¼š]*([A-Za-z0-9]{4,20})',
    ]
    
    for pattern in partner_id_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            result['sponsor_partner_id'] = match.group(1).strip()
            break
    
    return result

def extract_hashtags(text: str, soup: BeautifulSoup = None) -> str:
    """
    í•´ì‹œíƒœê·¸ ì¶”ì¶œ (ì´ì „ ë²„ì „ì˜ ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ë¡œì§ ë³µì›)
    
    Args:
        text: ê²Œì‹œë¬¼ ë³¸ë¬¸
        soup: BeautifulSoup ê°ì²´ (ì„ íƒ)
        
    Returns:
        ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í•´ì‹œíƒœê·¸ ë¬¸ìì—´ (# ê¸°í˜¸ í¬í•¨)
    """
    hashtags = set()
    
    if not text:
        return ""
    
    # ë°©ë²• 1: ë³¸ë¬¸ì—ì„œ #í•´ì‹œíƒœê·¸ íŒ¨í„´ ì¶”ì¶œ (ê°€ì¥ íš¨ê³¼ì )
    # í•œê¸€, ì˜ì–´, ìˆ«ìë¥¼ ëª¨ë‘ ì§€ì›í•˜ë©° ê³µë°±ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ì ì „ê¹Œì§€ ì¶”ì¶œ
    pattern = r'#([ê°€-í£a-zA-Z0-9_]+)'
    matches = re.findall(pattern, text)
    
    for match in matches:
        # # ê¸°í˜¸ë¥¼ ìœ ì§€í•˜ì—¬ ì €ì¥
        hashtags.add(f"#{match}")
    
    # ë°©ë²• 2: soupê°€ ìˆìœ¼ë©´ íƒœê·¸ ì˜ì—­ì—ì„œ ì¶”ì¶œ
    if soup:
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ íƒœê·¸ ì˜ì—­ ì„ íƒì
        tag_selectors = [
            'div.post_tag a.link___',
            'div.post-tag a',
            'span.se-fs- a',
            'div.tag_list a',
            'a[href*="/search/"]'
        ]
        
        for selector in tag_selectors:
            tag_elements = soup.select(selector)
            for tag in tag_elements:
                tag_text = tag.get_text().strip()
                if tag_text:
                    # íƒœê·¸ ì˜ì—­ì—ì„œëŠ” # ì—†ì´ ë‚˜ì˜¤ë¯€ë¡œ ì¶”ê°€
                    if not tag_text.startswith('#'):
                        tag_text = f"#{tag_text}"
                    hashtags.add(tag_text)
    
    # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ì‰¼í‘œë¡œ ê²°í•©
    return ','.join(sorted(hashtags))

def is_pm_related(title: str, content: str) -> bool:
    """
    PM International ê´€ë ¨ ê²Œì‹œë¬¼ ì—¬ë¶€ í™•ì¸
    
    Args:
        title: ì œëª©
        content: ë³¸ë¬¸
        
    Returns:
        True if PM ê´€ë ¨, False otherwise
    """
    combined_text = f"{title} {content}".lower()
    
    for keyword in PM_REQUIRED_KEYWORDS:
        if keyword.lower() in combined_text:
            return True
    
    return False

def generate_post_fingerprint(post_data: Dict) -> str:
    """
    ê²Œì‹œë¬¼ ì§€ë¬¸ ìƒì„± (ì¤‘ë³µ ê²€ì‚¬ìš©)
    
    Args:
        post_data: ê²Œì‹œë¬¼ ë°ì´í„°
        
    Returns:
        ì§€ë¬¸ ë¬¸ìì—´
    """
    title = post_data.get('title', '')
    blog_id = post_data.get('blog_id', '')
    post_id = post_data.get('post_id', '')
    
    return f"{blog_id}_{post_id}_{title[:50]}"

# ===========================
# Naver API í•¨ìˆ˜
# ===========================

def search_naver_blogs(keyword: str, display: int = 100, start: int = 1) -> List[Dict]:
    """
    Naver Open APIë¡œ ë¸”ë¡œê·¸ ê²€ìƒ‰
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        display: ê²°ê³¼ ê°œìˆ˜ (ìµœëŒ€ 100)
        start: ì‹œì‘ ìœ„ì¹˜
        
    Returns:
        ë¸”ë¡œê·¸ URL ë¦¬ìŠ¤íŠ¸
    """
    try:
        url = "https://openapi.naver.com/v1/search/blog.json"
        headers = {
            "X-Naver-Client-Id": NAVER_CLIENT_ID,
            "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
        }
        params = {
            "query": keyword,
            "display": min(display, 100),
            "start": start,
            "sort": "date"  # ìµœì‹ ìˆœ ì •ë ¬
        }
        
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        items = data.get('items', [])
        
        logger.info(f"âœ… '{keyword}' ê²€ìƒ‰ ì™„ë£Œ: {len(items)}ê°œ ë°œê²¬")
        return items
        
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ API ìš”ì²­ ì‹¤íŒ¨ ({keyword}): {str(e)}")
        return []
    except Exception as e:
        logger.error(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ({keyword}): {str(e)}")
        return []

# ===========================
# í¬ë¡¤ë§ í•¨ìˆ˜
# ===========================

def crawl_blog_post_selenium(
    driver: webdriver.Chrome,
    url: str,
    blog_id: str,
    post_id: str,
    failed_url_manager: 'FailedURLManager'
) -> Optional[Dict]:
    """
    Seleniumìœ¼ë¡œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ í¬ë¡¤ë§
    
    Args:
        driver: Selenium ë“œë¼ì´ë²„
        url: ë¸”ë¡œê·¸ URL
        blog_id: ë¸”ë¡œê·¸ ID
        post_id: í¬ìŠ¤íŠ¸ ID
        failed_url_manager: ì‹¤íŒ¨ URL ê´€ë¦¬ì
        
    Returns:
        ê²Œì‹œë¬¼ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
    """
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            driver.get(url)
            time.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # iframeìœ¼ë¡œ ì „í™˜
            try:
                iframe = WebDriverWait(driver, ELEMENT_WAIT_TIMEOUT).until(
                    EC.presence_of_element_located((By.ID, "mainFrame"))
                )
                driver.switch_to.frame(iframe)
            except TimeoutException:
                logger.debug(f"iframe ì—†ìŒ (ì‹œë„ {attempt}/{MAX_RETRIES})")
            
            # BeautifulSoupë¡œ íŒŒì‹±
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # 1. ì œëª© ì¶”ì¶œ
            title_selectors = [
                'div.se-title-text',
                'div.pcol1 h3.se_textarea',
                'h3.se_textarea',
                'span.se-fs-',
                'div.se-module-text h3'
            ]
            
            title = None
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text().strip()
                    if title:
                        break
            
            if not title:
                logger.debug(f"ì œëª© ì—†ìŒ: {url}")
                driver.switch_to.default_content()
                if attempt == MAX_RETRIES:
                    failed_url_manager.add_failed_url(url, blog_id, post_id, "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨")
                continue
            
            # 2. ë³¸ë¬¸ ì¶”ì¶œ
            content_selectors = [
                'div.se-main-container',
                'div.se-component',
                'div#postViewArea',
                'div.post-view'
            ]
            
            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                    if content:
                        break
            
            # 3. PM ê´€ë ¨ ì—¬ë¶€ í™•ì¸
            if not is_pm_related(title, content):
                logger.debug(f"PM ê´€ë ¨ ì—†ìŒ: {title[:30]}")
                driver.switch_to.default_content()
                return None
            
            # 4. ë°œí–‰ì¼ ì¶”ì¶œ
            published_date = None
            date_selectors = [
                'span.se_publishDate',
                'span.date',
                'span.se-ff-nanummyeongjo span',
                'p.date',
                'div.post_date',
                'span.blog2_series'
            ]
            
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem:
                    date_text = date_elem.get_text().strip()
                    published_date = parse_date(date_text)
                    if published_date:
                        break
            
            # 5. ì¶”ì²œì¸ ì •ë³´ ì¶”ì¶œ
            sponsor_info = extract_sponsor_info(content)
            
            # 6. ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ
            like_count = 0
            like_selectors = [
                'em.u_cnt._count',
                'span.u_cnt',
                'em.cnt_like',
                'span.like_count'
            ]
            
            for selector in like_selectors:
                like_elem = soup.select_one(selector)
                if like_elem:
                    like_text = like_elem.get_text().strip()
                    try:
                        like_count = int(re.sub(r'[^\d]', '', like_text))
                        if like_count > 0:
                            break
                    except (ValueError, AttributeError):
                        pass
            
            # 7. ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ
            comment_count = 0
            comment_selectors = [
                'span.u_cnt._count',
                'span.num_cmt',
                'em.cnt_cmt',
                'span.comment_count'
            ]
            
            for selector in comment_selectors:
                comment_elem = soup.select_one(selector)
                if comment_elem:
                    comment_text = comment_elem.get_text().strip()
                    try:
                        comment_count = int(re.sub(r'[^\d]', '', comment_text))
                        if comment_count > 0:
                            break
                    except (ValueError, AttributeError):
                        pass
            
            # 8. í•´ì‹œíƒœê·¸ ì¶”ì¶œ
            hashtags = extract_hashtags(content, soup)
            
            # 9. ì´ë¯¸ì§€ URL ì¶”ì¶œ
            image_urls = []
            img_elements = soup.select('img.se-image-resource')
            for img in img_elements:
                img_url = img.get('data-lazy-src') or img.get('src')
                if img_url and img_url.startswith('http'):
                    image_urls.append(img_url)
            
            # 10. ë™ì˜ìƒ URL ì¶”ì¶œ
            video_urls = []
            video_elements = soup.select('div.se-video')
            for video in video_elements:
                video_url = video.get('data-src') or video.get('src')
                if video_url and video_url.startswith('http'):
                    video_urls.append(video_url)
            
            # ë°ì´í„° êµ¬ì„±
            post_data = {
                'platform': 'naver_blog',
                'post_id': f"{blog_id}_{post_id}",
                'blog_id': blog_id,
                'url': url,
                'title': title,
                'content': content[:5000],  # 5000ì ì œí•œ
                'published_date': published_date,
                'sponsor_name': sponsor_info['sponsor_name'],
                'sponsor_phone': sponsor_info['sponsor_phone'],
                'sponsor_partner_id': sponsor_info['sponsor_partner_id'],
                'like_count': like_count,
                'comment_count': comment_count,
                'hashtags': hashtags,
                'image_urls': json.dumps(image_urls, ensure_ascii=False),
                'video_urls': json.dumps(video_urls, ensure_ascii=False),
                'collected_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            logger.debug(f"âœ… í¬ë¡¤ë§ ì„±ê³µ: {title[:30]}")
            driver.switch_to.default_content()
            return post_data
            
        except TimeoutException as e:
            logger.warning(f"â±ï¸  íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt}/{MAX_RETRIES}): {url}")
            driver.switch_to.default_content()
            if attempt == MAX_RETRIES:
                failed_url_manager.add_failed_url(url, blog_id, post_id, f"TimeoutException: {str(e)}")
        
        except WebDriverException as e:
            logger.warning(f"ğŸŒ WebDriver ì˜¤ë¥˜ (ì‹œë„ {attempt}/{MAX_RETRIES}): {str(e)}")
            driver.switch_to.default_content()
            if attempt == MAX_RETRIES:
                failed_url_manager.add_failed_url(url, blog_id, post_id, f"WebDriverException: {str(e)}")
        
        except Exception as e:
            logger.error(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ (ì‹œë„ {attempt}/{MAX_RETRIES}): {str(e)}")
            driver.switch_to.default_content()
            if attempt == MAX_RETRIES:
                failed_url_manager.add_failed_url(url, blog_id, post_id, f"Exception: {str(e)}")
        
        # ì¬ì‹œë„ ì „ ëŒ€ê¸° (ì§€ìˆ˜ ë°±ì˜¤í”„)
        if attempt < MAX_RETRIES:
            wait_time = 2 ** attempt
            time.sleep(wait_time)
    
    return None

# ===========================
# ì‹¤íŒ¨ URL ê´€ë¦¬
# ===========================

class FailedURLManager:
    """ì‹¤íŒ¨í•œ URL ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, filename: str = "failed_urls.json"):
        self.filename = filename
        self.failed_urls = []
    
    def add_failed_url(self, url: str, blog_id: str, post_id: str, reason: str):
        """ì‹¤íŒ¨ URL ì¶”ê°€"""
        self.failed_urls.append({
            'url': url,
            'blog_id': blog_id,
            'post_id': post_id,
            'reason': reason,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
    
    def save_to_file(self):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        if self.failed_urls:
            with open(self.filename, 'w', encoding='utf-8') as f:
                json.dump(self.failed_urls, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ ì‹¤íŒ¨ URL ì €ì¥: {self.filename} ({len(self.failed_urls)}ê°œ)")
    
    def get_failed_count(self) -> int:
        """ì‹¤íŒ¨ URL ê°œìˆ˜ ë°˜í™˜"""
        return len(self.failed_urls)

# ===========================
# í†µê³„ í´ë˜ìŠ¤
# ===========================

class CrawlingStats:
    """í¬ë¡¤ë§ í†µê³„ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.total_attempted = 0
        self.success_count = 0
        self.failed_count = 0
        self.filtered_count = 0
        self.duplicate_count = 0
        self.start_time = time.time()
    
    def add_success(self):
        self.total_attempted += 1
        self.success_count += 1
    
    def add_failed(self):
        self.total_attempted += 1
        self.failed_count += 1
    
    def add_filtered(self):
        self.total_attempted += 1
        self.filtered_count += 1
    
    def add_duplicate(self):
        self.total_attempted += 1
        self.duplicate_count += 1
    
    def print_stats(self):
        """í†µê³„ ì¶œë ¥"""
        elapsed = time.time() - self.start_time
        minutes = int(elapsed // 60)
        seconds = int(elapsed % 60)
        
        success_rate = (self.success_count / self.total_attempted * 100) if self.total_attempted > 0 else 0
        
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ“Š í¬ë¡¤ë§ í†µê³„")
        logger.info(f"{'='*70}")
        logger.info(f"ì´ ì‹œë„: {self.total_attempted}ê°œ")
        logger.info(f"âœ… ì„±ê³µ: {self.success_count}ê°œ ({success_rate:.1f}%)")
        logger.info(f"âŒ ì‹¤íŒ¨: {self.failed_count}ê°œ")
        logger.info(f"ğŸš« í•„í„°ë§: {self.filtered_count}ê°œ (PM ê´€ë ¨ ì—†ìŒ)")
        logger.info(f"ğŸ” ì¤‘ë³µ: {self.duplicate_count}ê°œ")
        logger.info(f"â±ï¸  ì†Œìš” ì‹œê°„: {minutes}ë¶„ {seconds}ì´ˆ")
        logger.info(f"{'='*70}\n")

# ===========================
# ë©”ì¸ í•¨ìˆ˜
# ===========================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info(f"\n{'='*70}")
    logger.info(f"ğŸš€ PM-International Korea ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v7.0")
    logger.info(f"{'='*70}\n")
    
    driver = create_driver()
    failed_url_manager = FailedURLManager()
    stats = CrawlingStats()
    
    collected_posts = []
    collected_urls = set()
    collected_fingerprints = set()
    
    try:
        for keyword in SEARCH_KEYWORDS:
            if len(collected_posts) >= TOTAL_TARGET:
                break
            
            logger.info(f"\nğŸ” í‚¤ì›Œë“œ: '{keyword}' ê²€ìƒ‰ ì¤‘...")
            
            # APIë¡œ URL ëª©ë¡ ìˆ˜ì§‘
            blog_items = search_naver_blogs(keyword, display=MAX_RESULTS_PER_KEYWORD)
            
            if not blog_items:
                continue
            
            for item in blog_items:
                if len(collected_posts) >= TOTAL_TARGET:
                    break
                
                blog_url = item.get('link', '')
                
                # URLì—ì„œ blog_id, post_id ì¶”ì¶œ
                blog_info = extract_blog_info(blog_url)
                if not blog_info:
                    continue
                
                blog_id = blog_info['blog_id']
                post_id = blog_info['post_id']
                normalized_url = normalize_blog_url(blog_id, post_id)
                
                # ì¤‘ë³µ ì²´í¬
                if normalized_url in collected_urls:
                    stats.add_duplicate()
                    continue
                
                logger.info(f"[{len(collected_posts)+1}/{TOTAL_TARGET}] í¬ë¡¤ë§ ì¤‘...")
                
                # í¬ë¡¤ë§ ì‹¤í–‰
                post_data = crawl_blog_post_selenium(
                    driver, normalized_url, blog_id, post_id, failed_url_manager
                )
                
                if post_data:
                    # ì¤‘ë³µ ì²´í¬ (ì§€ë¬¸ ê¸°ë°˜)
                    fingerprint = generate_post_fingerprint(post_data)
                    if fingerprint not in collected_fingerprints:
                        collected_posts.append(post_data)
                        collected_urls.add(normalized_url)
                        collected_fingerprints.add(fingerprint)
                        stats.add_success()
                        logger.info(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {post_data['title'][:50]}")
                    else:
                        stats.add_duplicate()
                else:
                    stats.add_filtered()
                
                # ìš”ì²­ ê°„ ì§€ì—°
                time.sleep(random.uniform(REQUEST_DELAY_MIN, REQUEST_DELAY_MAX))
            
            # í‚¤ì›Œë“œ ê°„ ëŒ€ê¸°
            if len(collected_posts) < TOTAL_TARGET:
                time.sleep(random.uniform(2, 4))
        
        # í†µê³„ ì¶œë ¥
        stats.print_stats()
        
        # CSVë¡œ ì €ì¥
        if collected_posts:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'naver_blog_pm_{timestamp}.csv'
            
            df = pd.DataFrame(collected_posts)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            logger.info(f"\n{'='*70}")
            logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
            logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(collected_posts)}ê°œ")
            logger.info(f"{'='*70}")
        else:
            logger.warning("âš ï¸  ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì‹¤íŒ¨ URL ì €ì¥
        failed_url_manager.save_to_file()
        if failed_url_manager.get_failed_count() > 0:
            logger.info(f"âŒ ì‹¤íŒ¨ URL: {failed_url_manager.get_failed_count()}ê°œ (failed_urls.json)")
    
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  ì‚¬ìš©ì ì¤‘ë‹¨")
        stats.print_stats()
    
    except Exception as e:
        logger.error(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {str(e)}")
        stats.print_stats()
    
    finally:
        driver.quit()
        logger.info("âœ… ë“œë¼ì´ë²„ ì¢…ë£Œ")

if __name__ == "__main__":
    main()
