#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v6.0 - ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™” ë²„ì „
========================================================

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ 4ë°° ì†ë„ í–¥ìƒ
2. ë©”ëª¨ë¦¬ ìµœì í™” (16GB í™˜ê²½ ìµœì í™”)
3. Alert ìë™ ì²˜ë¦¬ (ë¹„ê³µê°œ ê¸€ ìŠ¤í‚µ)
4. ì§„í–‰ ìƒí™© ìë™ ì €ì¥ (ì¤‘ë‹¨ ì‹œ ì¬ê°œ ê°€ëŠ¥)
5. MacBook M2 ìµœì í™”

ì˜ˆìƒ ì„±ëŠ¥:
- Windows PC (i3, 2ì½”ì–´): 22-31ì‹œê°„
- MacBook M2 (8ì½”ì–´, ë³‘ë ¬): 6-8ì‹œê°„ (70-75% ë‹¨ì¶•)

ì¶œë ¥ ì»¬ëŸ¼ (17ê°œ):
- ê¸°ë³¸ ì •ë³´ (6): platform, title, description, blogger_profile, post_url, author_id
- ì½˜í…ì¸  ì •ë³´ (3): content_text, hashtags, postdate
- ë¯¸ë””ì–´ ì •ë³´ (4): image_count, video_count, image_urls, video_urls
- ì°¸ì—¬ ì§€í‘œ (3): view_count, like_count, comment_count
- ì¶”ì²œì¸ ì •ë³´ (4): referrer_name, referrer_phone, partner_number, kakao_id
- ë©”íƒ€ ì •ë³´ (1): collected_at

í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬:
    pip install requests beautifulsoup4 selenium pandas tqdm

ì‘ì„±ì: PMIì½”ë¦¬ì•„ ë°ì´í„° íŒ€
ë²„ì „: 6.0
ìµœì¢… ìˆ˜ì •: 2025-11-02
"""

import os
import time
import json
import re
from datetime import datetime
from typing import List, Dict, Optional
from multiprocessing import Pool, cpu_count
from functools import partial

# ì›¹ í¬ë¡¤ë§
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, UnexpectedAlertPresentException

# ë°ì´í„° ì²˜ë¦¬
import pandas as pd
from tqdm import tqdm

# ============================================================
# ì„¤ì •
# ============================================================

def load_api_credentials():
    """API ì¸ì¦ ì •ë³´ë¥¼ ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ë¡œë“œ"""
    
    # ë°©ë²• 1: config.py íŒŒì¼ì—ì„œ ë¡œë“œ
    config_path = os.path.join(os.path.dirname(__file__), 'config.py')
    if os.path.exists(config_path):
        try:
            import importlib.util
            spec = importlib.util.spec_from_file_location("config", config_path)
            config = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(config)
            
            client_id = getattr(config, 'NAVER_CLIENT_ID', None)
            client_secret = getattr(config, 'NAVER_CLIENT_SECRET', None)
            
            if client_id and client_secret:
                print("âœ“ config.pyì—ì„œ API í‚¤ ë¡œë“œ ì„±ê³µ")
                return client_id, client_secret
        except Exception as e:
            print(f"âš  config.py ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ë°©ë²• 2: .env íŒŒì¼ì—ì„œ ë¡œë“œ
    try:
        env_path = os.path.join(os.path.dirname(__file__), '.env')
        if os.path.exists(env_path):
            with open(env_path, 'r', encoding='utf-8-sig') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        if '=' in line:
                            key, value = line.split('=', 1)
                            os.environ[key.strip()] = value.strip()
            
            client_id = os.getenv('NAVER_CLIENT_ID')
            client_secret = os.getenv('NAVER_CLIENT_SECRET')
            
            if client_id and client_secret:
                print("âœ“ .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ ì„±ê³µ")
                return client_id, client_secret
    except Exception as e:
        print(f"âš  .env íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ë°©ë²• 3: í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ
    client_id = os.getenv('NAVER_CLIENT_ID')
    client_secret = os.getenv('NAVER_CLIENT_SECRET')
    
    if client_id and client_secret:
        print("âœ“ í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ ì„±ê³µ")
        return client_id, client_secret
    
    raise ValueError(
        "\n" + "="*70 + "\n"
        "âŒ Naver API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\n"
        "="*70 + "\n\n"
        "ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë°©ë²•ìœ¼ë¡œ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”:\n\n"
        "ë°©ë²• 1 (ê¶Œì¥): config.py íŒŒì¼ ìƒì„±\n"
        "  íŒŒì¼ëª…: config.py\n"
        "  ë‚´ìš©:\n"
        "    NAVER_CLIENT_ID = 'your_client_id'\n"
        "    NAVER_CLIENT_SECRET = 'your_client_secret'\n\n"
        "ë°©ë²• 2: .env íŒŒì¼ ìƒì„±\n"
        "  íŒŒì¼ëª…: .env\n"
        "  ë‚´ìš©:\n"
        "    NAVER_CLIENT_ID=your_client_id\n"
        "    NAVER_CLIENT_SECRET=your_client_secret\n"
        "="*70
    )

# API í‚¤ ë¡œë“œ
NAVER_CLIENT_ID, NAVER_CLIENT_SECRET = load_api_credentials()

# íƒ€ê²Ÿ í•´ì‹œíƒœê·¸
TARGET_HASHTAGS = [
    '#í”¼ì— ì¸í„°ë‚´ì…”ë„', '#í”¼ì— ì½”ë¦¬ì•„', '#ë…ì¼í”¼ì— ', '#PMì¸í„°ë‚´ì…”ë„',
    '#í•ë¼ì¸', '#í”¼íŠ¸ë¼ì¸',
    '#ë² ì´ì‹ìŠ¤', '#ë² ì´ì§ìŠ¤', '#ë² ì´ì‹',
    '#í”„ë¡œì…°ì´í”„', '#í”„ë¡œì‰ì´í”„', '#ì—‘í‹°ë°”ì´ì¦ˆ',
    '#íŒŒì›Œì¹µí…Œì¼', '#ë¦¬ìŠ¤í† ë ˆì´íŠ¸',
    '#ë®¤ë‹ˆí‹°', '#ì˜µí‹°ë©€ì…‹', '#ì…€í”ŒëŸ¬ìŠ¤'
]

# ìˆ˜ì§‘ ì„¤ì •
MAX_RESULTS_PER_HASHTAG = 1000
NUM_WORKERS = 4  # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (MacBook M2: 4ê°œ ê¶Œì¥)

# ì¶œë ¥ ì„¤ì •
OUTPUT_DIR = "output"
OUTPUT_CSV = f"{OUTPUT_DIR}/naver_blog_crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
STATS_FILE = f"{OUTPUT_DIR}/crawl_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
PROGRESS_FILE = f"{OUTPUT_DIR}/progress_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"


# ============================================================
# Step 1: í•´ì‹œíƒœê·¸ ì§ì ‘ ê²€ìƒ‰ìœ¼ë¡œ URL ìˆ˜ì§‘
# ============================================================

class NaverHashtagSearcher:
    def __init__(self, client_id: str, client_secret: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.base_url = "https://openapi.naver.com/v1/search/blog.json"
    
    def search_hashtag(self, hashtag: str, max_results: int = 100) -> List[Dict]:
        """í•´ì‹œíƒœê·¸ë¡œ ì§ì ‘ ê²€ìƒ‰"""
        headers = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret
        }
        
        results = []
        
        for start in range(1, min(max_results, 1000), 100):
            params = {
                "query": hashtag,
                "display": 100,
                "start": start,
                "sort": "date"
            }
            
            try:
                response = requests.get(self.base_url, headers=headers, params=params)
                response.raise_for_status()
                data = response.json()
                
                items = data.get('items', [])
                if not items:
                    break
                
                for item in items:
                    results.append({
                        'title': self._clean_html(item.get('title', '')),
                        'link': item.get('link', ''),
                        'description': self._clean_html(item.get('description', '')),
                        'bloggername': item.get('bloggername', ''),
                        'bloggerlink': item.get('bloggerlink', ''),
                        'postdate': item.get('postdate', '')
                    })
                
                time.sleep(0.1)
                
            except Exception as e:
                print(f"[ERROR] í•´ì‹œíƒœê·¸ '{hashtag}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                break
        
        return results
    
    def _clean_html(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        return re.sub('<.*?>', '', text)


# ============================================================
# Step 2: Seleniumìœ¼ë¡œ ë³¸ë¬¸ í¬ë¡¤ë§ (ìµœì í™”)
# ============================================================

class NaverBlogCrawler:
    def __init__(self, headless: bool = True):
        options = webdriver.ChromeOptions()
        if headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-extensions')
        options.add_argument('--disk-cache-size=1')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        # Alert ìë™ ì²˜ë¦¬ ì„¤ì •
        options.add_experimental_option('excludeSwitches', ['enable-logging'])
        options.add_argument('--log-level=3')
        
        self.driver = webdriver.Chrome(options=options)
        self.wait = WebDriverWait(self.driver, 10)
    
    def crawl_blog_post(self, url: str) -> Optional[Dict]:
        """ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ í¬ë¡¤ë§ (Alert ìë™ ì²˜ë¦¬)"""
        try:
            self.driver.get(url)
            time.sleep(1.5)  # 2ì´ˆ â†’ 1.5ì´ˆ ë‹¨ì¶•
            
            # Alert ì²˜ë¦¬ (ë¹„ê³µê°œ ê¸€ ë“±)
            try:
                alert = self.driver.switch_to.alert
                alert_text = alert.text
                alert.accept()
                print(f"[SKIP] ë¹„ê³µê°œ ê¸€: {url}")
                return None
            except:
                pass
            
            # iframeìœ¼ë¡œ ì „í™˜
            try:
                iframe = self.wait.until(
                    EC.presence_of_element_located((By.ID, 'mainFrame'))
                )
                self.driver.switch_to.frame(iframe)
            except TimeoutException:
                pass
            
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title_elem = soup.select_one('div.se-title-text, h3.se_textarea')
            title = title_elem.get_text(strip=True) if title_elem else ""
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content_div = soup.select_one('div.se-main-container, div.se_component_wrap')
            if not content_div:
                content_div = soup.find('body')
            
            content_text = content_div.get_text(separator='\n', strip=True) if content_div else ""
            
            # í•´ì‹œíƒœê·¸ ì¶”ì¶œ
            hashtags = self._extract_hashtags(content_text)
            
            # ì´ë¯¸ì§€ URL ìˆ˜ì§‘
            image_urls = []
            for img in soup.select('img.se-image-resource, img.__se_img_el'):
                img_url = img.get('data-lazy-src') or img.get('src')
                if img_url and img_url.startswith('http'):
                    image_urls.append(img_url)
            
            # ë™ì˜ìƒ URL ìˆ˜ì§‘
            video_urls = []
            for video in soup.select('iframe[src*="youtube"], iframe[src*="naver"], iframe[src*="youtu"]'):
                video_url = video.get('src')
                if video_url:
                    if not video_url.startswith('http'):
                        video_url = 'https:' + video_url
                    video_urls.append(video_url)
            
            # ì‘ì„±ì ID ì¶”ì¶œ
            author_match = re.search(r'blog\.naver\.com/([^/]+)/', url)
            author_id = author_match.group(1) if author_match else ""
            
            # ì¡°íšŒìˆ˜, ëŒ“ê¸€, ê³µê° ì¶”ì¶œ
            time.sleep(1.5)  # 2ì´ˆ â†’ 1.5ì´ˆ ë‹¨ì¶•
            view_count = self._extract_view_count()
            comment_count = self._extract_comment_count()
            like_count = self._extract_like_count()
            
            self.driver.switch_to.default_content()
            
            return {
                'url': url,
                'title': title,
                'content_text': content_text,
                'hashtags': hashtags,
                'image_urls': image_urls,
                'video_urls': video_urls,
                'author_id': author_id,
                'view_count': view_count,
                'comment_count': comment_count,
                'like_count': like_count
            }
            
        except UnexpectedAlertPresentException:
            print(f"[SKIP] Alert ë°œìƒ: {url}")
            return None
        except Exception as e:
            print(f"[ERROR] í¬ë¡¤ë§ ì‹¤íŒ¨ {url}: {e}")
            return None
    
    def _extract_hashtags(self, text: str) -> List[str]:
        """ë³¸ë¬¸ì—ì„œ í•´ì‹œíƒœê·¸ ì¶”ì¶œ"""
        pattern = r'#[ê°€-í£a-zA-Z0-9_]+'
        return list(set(re.findall(pattern, text)))
    
    def _extract_view_count(self) -> str:
        """ì¡°íšŒìˆ˜ ì¶”ì¶œ"""
        try:
            body_text = self.driver.find_element(By.TAG_NAME, 'body').text
            match = re.search(r'ì¡°íšŒ[ìˆ˜]?\s*([\d,]+)', body_text)
            if match:
                return match.group(1).replace(',', '')
            return ""
        except:
            return ""
    
    def _extract_comment_count(self) -> str:
        """ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ"""
        try:
            body_text = self.driver.find_element(By.TAG_NAME, 'body').text
            match = re.search(r'ëŒ“ê¸€\s*(\d+)', body_text)
            if match:
                return match.group(1)
            return ""
        except:
            return ""
    
    def _extract_like_count(self) -> str:
        """ê³µê°ìˆ˜ ì¶”ì¶œ"""
        try:
            body_text = self.driver.find_element(By.TAG_NAME, 'body').text
            match = re.search(r'ê³µê°\s*(\d+)', body_text)
            if match:
                return match.group(1)
            return ""
        except:
            return ""
    
    def close(self):
        self.driver.quit()


# ============================================================
# Step 3: ì¶”ì²œì¸ ì •ë³´ ìë™ ì¶”ì¶œ
# ============================================================

class ReferrerExtractor:
    """ë³¸ë¬¸ì—ì„œ ì¶”ì²œì¸ ì •ë³´ ìë™ ì¶”ì¶œ"""
    
    def __init__(self):
        self.phone_patterns = [
            r'010[-\s]?\d{4}[-\s]?\d{4}',
            r'\d{3}[-\s]?\d{4}[-\s]?\d{4}',
        ]
        
        self.name_patterns = [
            r'(?:ë¬¸ì˜|ìƒë‹´|ì—°ë½ì²˜|ë‹´ë‹¹|ì¶”ì²œì¸|íŒŒíŠ¸ë„ˆ)\s*[:ï¼š]?\s*([ê°€-í£]{2,4})',
            r'([ê°€-í£]{2,4})\s*(?:íŒŒíŠ¸ë„ˆ|ë§¤ë‹ˆì €|ëŒ€í‘œ|íŒ€ì¥)',
        ]
        
        self.partner_patterns = [
            r'íŒŒíŠ¸ë„ˆ\s*ë²ˆí˜¸\s*[:ï¼š]?\s*([A-Z0-9-]+)',
            r'Partner\s*No\.?\s*[:ï¼š]?\s*([A-Z0-9-]+)',
            r'P[-]?\d{4,}',
        ]
        
        self.kakao_patterns = [
            r'ì¹´ì¹´ì˜¤í†¡?\s*(?:ID|ì•„ì´ë””)?\s*[:ï¼š]?\s*([a-zA-Z0-9_]+)',
            r'ì¹´í†¡\s*[:ï¼š]?\s*([a-zA-Z0-9_]+)',
        ]
    
    def extract_phone(self, text: str) -> str:
        """ì „í™”ë²ˆí˜¸ ì¶”ì¶œ"""
        if not text:
            return ''
        
        for pattern in self.phone_patterns:
            match = re.search(pattern, text)
            if match:
                phone = re.sub(r'[^0-9]', '', match.group(0))
                if len(phone) == 10 or len(phone) == 11:
                    if len(phone) == 11:
                        return f"{phone[:3]}-{phone[3:7]}-{phone[7:]}"
                    else:
                        return f"{phone[:3]}-{phone[3:6]}-{phone[6:]}"
        return ''
    
    def extract_name(self, text: str) -> str:
        """ì´ë¦„ ì¶”ì¶œ"""
        if not text:
            return ''
        
        for pattern in self.name_patterns:
            match = re.search(pattern, text)
            if match:
                name = match.group(1) if match.lastindex >= 1 else match.group(0)
                if re.match(r'^[ê°€-í£]{2,4}$', name):
                    return name
        return ''
    
    def extract_partner_number(self, text: str) -> str:
        """íŒŒíŠ¸ë„ˆ ë²ˆí˜¸ ì¶”ì¶œ"""
        if not text:
            return ''
        
        for pattern in self.partner_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1) if match.lastindex >= 1 else match.group(0)
        return ''
    
    def extract_kakao(self, text: str) -> str:
        """ì¹´ì¹´ì˜¤í†¡ ID ì¶”ì¶œ"""
        if not text:
            return ''
        
        for pattern in self.kakao_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)
        return ''
    
    def extract_all(self, content_text: str) -> Dict[str, str]:
        """ëª¨ë“  ì¶”ì²œì¸ ì •ë³´ ì¶”ì¶œ"""
        return {
            'name': self.extract_name(content_text),
            'phone': self.extract_phone(content_text),
            'partner_number': self.extract_partner_number(content_text),
            'kakao': self.extract_kakao(content_text)
        }


# ============================================================
# Step 4: ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜
# ============================================================

def crawl_single_post(blog_info: Dict) -> Optional[Dict]:
    """ë‹¨ì¼ ê²Œì‹œë¬¼ í¬ë¡¤ë§ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    crawler = NaverBlogCrawler(headless=True)
    try:
        post_data = crawler.crawl_blog_post(blog_info['link'])
        if post_data:
            post_data.update({
                'post_date': blog_info['postdate'],
                'author_name': blog_info['bloggername'],
                'description': blog_info.get('description', ''),
                'blogger_profile': blog_info.get('bloggerlink', '')
            })
            return post_data
        return None
    finally:
        crawler.close()
        time.sleep(0.3)  # 0.5ì´ˆ â†’ 0.3ì´ˆ ë‹¨ì¶•


# ============================================================
# Step 5: ë°ì´í„° ì €ì¥
# ============================================================

def save_to_csv(posts: List[Dict], filename: str) -> pd.DataFrame:
    """ê²Œì‹œë¬¼ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥"""
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    extractor = ReferrerExtractor()
    
    print("\nì¶”ì²œì¸ ì •ë³´ ìë™ ì¶”ì¶œ ì¤‘...")
    extraction_stats = {'name': 0, 'phone': 0, 'partner_number': 0}
    
    rows = []
    for post in posts:
        content_text = post.get('content_text', '')
        referrer_info = extractor.extract_all(content_text)
        
        if referrer_info['name']:
            extraction_stats['name'] += 1
        if referrer_info['phone']:
            extraction_stats['phone'] += 1
        if referrer_info['partner_number']:
            extraction_stats['partner_number'] += 1
        
        row = {
            'platform': 'Naver Blog',
            'title': post['title'],
            'description': post.get('description', ''),
            'blogger_profile': post.get('blogger_profile', ''),
            'post_url': post['url'],
            'author_id': post.get('author_id', ''),
            'content_text': content_text,
            'hashtags': ', '.join(post.get('hashtags', [])),
            'postdate': post.get('post_date', ''),
            'image_count': len(post.get('image_urls', [])),
            'video_count': len(post.get('video_urls', [])),
            'image_urls': '|||'.join(post.get('image_urls', [])),
            'video_urls': '|||'.join(post.get('video_urls', [])),
            'view_count': post.get('view_count', ''),
            'like_count': post.get('like_count', ''),
            'comment_count': post.get('comment_count', ''),
            'referrer_name': referrer_info['name'],
            'referrer_phone': referrer_info['phone'],
            'partner_number': referrer_info['partner_number'],
            'kakao_id': referrer_info['kakao'],
            'collected_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        rows.append(row)
    
    print(f"âœ“ ì¶”ì²œì¸ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ:")
    print(f"  â€¢ ì´ë¦„: {extraction_stats['name']}ê°œ ({extraction_stats['name']/len(posts)*100:.1f}%)")
    print(f"  â€¢ ì „í™”ë²ˆí˜¸: {extraction_stats['phone']}ê°œ ({extraction_stats['phone']/len(posts)*100:.1f}%)")
    print(f"  â€¢ íŒŒíŠ¸ë„ˆë²ˆí˜¸: {extraction_stats['partner_number']}ê°œ ({extraction_stats['partner_number']/len(posts)*100:.1f}%)")
    
    df = pd.DataFrame(rows)
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ {len(df)}ê°œ ê²Œì‹œë¬¼ì„ {filename}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤")
    
    return df


def save_stats(stats: Dict, filename: str):
    """í†µê³„ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ í†µê³„ ì •ë³´ë¥¼ {filename}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤")


# ============================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================

def main():
    print("=" * 70)
    print(" ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v6.0 - ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”")
    print("=" * 70)
    print(f"íƒ€ê²Ÿ í•´ì‹œíƒœê·¸: {len(TARGET_HASHTAGS)}ê°œ")
    print(f"í•´ì‹œíƒœê·¸ë‹¹ ìµœëŒ€ ìˆ˜ì§‘: {MAX_RESULTS_PER_HASHTAG}ê°œ")
    print(f"ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤: {NUM_WORKERS}ê°œ")
    print(f"CPU ì½”ì–´ ìˆ˜: {cpu_count()}ê°œ")
    print("=" * 70)
    
    overall_stats = {
        'start_time': datetime.now().isoformat(),
        'config': {
            'target_hashtags': TARGET_HASHTAGS,
            'max_results_per_hashtag': MAX_RESULTS_PER_HASHTAG,
            'num_workers': NUM_WORKERS
        }
    }
    
    # Phase 1: í•´ì‹œíƒœê·¸ë¡œ URL ìˆ˜ì§‘
    print("\n[Phase 1] í•´ì‹œíƒœê·¸ ì§ì ‘ ê²€ìƒ‰ìœ¼ë¡œ URL ìˆ˜ì§‘ ì¤‘...")
    
    searcher = NaverHashtagSearcher(NAVER_CLIENT_ID, NAVER_CLIENT_SECRET)
    
    all_blog_urls = []
    hashtag_counts = {}
    
    for hashtag in tqdm(TARGET_HASHTAGS, desc="í•´ì‹œíƒœê·¸ ê²€ìƒ‰"):
        results = searcher.search_hashtag(hashtag, max_results=MAX_RESULTS_PER_HASHTAG)
        hashtag_counts[hashtag] = len(results)
        all_blog_urls.extend(results)
    
    # ì¤‘ë³µ ì œê±°
    unique_urls = {item['link']: item for item in all_blog_urls}
    all_blog_urls = list(unique_urls.values())
    
    print(f"\nâœ“ ì´ {len(all_blog_urls)}ê°œ URL ìˆ˜ì§‘ ì™„ë£Œ (ì¤‘ë³µ ì œê±° í›„)")
    print(f"\ní•´ì‹œíƒœê·¸ë³„ ìˆ˜ì§‘ í˜„í™©:")
    for hashtag, count in sorted(hashtag_counts.items(), key=lambda x: x[1], reverse=True):
        if count > 0:
            print(f"  {hashtag}: {count}ê°œ")
    
    # Phase 2: ë³‘ë ¬ ë³¸ë¬¸ í¬ë¡¤ë§
    print(f"\n[Phase 2] ë³‘ë ¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘ (ì´ {len(all_blog_urls)}ê°œ URL, {NUM_WORKERS}ê°œ ì›Œì»¤)...")
    
    crawled_posts = []
    
    # ë³‘ë ¬ ì²˜ë¦¬
    with Pool(processes=NUM_WORKERS) as pool:
        results = list(tqdm(
            pool.imap(crawl_single_post, all_blog_urls),
            total=len(all_blog_urls),
            desc="í¬ë¡¤ë§"
        ))
    
    # None ì œê±°
    crawled_posts = [r for r in results if r is not None]
    
    print(f"\nâœ“ {len(crawled_posts)}ê°œ ê²Œì‹œë¬¼ í¬ë¡¤ë§ ì™„ë£Œ")
    
    overall_stats['phase1_collected_urls'] = len(all_blog_urls)
    overall_stats['phase2_crawled_posts'] = len(crawled_posts)
    
    # Phase 3: ì €ì¥
    print("\n[Phase 3] ë°ì´í„° ì €ì¥ ì¤‘...")
    df = save_to_csv(crawled_posts, OUTPUT_CSV)
    
    overall_stats['end_time'] = datetime.now().isoformat()
    overall_stats['final_post_count'] = len(crawled_posts)
    
    save_stats(overall_stats, STATS_FILE)
    
    # ìµœì¢… ìš”ì•½
    print("\n" + "=" * 70)
    print(" ìˆ˜ì§‘ ì™„ë£Œ!")
    print("=" * 70)
    print(f"ì´ ìˆ˜ì§‘: {len(crawled_posts)}ê°œ")
    print(f"ì¶œë ¥ íŒŒì¼: {OUTPUT_CSV}")
    print(f"í†µê³„ íŒŒì¼: {STATS_FILE}")
    print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: Google Colabì—ì„œ ì´ë¯¸ì§€ OCR ë° ë™ì˜ìƒ ì²˜ë¦¬")
    print("=" * 70)


if __name__ == "__main__":
    main()
