#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PM-International Korea ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v7.3 (ìµœì¢… ì™„ì„±íŒ)
ëª¨ë“  ê°œì„ ì‚¬í•­ í†µí•© ë²„ì „

ğŸ¯ v7.3 ì‹ ê·œ ê°œì„  ì‚¬í•­:
1. âœ… published_date â†’ published_datetime (ë‚ ì§œ+ì‹œê°„ ì •ë³´)
2. âœ… ë©”ëª¨ë¦¬ ìµœì í™” (ê°€ë¹„ì§€ ì»¬ë ‰ì…˜, ì ì‘í˜• ëŒ€ê¸°)
3. âœ… ì—°ì† ì—ëŸ¬ ì‹œ ë“œë¼ì´ë²„ ìë™ ì¬ì‹œì‘
4. âœ… ì ì‘í˜• ì†ë„ ì¡°ì ˆ (ì„±ê³µ/ì‹¤íŒ¨ì— ë”°ë¼ ë™ì  ì¡°ì •)
5. âœ… ì»¬ëŸ¼ ìˆœì„œ ëª…ì‹œì  ì •ì˜

ğŸ”§ v7.1 ìœ ì§€ ì‚¬í•­:
- sponsor_name ì‚­ì œ (ì˜¤ì‘ë™ ë°©ì§€)
- sponsor_phone: 010-xxxx-xxxx í˜•ì‹ë§Œ ìˆ˜ì§‘
- sponsor_partner_id: 8ìë¦¬ ìˆ«ìë§Œ ìˆ˜ì§‘
- like_count, comment_count: ë³„ë„ í•¨ìˆ˜ë¡œ ì¶”ì¶œ
- hashtags: # ê¸°í˜¸ ìœ ì§€
- ì½˜í…ì¸  í•„í„°ë§: PM í‚¤ì›Œë“œ + ì¶”ì²œì¸/8ìë¦¬ í•„ìˆ˜
- post_id: {blog_id}_{post_id} í˜•ì‹

ğŸ“Š ì¶œë ¥ ì»¬ëŸ¼ (15ê°œ):
- ê¸°ë³¸: platform, post_id, blog_id, url, title, content, published_datetime
- ì¶”ì²œì¸: sponsor_phone, sponsor_partner_id
- ì°¸ì—¬: like_count, comment_count
- ì½˜í…ì¸ : hashtags, image_urls, video_urls
- ë©”íƒ€: collected_date

ì‘ì„±ì: PMI Korea ë°ì´í„° ë¶„ì„íŒ€
ë²„ì „: 7.3
ìµœì¢… ìˆ˜ì •ì¼: 2025-11-06
"""

import os
import re
import json
import time
import random
import logging
import gc
from datetime import datetime
from urllib.parse import urlparse, parse_qs, unquote
from typing import List, Dict, Optional, Set, Tuple

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd
import config

# ===========================
# ë¡œê¹… ì„¤ì •
# ===========================

class ColoredFormatter(logging.Formatter):
    """ì»¬ëŸ¬ ë¡œê¹… í¬ë§·í„°"""
    
    COLORS = {
        'DEBUG': '\033[94m',
        'INFO': '\033[92m',
        'WARNING': '\033[93m',
        'ERROR': '\033[91m',
        'CRITICAL': '\033[95m',
        'RESET': '\033[0m'
    }
    
    def format(self, record):
        log_color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
        record.levelname = f"{log_color}{record.levelname}{self.COLORS['RESET']}"
        return super().format(record)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

for handler in logger.handlers:
    handler.setFormatter(ColoredFormatter(
        '%(asctime)s [%(levelname)s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    ))

# ===========================
# ì„¤ì •ê°’
# ===========================

# Naver Open API ì„¤ì •
NAVER_CLIENT_ID = config.NAVER_CLIENT_ID
NAVER_CLIENT_SECRET = config.NAVER_CLIENT_SECRET

# User-Agent ëª©ë¡
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0'
]

# ê²€ìƒ‰ ì„¤ì •
SEARCH_KEYWORDS = ["í”¼ì— ì¸í„°ë‚´ì…”ë„", "PMInternational", "FitLine"]
MAX_SEARCH_RESULTS = 100
TOTAL_TARGET = 100

# í•„í„°ë§ í‚¤ì›Œë“œ
PM_BRAND_KEYWORDS = [
    "í”¼ì— ", "í”¼ì— ì¸í„°ë‚´ì…”ë„", "PM International", "PMInternational",
    "PM", "FitLine", "í•ë¼ì¸", "í”¼íŠ¸ë¼ì¸"
]

PM_SALES_KEYWORDS = [
    "ì¶”ì²œì¸", "ì¶”ì²œì¸ì½”ë“œ", "ì¶”ì²œì¸ ì½”ë“œ", "ì¶”ì²œì¸ë²ˆí˜¸", "ì¶”ì²œì¸ ë²ˆí˜¸",
    "íŒŒíŠ¸ë„ˆ", "íŒŒíŠ¸ë„ˆì½”ë“œ", "íŒŒíŠ¸ë„ˆ ì½”ë“œ", "íŒŒíŠ¸ë„ˆë²ˆí˜¸", "íŒŒíŠ¸ë„ˆ ë²ˆí˜¸",
    "ë“±ë¡", "ê°€ì…", "ë¬¸ì˜"
]

EXCLUDE_KEYWORDS = [
    "ë‰´ìŠ¤", "ê¸°ì‚¬", "ë³´ë„", "ê³µì§€", "ì•„ì¹´ë°ë¯¸", "ì„¸ë¯¸ë‚˜", "íŒ½ì°½íƒ±í¬", "ë°°ê´€"
]

# v7.3: í¬ë¡¤ë§ ì„¤ì •
PAGE_LOAD_TIMEOUT = 15
REQUEST_DELAY_MIN = 2.0
REQUEST_DELAY_MAX = 4.0
MAX_CONSECUTIVE_ERRORS = 5

# ===========================
# v7.3: ì ì‘í˜• ì†ë„ ì¡°ì ˆ
# ===========================

class AdaptiveDelay:
    """ì„±ê³µ/ì‹¤íŒ¨ì— ë”°ë¼ ëŒ€ê¸° ì‹œê°„ì„ ë™ì ìœ¼ë¡œ ì¡°ì ˆ"""
    
    def __init__(self, initial_min=2.0, initial_max=4.0):
        self.delay_min = initial_min
        self.delay_max = initial_max
        self.success_count = 0
        self.fail_count = 0
    
    def on_success(self):
        """ì„±ê³µ ì‹œ ëŒ€ê¸° ì‹œê°„ ë‹¨ì¶• (ìµœì†Œ 1ì´ˆê¹Œì§€)"""
        self.success_count += 1
        if self.success_count >= 3:
            self.delay_min = max(1.0, self.delay_min - 0.2)
            self.delay_max = max(2.0, self.delay_max - 0.3)
            self.success_count = 0
    
    def on_fail(self):
        """ì‹¤íŒ¨ ì‹œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€ (ìµœëŒ€ 10ì´ˆê¹Œì§€)"""
        self.fail_count += 1
        if self.fail_count >= 2:
            self.delay_min = min(5.0, self.delay_min + 0.5)
            self.delay_max = min(10.0, self.delay_max + 1.0)
            self.fail_count = 0
    
    def get_delay(self) -> float:
        """í˜„ì¬ ëŒ€ê¸° ì‹œê°„ ë²”ìœ„ì—ì„œ ëœë¤ ê°’ ë°˜í™˜"""
        return random.uniform(self.delay_min, self.delay_max)

# ===========================
# ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤
# ===========================

class CrawlStats:
    """í¬ë¡¤ë§ í†µê³„"""
    
    def __init__(self):
        self.total_attempts = 0
        self.success = 0
        self.filtered = 0
        self.duplicates = 0
        self.errors = 0
        self.start_time = time.time()
    
    def add_success(self):
        self.success += 1
        self.total_attempts += 1
    
    def add_filtered(self):
        self.filtered += 1
        self.total_attempts += 1
    
    def add_duplicate(self):
        self.duplicates += 1
        self.total_attempts += 1
    
    def add_error(self):
        self.errors += 1
        self.total_attempts += 1
    
    def print_stats(self):
        elapsed = time.time() - self.start_time
        logger.info(f"\n{'='*70}")
        logger.info("ğŸ“Š í¬ë¡¤ë§ í†µê³„")
        logger.info(f"{'='*70}")
        logger.info(f"ì´ ì‹œë„: {self.total_attempts}")
        logger.info(f"âœ… ì„±ê³µ: {self.success}")
        logger.info(f"ğŸ” í•„í„°ë§: {self.filtered}")
        logger.info(f"ğŸ”„ ì¤‘ë³µ: {self.duplicates}")
        logger.info(f"âŒ ì—ëŸ¬: {self.errors}")
        logger.info(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
        logger.info(f"{'='*70}")

class FailedURLManager:
    """ì‹¤íŒ¨ URL ê´€ë¦¬"""
    
    def __init__(self, filename='failed_urls.json'):
        self.filename = filename
        self.failed_urls = {}
        self.load_from_file()
    
    def add_failed(self, url: str, reason: str):
        if url not in self.failed_urls:
            self.failed_urls[url] = {
                'reason': reason,
                'count': 1,
                'last_attempt': datetime.now().isoformat()
            }
        else:
            self.failed_urls[url]['count'] += 1
            self.failed_urls[url]['last_attempt'] = datetime.now().isoformat()
    
    def load_from_file(self):
        if os.path.exists(self.filename):
            try:
                with open(self.filename, 'r', encoding='utf-8') as f:
                    loaded_data = json.load(f)
                    # listì¸ ê²½ìš° dictë¡œ ë³€í™˜
                    if isinstance(loaded_data, list):
                        self.failed_urls = {}
                    elif isinstance(loaded_data, dict):
                        self.failed_urls = loaded_data
                    else:
                        self.failed_urls = {}
            except:
                self.failed_urls = {}
    
    def save_to_file(self):
        if self.failed_urls:
            with open(self.filename, 'w', encoding='utf-8') as f:
                json.dump(self.failed_urls, f, ensure_ascii=False, indent=2)
    
    def get_failed_count(self) -> int:
        return len(self.failed_urls)

# ===========================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ===========================

def normalize_blog_url(blog_id: str, post_id: str) -> str:
    """ë¸”ë¡œê·¸ URL ì •ê·œí™”"""
    return f"https://blog.naver.com/{blog_id}/{post_id}"

def generate_post_fingerprint(post_data: Dict) -> str:
    """ê²Œì‹œë¬¼ ê³ ìœ  ì§€ë¬¸ ìƒì„± (ì¤‘ë³µ ë°©ì§€)"""
    title = post_data.get('title', '')
    content = post_data.get('content', '')[:200]
    return f"{title}_{content}"

def extract_blog_info_from_url(url: str) -> Optional[Dict[str, str]]:
    """URLì—ì„œ blog_idì™€ post_id ì¶”ì¶œ"""
    try:
        parsed = urlparse(url)
        
        # ë°©ë²• 1: /blog_id/post_id í˜•ì‹
        path_parts = [p for p in parsed.path.split('/') if p]
        if len(path_parts) >= 2:
            return {'blog_id': path_parts[0], 'post_id': path_parts[1]}
        
        # ë°©ë²• 2: ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°
        query_params = parse_qs(parsed.query)
        if 'blogId' in query_params and 'logNo' in query_params:
            return {
                'blog_id': query_params['blogId'][0],
                'post_id': query_params['logNo'][0]
            }
        
        return None
    except:
        return None

def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ì œ"""
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[\r\n\t]', ' ', text)
    return text.strip()

# ===========================
# v7.3: ë‚ ì§œ+ì‹œê°„ ì¶”ì¶œ í•¨ìˆ˜
# ===========================

def parse_published_datetime(date_text: str) -> str:
    """ë°œí–‰ ë‚ ì§œ+ì‹œê°„ íŒŒì‹± (v7.3)"""
    if not date_text:
        return ""
    
    try:
        date_text = date_text.strip()
        now = datetime.now()
        
        # íŒ¨í„´ 1: YYYY.MM.DD. HH:MM
        match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})\.\s*(\d{1,2}):(\d{2})', date_text)
        if match:
            year, month, day, hour, minute = match.groups()
            return f"{year}-{int(month):02d}-{int(day):02d} {int(hour):02d}:{minute}:00"
        
        # íŒ¨í„´ 2: YYYY.MM.DD (ì‹œê°„ ì •ë³´ ì—†ìŒ)
        match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})', date_text)
        if match:
            year, month, day = match.groups()
            return f"{year}-{int(month):02d}-{int(day):02d} 00:00:00"
        
        # íŒ¨í„´ 3: "Nì‹œê°„ ì „", "Në¶„ ì „"
        if 'ì‹œê°„ ì „' in date_text or 'ë¶„ ì „' in date_text:
            return now.strftime('%Y-%m-%d %H:%M:%S')
        
        # íŒ¨í„´ 4: "ì–´ì œ", "ê·¸ì œ"
        if 'ì–´ì œ' in date_text:
            yesterday = now - timedelta(days=1)
            return yesterday.strftime('%Y-%m-%d 00:00:00')
        elif 'ê·¸ì œ' in date_text:
            day_before = now - timedelta(days=2)
            return day_before.strftime('%Y-%m-%d 00:00:00')
        
        return ""
    except:
        return ""

# ===========================
# ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜
# ===========================

def extract_sponsor_phone(text: str) -> str:
    """ì¶”ì²œì¸ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ (v7.1: ì—„ê²©í•œ íŒ¨í„´)"""
    if not text:
        return ""
    
    # 010-xxxx-xxxx í˜•ì‹ë§Œ ìˆ˜ì§‘
    phone_patterns = [
        r'010[-\s]?\d{4}[-\s]?\d{4}',
        r'ì¶”ì²œì¸.*?010[-\s]?\d{4}[-\s]?\d{4}',
        r'ë¬¸ì˜.*?010[-\s]?\d{4}[-\s]?\d{4}',
        r'ì—°ë½ì²˜.*?010[-\s]?\d{4}[-\s]?\d{4}',
    ]
    
    for pattern in phone_patterns:
        match = re.search(pattern, text)
        if match:
            phone = match.group(0)
            # ìˆ«ìë§Œ ì¶”ì¶œ í›„ 010ìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
            digits = re.sub(r'\D', '', phone)
            if digits.startswith('010') and len(digits) == 11:
                return f"{digits[:3]}-{digits[3:7]}-{digits[7:]}"
    
    return ""

def extract_sponsor_partner_id(text: str) -> str:
    """ì¶”ì²œì¸ íŒŒíŠ¸ë„ˆ ID ì¶”ì¶œ (v7.1: 8ìë¦¬ ìˆ«ìë§Œ)"""
    if not text:
        return ""
    
    # ì •í™•íˆ 8ìë¦¬ ìˆ«ìë§Œ ì¶”ì¶œ
    partner_patterns = [
        r'ì¶”ì²œì¸\s*(?:ì½”ë“œ|ë²ˆí˜¸|ID)?\s*[:ï¼š]?\s*(\d{8})\b',
        r'íŒŒíŠ¸ë„ˆ\s*(?:ì½”ë“œ|ë²ˆí˜¸|ID)?\s*[:ï¼š]?\s*(\d{8})\b',
        r'ë“±ë¡\s*(?:ì½”ë“œ|ë²ˆí˜¸|ID)?\s*[:ï¼š]?\s*(\d{8})\b',
        r'\b(\d{8})\b',
    ]
    
    for pattern in partner_patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            # ì •í™•íˆ 8ìë¦¬ì¸ì§€ í™•ì¸
            if len(match) == 8:
                return match
    
    return ""

def extract_hashtags(soup: BeautifulSoup, content_text: str) -> str:
    """í•´ì‹œíƒœê·¸ ì¶”ì¶œ (v7.1: # ê¸°í˜¸ ìœ ì§€, ë³¸ë¬¸ í¬í•¨)"""
    hashtags = set()
    
    # ë°©ë²• 1: íƒœê·¸ ì˜ì—­ì—ì„œ ì¶”ì¶œ
    tag_elements = soup.select('a.link_tag, a[href*="tag"], .se_tag a')
    for elem in tag_elements:
        tag_text = elem.get_text(strip=True)
        if tag_text:
            if not tag_text.startswith('#'):
                tag_text = '#' + tag_text
            hashtags.add(tag_text)
    
    # ë°©ë²• 2: ë³¸ë¬¸ì—ì„œ #íƒœê·¸ ì¶”ì¶œ
    hashtag_pattern = r'#([ê°€-í£a-zA-Z0-9_]+)'
    matches = re.findall(hashtag_pattern, content_text)
    for match in matches:
        hashtags.add('#' + match)
    
    # ë°©ë²• 3: ë©”íƒ€ íƒœê·¸
    meta_keywords = soup.find('meta', {'name': 'keywords'})
    if meta_keywords and meta_keywords.get('content'):
        keywords = meta_keywords['content'].split(',')
        for keyword in keywords:
            keyword = keyword.strip()
            if keyword and len(keyword) > 0:
                if not keyword.startswith('#'):
                    keyword = '#' + keyword
                hashtags.add(keyword)
    
    return ', '.join(sorted(list(hashtags))) if hashtags else ""

def extract_image_urls(soup: BeautifulSoup) -> str:
    """ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
    image_urls = set()
    
    # ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ì„ íƒì
    img_elements = soup.select('img[src], img[data-src], .se-image-resource')
    
    for img in img_elements:
        src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
        if src and ('blogfiles.naver.net' in src or 'pstatic.net' in src):
            # ì¸ë„¤ì¼ì´ ì•„ë‹Œ ì›ë³¸ ì´ë¯¸ì§€ URLë¡œ ë³€í™˜
            src = re.sub(r'\?type=\w\d+', '', src)
            image_urls.add(src)
    
    return ', '.join(list(image_urls)[:10]) if image_urls else ""

def extract_video_urls(soup: BeautifulSoup) -> str:
    """ë¹„ë””ì˜¤ URL ì¶”ì¶œ"""
    video_urls = set()
    
    # ë¹„ë””ì˜¤ ë° iframe ì„ íƒì
    video_elements = soup.select('video source, iframe[src*="youtube"], iframe[src*="youtu.be"], iframe[src*="vimeo"]')
    
    for elem in video_elements:
        src = elem.get('src')
        if src:
            video_urls.add(src)
    
    return ', '.join(list(video_urls)[:5]) if video_urls else ""

def extract_like_count(driver: webdriver.Chrome, soup: BeautifulSoup) -> int:
    """ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ (v7.1: ë³„ë„ í•¨ìˆ˜)"""
    try:
        # ë°©ë²• 1: Seleniumìœ¼ë¡œ ì¶”ì¶œ
        like_selectors = [
            '.btn_empathy .count',
            '.area_like .count',
            'em.u_cnt._count',
            '.btn_like .count'
        ]
        
        for selector in like_selectors:
            try:
                elem = driver.find_element(By.CSS_SELECTOR, selector)
                like_text = elem.text.strip()
                like_count = int(re.sub(r'\D', '', like_text))
                if like_count > 0:
                    return like_count
            except:
                continue
        
        # ë°©ë²• 2: BeautifulSoupìœ¼ë¡œ ì¶”ì¶œ
        page_text = soup.get_text()
        like_patterns = [
            r'ê³µê°\s*(\d+)',
            r'ì¢‹ì•„ìš”\s*(\d+)',
            r'empathy.*?(\d+)'
        ]
        for pattern in like_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                return int(match.group(1))
        
        return 0
    except Exception as e:
        logger.debug(f"ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return 0

def extract_comment_count(driver: webdriver.Chrome, soup: BeautifulSoup) -> int:
    """ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ (v7.1: ë³„ë„ í•¨ìˆ˜)"""
    try:
        # ë°©ë²• 1: Seleniumìœ¼ë¡œ ì¶”ì¶œ
        comment_selectors = [
            '.btn_comment .count',
            '.area_comment .count',
            'em.u_cnt._count',
            '.cmt_count'
        ]
        
        for selector in comment_selectors:
            try:
                elem = driver.find_element(By.CSS_SELECTOR, selector)
                comment_text = elem.text.strip()
                comment_count = int(re.sub(r'\D', '', comment_text))
                if comment_count > 0:
                    return comment_count
            except:
                continue
        
        # ë°©ë²• 2: BeautifulSoupìœ¼ë¡œ ì¶”ì¶œ
        page_text = soup.get_text()
        comment_patterns = [
            r'ëŒ“ê¸€\s*(\d+)',
            r'comment.*?(\d+)'
        ]
        for pattern in comment_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                return int(match.group(1))
        
        return 0
    except Exception as e:
        logger.debug(f"ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return 0

# ===========================
# í•„í„°ë§ í•¨ìˆ˜
# ===========================

def content_passes_filter(title: str, content: str, full_text: str, 
                          sponsor_partner_id: str) -> Tuple[bool, str]:
    """ì½˜í…ì¸  í•„í„°ë§ (v7.1 ê·œì¹™)
    
    ê·œì¹™:
    1. ["í”¼ì— ", "í”¼ì— ì¸í„°ë‚´ì…”ë„"] ì¤‘ í•˜ë‚˜ ë°˜ë“œì‹œ ì¡´ì¬
    2. ["ì¶”ì²œì¸" í‚¤ì›Œë“œ OR 8ìë¦¬ ìˆ«ì] ì¤‘ í•˜ë‚˜ ë°˜ë“œì‹œ ì¡´ì¬
    3. ì œì™¸ í‚¤ì›Œë“œ ì¤‘ ë‘˜ ì´ìƒ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì œì™¸
    
    Returns:
        (í†µê³¼ì—¬ë¶€, ì‹¤íŒ¨ì‚¬ìœ )
    """
    text_lower = full_text.lower()
    
    # ê·œì¹™ 1: PM ë¸Œëœë“œ í‚¤ì›Œë“œ ì²´í¬
    has_pm_keyword = any(keyword.lower() in text_lower for keyword in PM_BRAND_KEYWORDS)
    if not has_pm_keyword:
        return False, "PM ë¸Œëœë“œ í‚¤ì›Œë“œ ì—†ìŒ"
    
    # ê·œì¹™ 2: íŒë§¤ì› í™œë™ í‚¤ì›Œë“œ OR 8ìë¦¬ ìˆ«ì ì²´í¬
    has_sales_keyword = any(keyword in full_text for keyword in PM_SALES_KEYWORDS)
    has_8digit = bool(sponsor_partner_id)
    
    if not (has_sales_keyword or has_8digit):
        return False, "íŒë§¤ì› ê´€ë ¨ í‚¤ì›Œë“œ ì—†ìŒ"
    
    # ê·œì¹™ 3: ì œì™¸ í‚¤ì›Œë“œ ì²´í¬ (ë‘˜ ì´ìƒ ìˆìœ¼ë©´ ì œì™¸)
    exclude_count = sum(1 for keyword in EXCLUDE_KEYWORDS if keyword in full_text)
    if exclude_count >= 2:
        return False, f"ì œì™¸ í‚¤ì›Œë“œ {exclude_count}ê°œ ë°œê²¬"
    
    return True, ""

# ===========================
# Selenium ë“œë¼ì´ë²„ ì„¤ì •
# ===========================

def setup_driver() -> webdriver.Chrome:
    """Selenium ë“œë¼ì´ë²„ ì„¤ì • (v7.3: ë©”ëª¨ë¦¬ ìµœì í™”)"""
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # v7.3: ë©”ëª¨ë¦¬ ìµœì í™”
    chrome_options.add_argument('--disable-extensions')
    chrome_options.add_argument('--disable-plugins')
    chrome_options.add_argument('--disable-images')
    chrome_options.add_argument('--blink-settings=imagesEnabled=false')
    
    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    chrome_options.add_argument(f'user-agent={user_agent}')
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)
    
    # ìë™í™” ê°ì§€ ìš°íšŒ
    driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
        'source': '''
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            })
        '''
    })
    
    return driver

# ===========================
# ê²€ìƒ‰ í•¨ìˆ˜
# ===========================

def search_naver_blog(keyword: str, display: int = 100, start: int = 1) -> Optional[Dict]:
    """Naver Open Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡œê·¸ ê²€ìƒ‰"""
    url = "https://openapi.naver.com/v1/search/blog.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
        "User-Agent": random.choice(USER_AGENTS)
    }
    params = {
        "query": keyword,
        "display": display,
        "start": start,
        "sort": "sim"
    }
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {str(e)}")
        return None

def parse_search_results(search_data: Dict) -> List[Dict]:
    """API ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±"""
    results = []
    
    if not search_data or 'items' not in search_data:
        return results
    
    for item in search_data['items']:
        try:
            url = item.get('link', '')
            if not url or 'blog.naver.com' not in url:
                continue
            
            blog_info = extract_blog_info_from_url(url)
            if not blog_info:
                continue
            
            results.append({
                'url': url,
                'blog_id': blog_info['blog_id'],
                'post_id': blog_info['post_id'],
                'postdate': item.get('postdate', ''),
                'bloggername': item.get('bloggername', '')
            })
        except Exception as e:
            continue
    
    logger.info(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±: {len(results)}ê°œ")
    return results

# ===========================
# í¬ë¡¤ë§ í•¨ìˆ˜
# ===========================

def crawl_blog_post_selenium(driver: webdriver.Chrome, url: str, blog_id: str, 
                            post_id: str, failed_url_manager: FailedURLManager) -> Optional[Dict]:
    """Seleniumì„ ì‚¬ìš©í•œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ í¬ë¡¤ë§ (v7.3)"""
    try:
        logger.debug(f"í¬ë¡¤ë§ ì‹œì‘: {url}")
        driver.get(url)
        
        # iframe ëŒ€ê¸° ë° ì „í™˜
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.ID, 'mainFrame'))
            )
            driver.switch_to.frame('mainFrame')
        except TimeoutException:
            logger.debug("iframe ì—†ìŒ - ë³¸ë¬¸ ì§ì ‘ í¬ë¡¤ë§")
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        time.sleep(2)
        
        # HTML íŒŒì‹±
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ
        title = ""
        title_selectors = [
            '.se-title-text', '.pcol1', '.se_title', 
            '.post-view .tit', '.tit_h3', 'h3.se_title'
        ]
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = clean_text(title_elem.get_text())
                break
        
        if not title:
            failed_url_manager.add_failed(url, "ì œëª© ì—†ìŒ")
            return None
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        content = ""
        content_selectors = [
            '.se-main-container', '.post-view', '.se_component_wrap',
            '#postViewArea', '.post_ct'
        ]
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = clean_text(content_elem.get_text())
                break
        
        if not content:
            failed_url_manager.add_failed(url, "ë³¸ë¬¸ ì—†ìŒ")
            return None
        
        # v7.3: ë°œí–‰ ë‚ ì§œ+ì‹œê°„ ì¶”ì¶œ
        published_datetime = ""
        date_selectors = [
            '.se_publishDate', '.post-view .date', '.se_date',
            '.post_info .date', 'span.se_publishDate', '.blog2_series .date',
            '.blog-category .date', '.post_date', 'p.date', 'span.date',
            '.post-meta .date', '.entry-date'
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                published_datetime = parse_published_datetime(date_text)
                if published_datetime:
                    break
        
        # ì „ì²´ í…ìŠ¤íŠ¸ (í•„í„°ë§ìš©)
        full_text = f"{title} {content}"
        
        # ì¶”ì²œì¸ ì •ë³´ ì¶”ì¶œ
        sponsor_phone = extract_sponsor_phone(full_text)
        sponsor_partner_id = extract_sponsor_partner_id(full_text)
        
        # í•„í„°ë§ ê²€ì‚¬
        passes, reason = content_passes_filter(title, content, full_text, sponsor_partner_id)
        if not passes:
            logger.debug(f"í•„í„°ë§ë¨: {reason} - {title[:50]}")
            failed_url_manager.add_failed(url, f"í•„í„°ë§: {reason}")
            return None
        
        # í•´ì‹œíƒœê·¸ ì¶”ì¶œ
        hashtags = extract_hashtags(soup, content)
        
        # ì´ë¯¸ì§€/ë¹„ë””ì˜¤ URL ì¶”ì¶œ
        image_urls = extract_image_urls(soup)
        video_urls = extract_video_urls(soup)
        
        # ì¢‹ì•„ìš”/ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ (v7.1: ë³„ë„ í•¨ìˆ˜)
        like_count = extract_like_count(driver, soup)
        comment_count = extract_comment_count(driver, soup)
        
        # ë°ì´í„° êµ¬ì„±
        post_data = {
            'platform': 'naver_blog',
            'post_id': f"{blog_id}_{post_id}",  # v7.1 í˜•ì‹
            'blog_id': blog_id,
            'url': url,
            'title': title,
            'content': content,
            'published_datetime': published_datetime,  # v7.3: ì‹œê°„ í¬í•¨
            'sponsor_phone': sponsor_phone,
            'sponsor_partner_id': sponsor_partner_id,
            'like_count': like_count,
            'comment_count': comment_count,
            'hashtags': hashtags,
            'image_urls': image_urls,
            'video_urls': video_urls,
            'collected_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        driver.switch_to.default_content()
        return post_data
        
    except TimeoutException:
        logger.debug(f"íƒ€ì„ì•„ì›ƒ: {url}")
        failed_url_manager.add_failed(url, "íƒ€ì„ì•„ì›ƒ")
        return None
    except Exception as e:
        logger.debug(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
        failed_url_manager.add_failed(url, f"ì˜¤ë¥˜: {str(e)}")
        return None
    finally:
        try:
            driver.switch_to.default_content()
        except:
            pass

# ===========================
# ë©”ì¸ í•¨ìˆ˜
# ===========================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (v7.3)"""
    logger.info("="*70)
    logger.info("ğŸš€ PM International ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v7.3 ì‹œì‘")
    logger.info("="*70)
    
    driver = setup_driver()
    stats = CrawlStats()
    failed_url_manager = FailedURLManager()
    adaptive = AdaptiveDelay()  # v7.3: ì ì‘í˜• ëŒ€ê¸°
    
    collected_posts = []
    collected_urls = set()
    collected_fingerprints = set()
    consecutive_errors = 0  # v7.3: ì—°ì† ì—ëŸ¬ ì¹´ìš´íŠ¸
    crawl_count = 0  # v7.3: í¬ë¡¤ë§ ì¹´ìš´íŠ¸
    
    try:
        # ê²€ìƒ‰ í‚¤ì›Œë“œë³„ í¬ë¡¤ë§
        for keyword in SEARCH_KEYWORDS:
            if len(collected_posts) >= TOTAL_TARGET:
                break
            
            logger.info(f"\n{'='*70}")
            logger.info(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰: {keyword}")
            logger.info(f"{'='*70}")
            
            # API ê²€ìƒ‰
            search_data = search_naver_blog(keyword, MAX_SEARCH_RESULTS)
            if not search_data:
                logger.warning(f"'{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                continue
            
            # ê²°ê³¼ íŒŒì‹±
            search_results = parse_search_results(search_data)
            logger.info(f"ğŸ“ '{keyword}' ìµœì¢… ê²°ê³¼: {len(search_results)}ê°œ")
            
            if not search_results:
                logger.warning(f"'{keyword}' íŒŒì‹± ê²°ê³¼ ì—†ìŒ")
                continue
            
            for result in search_results:
                if len(collected_posts) >= TOTAL_TARGET:
                    break
                
                blog_id = result['blog_id']
                post_id = result['post_id']
                normalized_url = normalize_blog_url(blog_id, post_id)
                
                # ì¤‘ë³µ ì²´í¬
                if normalized_url in collected_urls:
                    stats.add_duplicate()
                    continue
                
                logger.info(f"[{len(collected_posts)+1}/{TOTAL_TARGET}] í¬ë¡¤ë§ ì¤‘...")
                
                # í¬ë¡¤ë§ ì‹¤í–‰
                post_data = crawl_blog_post_selenium(
                    driver, normalized_url, blog_id, post_id, failed_url_manager
                )
                
                if post_data:
                    # ì¤‘ë³µ ì²´í¬ (ì§€ë¬¸ ê¸°ë°˜)
                    fingerprint = generate_post_fingerprint(post_data)
                    if fingerprint not in collected_fingerprints:
                        collected_posts.append(post_data)
                        collected_urls.add(normalized_url)
                        collected_fingerprints.add(fingerprint)
                        stats.add_success()
                        consecutive_errors = 0  # v7.3: ì„±ê³µ ì‹œ ì—ëŸ¬ ì¹´ìš´íŠ¸ ë¦¬ì…‹
                        adaptive.on_success()  # v7.3: ì„±ê³µ ì‹œ ëŒ€ê¸° ì‹œê°„ ë‹¨ì¶•
                        logger.info(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {post_data['title'][:50]}")
                    else:
                        stats.add_duplicate()
                else:
                    stats.add_filtered()
                    consecutive_errors += 1  # v7.3: ì—ëŸ¬ ì¹´ìš´íŠ¸ ì¦ê°€
                    adaptive.on_fail()  # v7.3: ì‹¤íŒ¨ ì‹œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                
                # v7.3: ì—°ì† ì—ëŸ¬ ì‹œ ë“œë¼ì´ë²„ ì¬ì‹œì‘
                if consecutive_errors >= MAX_CONSECUTIVE_ERRORS:
                    logger.warning(f"âš ï¸  ì—°ì† {MAX_CONSECUTIVE_ERRORS}íšŒ ì—ëŸ¬ - ë“œë¼ì´ë²„ ì¬ì‹œì‘")
                    driver.quit()
                    time.sleep(3)
                    driver = setup_driver()
                    consecutive_errors = 0
                    gc.collect()  # v7.3: ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                
                crawl_count += 1
                
                # v7.3: ì ì‘í˜• ëŒ€ê¸° ì‹œê°„
                delay = adaptive.get_delay()
                time.sleep(delay)
                
                # v7.3: ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬
                if crawl_count % 20 == 0:
                    gc.collect()
            
            if len(collected_posts) < TOTAL_TARGET:
                time.sleep(random.uniform(2, 4))
        
        stats.print_stats()
        
        # CSV ì €ì¥ (v7.3: ì»¬ëŸ¼ ìˆœì„œ ëª…ì‹œ)
        if collected_posts:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'naver_blog_pm_v7_3_{timestamp}.csv'
            
            df = pd.DataFrame(collected_posts)
            
            # v7.3: ì»¬ëŸ¼ ìˆœì„œ ëª…ì‹œì  ì •ì˜
            column_order = [
                'platform', 'post_id', 'blog_id', 'url', 'title', 'content',
                'published_datetime', 'sponsor_phone', 'sponsor_partner_id',
                'like_count', 'comment_count', 'hashtags', 'image_urls',
                'video_urls', 'collected_date'
            ]
            
            df = df[column_order]
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            logger.info(f"\n{'='*70}")
            logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
            logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(collected_posts)}ê°œ")
            logger.info(f"ğŸ“‹ ì»¬ëŸ¼: {len(column_order)}ê°œ")
            logger.info(f"{'='*70}")
        else:
            logger.warning("âš ï¸  ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        failed_url_manager.save_to_file()
        if failed_url_manager.get_failed_count() > 0:
            logger.info(f"âŒ ì‹¤íŒ¨ URL: {failed_url_manager.get_failed_count()}ê°œ (failed_urls.json)")
    
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  ì‚¬ìš©ì ì¤‘ë‹¨")
        stats.print_stats()
    
    except Exception as e:
        logger.error(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        stats.print_stats()
    
    finally:
        driver.quit()
        logger.info("âœ… ë“œë¼ì´ë²„ ì¢…ë£Œ")
        logger.info("="*70)
        logger.info("ğŸ í¬ë¡¤ë§ ì™„ë£Œ")
        logger.info("="*70)

if __name__ == "__main__":
    main()
