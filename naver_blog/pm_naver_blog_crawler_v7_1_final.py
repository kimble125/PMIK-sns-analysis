#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PM-International Korea ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v7.1 (ìµœì¢… ì™„ì„±íŒ)

ğŸ¯ ì£¼ìš” ê°œì„  ì‚¬í•­:
1. âœ… sponsor_name ì‚­ì œ (ì˜¤ì‘ë™ ë°©ì§€)
2. âœ… sponsor_phone: 010-xxxx-xxxx í˜•ì‹ë§Œ ì—„ê²©í•˜ê²Œ ìˆ˜ì§‘
3. âœ… sponsor_partner_id: ì •í™•íˆ 8ìë¦¬ ìˆ«ìë§Œ ìˆ˜ì§‘
4. âœ… like_count, comment_count: v6.6ì˜ ê²€ì¦ëœ ë¡œì§ ë³µì› (ë³„ë„ í•¨ìˆ˜)
5. âœ… hashtags: # ê¸°í˜¸ ìœ ì§€, ë³¸ë¬¸ ë‚´ í•´ì‹œíƒœê·¸ë„ ëª¨ë‘ ì¶”ì¶œ
6. âœ… ì½˜í…ì¸  í•„í„°ë§: 
    - ["í”¼ì— ", "í”¼ì— ì¸í„°ë‚´ì…”ë„"] ì¤‘ í•˜ë‚˜ ë°˜ë“œì‹œ ì¡´ì¬
    - ["ì¶”ì²œì¸" ë˜ëŠ” 8ìë¦¬ ìˆ«ì] ì¤‘ í•˜ë‚˜ ë°˜ë“œì‹œ ì¡´ì¬
    - ì œì™¸ í‚¤ì›Œë“œ ë‘˜ ì´ìƒ ì‹œ ë°˜ë“œì‹œ ì œì™¸
7. âœ… post_id: {blog_id}_{post_id} í˜•ì‹ìœ¼ë¡œ ë³€ê²½
8. âœ… platform ì»¬ëŸ¼ ì¶”ê°€

ğŸ“Š ì¶œë ¥ ì»¬ëŸ¼ (15ê°œ):
- ê¸°ë³¸: platform, post_id, blog_id, url, title, content, published_date
- ì¶”ì²œì¸: sponsor_phone, sponsor_partner_id
- ì°¸ì—¬: like_count, comment_count
- ì½˜í…ì¸ : hashtags, image_urls, video_urls
- ë©”íƒ€: collected_date

ì‘ì„±ì: PMI Korea ë°ì´í„° ë¶„ì„íŒ€
ë²„ì „: 7.1
ìµœì¢… ìˆ˜ì •ì¼: 2025-11-06
"""

import os
import re
import json
import time
import random
import logging
from datetime import datetime
from typing import List, Dict, Optional, Set, Tuple
from urllib.parse import urlparse, parse_qs, unquote

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd
import config

# ===========================
# ë¡œê¹… ì„¤ì •
# ===========================

class ColoredFormatter(logging.Formatter):
    """ì»¬ëŸ¬ ë¡œê¹… í¬ë§·í„°"""
    
    COLORS = {
        'DEBUG': '\033[94m',    # íŒŒë€ìƒ‰
        'INFO': '\033[92m',     # ì´ˆë¡ìƒ‰
        'WARNING': '\033[93m',  # ë…¸ë€ìƒ‰
        'ERROR': '\033[91m',    # ë¹¨ê°„ìƒ‰
        'CRITICAL': '\033[95m', # ë³´ë¼ìƒ‰
        'RESET': '\033[0m'
    }
    
    def format(self, record):
        log_color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
        record.levelname = f"{log_color}{record.levelname}{self.COLORS['RESET']}"
        return super().format(record)

logging.basicConfig(
    level=logging.DEBUG,  # INFO â†’ DEBUGë¡œ ë³€ê²½
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)
logger.propagate = False

# Ensure at least one handler exists and apply colored formatter safely
if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.DEBUG)  # INFO â†’ DEBUGë¡œ ë³€ê²½
    console_handler.setFormatter(ColoredFormatter('%(asctime)s - %(levelname)s - %(message)s'))
    logger.addHandler(console_handler)
else:
    # Update the first handler's formatter
    logger.handlers[0].setFormatter(ColoredFormatter('%(asctime)s - %(levelname)s - %(message)s'))

# ===========================
# ì„¤ì •
# ===========================

# Naver Open API ì„¤ì •
NAVER_CLIENT_ID = config.NAVER_CLIENT_ID
NAVER_CLIENT_SECRET = config.NAVER_CLIENT_SECRET

# User-Agent ëª©ë¡
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0'
]

# ê²€ìƒ‰ í‚¤ì›Œë“œ
SEARCH_KEYWORDS = [
    "í”¼ì— ì¸í„°ë‚´ì…”ë„",
    "í”¼ì— ì½”ë¦¬ì•„",
    "PMì¸í„°ë‚´ì…”ë„",
    "í•ë¼ì¸",
    "FitLine"
]

# í•„ìˆ˜ í¬í•¨ í‚¤ì›Œë“œ (v7.1: ë¸Œëœë“œ í‚¤ì›Œë“œ)
PM_BRAND_KEYWORDS = ["í”¼ì— ", "í”¼ì— ì¸í„°ë‚´ì…”ë„", "PM", "PMì¸í„°ë‚´ì…”ë„", "PM-International"]

# í•„ìˆ˜ í¬í•¨ í‚¤ì›Œë“œ (v7.1: íŒë§¤ì› í™œë™ í‚¤ì›Œë“œ)
PM_SALES_KEYWORDS = ["ì¶”ì²œì¸", "íŒŒíŠ¸ë„ˆë²ˆí˜¸", "íšŒì›ë²ˆí˜¸", "íŒŒíŠ¸ë„ˆID", "ë“±ë¡ë²ˆí˜¸"]

# ì œì™¸ í‚¤ì›Œë“œ (v7.1: ì‚¬ìš©ì ì •ì˜ - í¬ë¡¤ë§í•˜ë©´ì„œ í™•ì¥)
EXCLUDE_KEYWORDS = [
    "ë‰´ìŠ¤", "ê¸°ì‚¬", "ë³´ë„", "ê³µì§€",
    "ì•„ì¹´ë°ë¯¸", "ì„¸ë¯¸ë‚˜", "íŒ½ì°½íƒ±í¬", "ë°°ê´€"
]

# ìˆ˜ì§‘ ì„¤ì •
POSTS_PER_KEYWORD = 20
TOTAL_TARGET = 100
MAX_SEARCH_RESULTS = 50

# íƒ€ì´ë° ì„¤ì •
SELENIUM_WAIT_TIMEOUT = 10
IFRAME_WAIT_TIMEOUT = 15
REQUEST_DELAY_MIN = 1.5
REQUEST_DELAY_MAX = 3.0

# ===========================
# ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤
# ===========================

class CrawlStats:
    """í¬ë¡¤ë§ í†µê³„"""
    def __init__(self):
        self.total_searched = 0
        self.total_collected = 0
        self.total_duplicates = 0
        self.total_filtered = 0
        self.total_errors = 0
        self.start_time = time.time()
    
    def add_success(self):
        self.total_collected += 1
    
    def add_duplicate(self):
        self.total_duplicates += 1
    
    def add_filtered(self):
        self.total_filtered += 1
    
    def add_error(self):
        self.total_errors += 1
    
    def print_stats(self):
        elapsed = time.time() - self.start_time
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ“Š í¬ë¡¤ë§ í†µê³„")
        logger.info(f"{'='*70}")
        logger.info(f"âœ… ìˆ˜ì§‘ ì„±ê³µ: {self.total_collected}ê°œ")
        logger.info(f"ğŸ”„ ì¤‘ë³µ ì œê±°: {self.total_duplicates}ê°œ")
        logger.info(f"ğŸš« í•„í„°ë§: {self.total_filtered}ê°œ")
        logger.info(f"âŒ ì˜¤ë¥˜: {self.total_errors}ê°œ")
        logger.info(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
        logger.info(f"{'='*70}\n")

class FailedURLManager:
    """ì‹¤íŒ¨í•œ URL ê´€ë¦¬"""
    def __init__(self):
        self.failed_urls: List[Dict] = []
    
    def add_failed_url(self, url: str, reason: str, error: str = ""):
        self.failed_urls.append({
            'url': url,
            'reason': reason,
            'error': str(error),
            'timestamp': datetime.now().isoformat()
        })
    
    def save_to_file(self, filename: str = 'failed_urls.json'):
        if self.failed_urls:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.failed_urls, f, ensure_ascii=False, indent=2)
    
    def get_failed_count(self) -> int:
        return len(self.failed_urls)

# ===========================
# í—¬í¼ í•¨ìˆ˜
# ===========================

def normalize_blog_url(blog_id: str, post_id: str) -> str:
    """ë¸”ë¡œê·¸ URL ì •ê·œí™”"""
    return f"https://blog.naver.com/{blog_id}/{post_id}"

def generate_post_fingerprint(post_data: Dict) -> str:
    """ê²Œì‹œë¬¼ ê³ ìœ  ì§€ë¬¸ ìƒì„±"""
    title = post_data.get('title', '')
    content = post_data.get('content', '')[:200]
    return f"{title}_{content}"

def extract_blog_info_from_url(url: str) -> Optional[Dict[str, str]]:
    """URLì—ì„œ blog_idì™€ post_id ì¶”ì¶œ
    
    ì§€ì› í˜•ì‹:
    - https://blog.naver.com/blog_id/post_id
    - https://blog.naver.com/PostView.nhn?blogId=blog_id&logNo=post_id
    - https://m.blog.naver.com/blog_id/post_id
    
    Returns:
        {'blog_id': str, 'post_id': str} ë˜ëŠ” None
    """
    try:
        parsed_url = urlparse(url)
        
        # í˜•ì‹ 1: /blog_id/post_id
        path_match = re.match(r'/([^/]+)/(\d+)', parsed_url.path)
        if path_match:
            return {
                'blog_id': path_match.group(1),
                'post_id': path_match.group(2)
            }
        
        # í˜•ì‹ 2: PostView.nhn?blogId=...&logNo=...
        if 'PostView' in parsed_url.path:
            query_params = parse_qs(parsed_url.query)
            blog_id = query_params.get('blogId', [None])[0]
            post_id = query_params.get('logNo', [None])[0]
            if blog_id and post_id:
                return {
                    'blog_id': blog_id,
                    'post_id': post_id
                }
        
        return None
    except Exception as e:
        logger.error(f"URL íŒŒì‹± ì‹¤íŒ¨: {url} - {str(e)}")
        return None

def parse_korean_date(date_str: str) -> Optional[str]:
    """í•œêµ­ì–´ ë‚ ì§œ ë¬¸ìì—´ì„ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    
    ì˜ˆ: '2024. 11. 5. 14:30' -> '2024-11-05'
    """
    try:
        # ë¶ˆí•„ìš”í•œ ê³µë°± ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        date_str = re.sub(r'\s+', ' ', date_str.strip())
        
        # íŒ¨í„´ 1: YYYY. MM. DD. HH:MM
        match = re.search(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.\s*\d{1,2}:\d{2}', date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{int(month):02d}-{int(day):02d}"
        
        # íŒ¨í„´ 2: YYYY. MM. DD.
        match = re.search(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.', date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{int(month):02d}-{int(day):02d}"
        
        # íŒ¨í„´ 3: YYYY-MM-DD
        match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', date_str)
        if match:
            year, month, day = match.groups()
            return f"{year}-{int(month):02d}-{int(day):02d}"
        
        return None
    except Exception as e:
        logger.debug(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str} - {str(e)}")
        return None

def extract_sponsor_phone(text: str) -> str:
    """ì¶”ì²œì¸ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ (010-xxxx-xxxx í˜•ì‹ë§Œ)"""
    if not text:
        return ""
    
    # 010-xxxx-xxxx í˜•ì‹ë§Œ ì¶”ì¶œ (ì •í™•íˆ 11ìë¦¬)
    pattern = r'010[-\s]?\d{4}[-\s]?\d{4}'
    matches = re.findall(pattern, text)
    
    if matches:
        # í•˜ì´í”ˆ ì •ê·œí™”
        phone = re.sub(r'\s', '', matches[0])
        phone = re.sub(r'(\d{3})(\d{4})(\d{4})', r'\1-\2-\3', phone)
        return phone
    
    return ""

def extract_sponsor_partner_id(text: str) -> str:
    """ì¶”ì²œì¸ íŒŒíŠ¸ë„ˆ ID ì¶”ì¶œ (ì •í™•íˆ 8ìë¦¬ ìˆ«ìë§Œ)"""
    if not text:
        return ""
    
    # ì •í™•íˆ 8ìë¦¬ ìˆ«ìë§Œ (ì•ë’¤ì— ìˆ«ìê°€ ì—†ì–´ì•¼ í•¨)
    pattern = r'(?<!\d)(\d{8})(?!\d)'
    matches = re.findall(pattern, text)
    
    if matches:
        return matches[0]
    
    return ""

def has_eight_digit_number(text: str) -> bool:
    """í…ìŠ¤íŠ¸ì— 8ìë¦¬ ìˆ«ìê°€ ìˆëŠ”ì§€ í™•ì¸"""
    if not text:
        return False
    pattern = r'(?<!\d)(\d{8})(?!\d)'
    return bool(re.search(pattern, text))

def extract_hashtags(soup: BeautifulSoup, content_text: str) -> str:
    """í•´ì‹œíƒœê·¸ ì¶”ì¶œ (# ê¸°í˜¸ í¬í•¨, ë³¸ë¬¸ ë‚´ í•´ì‹œíƒœê·¸ë„ ì¶”ì¶œ)"""
    hashtags_set = set()
    
    # ë°©ë²• 1: HTML íƒœê·¸ì—ì„œ ì¶”ì¶œ
    tag_selectors = [
        'a.link_tag',
        'a[href*="tag"]',
        'span.ell',
        'div.post_tag a',
        'div.tag_area a'
    ]
    
    for selector in tag_selectors:
        tags = soup.select(selector)
        for tag in tags:
            tag_text = tag.get_text(strip=True)
            if tag_text and not tag_text.startswith('#'):
                tag_text = f"#{tag_text}"
            if tag_text:
                hashtags_set.add(tag_text)
    
    # ë°©ë²• 2: ë³¸ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ # íŒ¨í„´ ì¶”ì¶œ
    if content_text:
        # #ë¡œ ì‹œì‘í•˜ê³  ê³µë°±ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ì ì „ê¹Œì§€
        hashtag_pattern = r'#[^\s#,ØŒ]+' 
        matches = re.findall(hashtag_pattern, content_text)
        for match in matches:
            # ëì˜ íŠ¹ìˆ˜ë¬¸ì ì œê±°
            cleaned = re.sub(r'[.,!?;:\)]+$', '', match)
            if len(cleaned) > 1:  # # ë‹¤ìŒì— ë¬¸ìê°€ ìˆëŠ” ê²½ìš°ë§Œ
                hashtags_set.add(cleaned)
    
    return ', '.join(sorted(hashtags_set)) if hashtags_set else ""

def extract_like_count(driver, soup) -> int:
    """ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ (v6.6 ë¡œì§ ë³µì›)"""
    try:
        # Seleniumìœ¼ë¡œ ì‹œë„
        like_selectors = [
            'span.u_cnt._count',
            'span.cnt_like',
            'em.u_cnt',
            'span.like_count',
            'a.btn_empathy span.u_cnt',
            'div.end_btn span.u_cnt',
            'span#printLog'
        ]
        
        for selector in like_selectors:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                for element in elements:
                    text = element.text.strip()
                    if text and text.isdigit():
                        return int(text)
            except:
                continue
        
        # BeautifulSoupë¡œ ì‹œë„
        for selector in like_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text(strip=True)
                if text and text.isdigit():
                    return int(text)
        
        # í…ìŠ¤íŠ¸ì—ì„œ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ
        page_text = soup.get_text()
        like_patterns = [
            r'ì¢‹ì•„ìš”\s*(\d+)',
            r'ê³µê°\s*(\d+)',
            r'empathy.*?(\d+)'
        ]
        for pattern in like_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                return int(match.group(1))
        
        return 0
    except Exception as e:
        logger.debug(f"ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return 0

def extract_comment_count(driver, soup) -> int:
    """ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ (v6.6 ë¡œì§ ë³µì›)"""
    try:
        # Seleniumìœ¼ë¡œ ì‹œë„
        comment_selectors = [
            'span.u_cnt._count.ccmtcnt',
            'span.cnt_cmt',
            'em.u_cmt',
            'span.comment_count',
            'a.btn_comment span.u_cnt',
            'div.end_btn span.u_cnt',
            'span.num'
        ]
        
        for selector in comment_selectors:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                for element in elements:
                    text = element.text.strip()
                    # ëŒ“ê¸€ ë²„íŠ¼ì— 'ëŒ“ê¸€ N' í˜•ì‹ì¼ ìˆ˜ ìˆìŒ
                    number_match = re.search(r'\d+', text)
                    if number_match:
                        return int(number_match.group())
            except:
                continue
        
        # BeautifulSoupë¡œ ì‹œë„
        for selector in comment_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text(strip=True)
                number_match = re.search(r'\d+', text)
                if number_match:
                    return int(number_match.group())
        
        # í…ìŠ¤íŠ¸ì—ì„œ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ
        page_text = soup.get_text()
        comment_patterns = [
            r'ëŒ“ê¸€\s*(\d+)',
            r'ì½”ë©˜íŠ¸\s*(\d+)',
            r'comment.*?(\d+)'
        ]
        for pattern in comment_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                return int(match.group(1))
        
        return 0
    except Exception as e:
        logger.debug(f"ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return 0

def content_passes_filter(title: str, content: str, full_text: str) -> Tuple[bool, str]:
    """ì½˜í…ì¸  í•„í„°ë§ (v7.1 ê°•í™”ëœ ë¡œì§)
    
    ê·œì¹™:
    1. ["í”¼ì— ", "í”¼ì— ì¸í„°ë‚´ì…”ë„"] ì¤‘ í•˜ë‚˜ ë°˜ë“œì‹œ ì¡´ì¬
    2. ["ì¶”ì²œì¸" ë˜ëŠ” 8ìë¦¬ ìˆ«ì] ì¤‘ í•˜ë‚˜ ë°˜ë“œì‹œ ì¡´ì¬
    3. ì œì™¸ í‚¤ì›Œë“œ ì¤‘ ë‘˜ ì´ìƒ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì œì™¸
    
    Returns:
        (í†µê³¼ì—¬ë¶€, ì‹¤íŒ¨ì‚¬ìœ )
    """
    # ì „ì²´ í…ìŠ¤íŠ¸ ì¤€ë¹„
    text_lower = full_text.lower()
    
    # ê·œì¹™ 1: PM ë¸Œëœë“œ í‚¤ì›Œë“œ ì²´í¬
    has_pm_keyword = any(keyword.lower() in text_lower for keyword in PM_BRAND_KEYWORDS)
    if not has_pm_keyword:
        return False, "PM ë¸Œëœë“œ í‚¤ì›Œë“œ ì—†ìŒ"
    
    # ê·œì¹™ 2: íŒë§¤ì› í™œë™ í‚¤ì›Œë“œ ë˜ëŠ” 8ìë¦¬ ìˆ«ì ì²´í¬
    has_sales_keyword = any(keyword in full_text for keyword in PM_SALES_KEYWORDS)
    has_eight_digit = has_eight_digit_number(full_text)
    
    if not (has_sales_keyword or has_eight_digit):
        return False, "íŒë§¤ì› ê´€ë ¨ í‚¤ì›Œë“œ/ë²ˆí˜¸ ì—†ìŒ"
    
    # ê·œì¹™ 3: ì œì™¸ í‚¤ì›Œë“œ ì²´í¬ (ë‘˜ ì´ìƒ ìˆìœ¼ë©´ ì œì™¸)
    exclude_count = sum(1 for keyword in EXCLUDE_KEYWORDS if keyword in full_text)
    if exclude_count >= 2:
        return False, f"ì œì™¸ í‚¤ì›Œë“œ {exclude_count}ê°œ ë°œê²¬"
    
    return True, ""

# ===========================
# í¬ë¡¤ë§ í•¨ìˆ˜
# ===========================

def setup_driver() -> webdriver.Chrome:
    """Selenium ë“œë¼ì´ë²„ ì„¤ì •"""
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    chrome_options.add_argument(f'user-agent={user_agent}')
    
    # Homebrew chromedriver ìš°ì„  ì‚¬ìš© (macOS Gatekeeper ë¬¸ì œ í•´ê²°)
    homebrew_chromedriver = '/opt/homebrew/bin/chromedriver'
    
    if os.path.exists(homebrew_chromedriver):
        service = Service(homebrew_chromedriver)
        logger.info("âœ… Homebrew ChromeDriver ì‚¬ìš©")
    else:
        service = Service(ChromeDriverManager().install())
        logger.info("âœ… webdriver-manager ChromeDriver ì‚¬ìš©")
    
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    # ìë™í™” ê°ì§€ ìš°íšŒ
    driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
        'source': '''
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            })
        '''
    })
    
    return driver

def search_naver_blog(keyword: str, display: int = 100, start: int = 1) -> Optional[Dict]:
    """Naver Open Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡œê·¸ ê²€ìƒ‰"""
    url = "https://openapi.naver.com/v1/search/blog.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
        "User-Agent": random.choice(USER_AGENTS)
    }
    params = {
        "query": keyword,
        "display": display,
        "start": start,
        "sort": "sim"
    }
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {str(e)}")
        return None

def parse_search_results(search_data: Dict) -> List[Dict]:
    """API ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±"""
    results = []
    
    if not search_data or 'items' not in search_data:
        logger.debug("ê²€ìƒ‰ ë°ì´í„° ì—†ìŒ")
        return results
    
    total_items = len(search_data['items'])
    logger.debug(f"API ì‘ë‹µ: {total_items}ê°œ ì•„ì´í…œ")
    
    for idx, item in enumerate(search_data['items'], 1):
        try:
            url = item.get('link', '')
            logger.debug(f"[{idx}/{total_items}] URL: {url}")
            
            if not url:
                logger.debug("  â†’ URL ì—†ìŒ, ìŠ¤í‚µ")
                continue
                
            if 'blog.naver.com' not in url:
                logger.debug(f"  â†’ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì•„ë‹˜, ìŠ¤í‚µ")
                continue
            
            # blog_idì™€ post_id ì¶”ì¶œ
            blog_info = extract_blog_info_from_url(url)
            if not blog_info:
                logger.debug(f"  â†’ blog_info ì¶”ì¶œ ì‹¤íŒ¨, ìŠ¤í‚µ")
                continue
            
            logger.debug(f"  â†’ âœ… ì¶”ê°€: {blog_info['blog_id']}/{blog_info['post_id']}")
            
            results.append({
                'url': url,
                'blog_id': blog_info['blog_id'],
                'post_id': blog_info['post_id'],
                'postdate': item.get('postdate', ''),
                'bloggername': item.get('bloggername', '')
            })
        except Exception as e:
            logger.debug(f"  â†’ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            continue
    
    logger.debug(f"ìµœì¢… íŒŒì‹± ê²°ê³¼: {len(results)}ê°œ")
    return results

def crawl_blog_post_selenium(
    driver: webdriver.Chrome,
    url: str,
    blog_id: str,
    post_id: str,
    failed_url_manager: FailedURLManager
) -> Optional[Dict]:
    """Seleniumì„ ì‚¬ìš©í•œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ í¬ë¡¤ë§"""
    
    try:
        driver.get(url)
        time.sleep(2)
        
        # iframe ëŒ€ê¸° ë° ì „í™˜
        try:
            WebDriverWait(driver, IFRAME_WAIT_TIMEOUT).until(
                EC.frame_to_be_available_and_switch_to_it((By.ID, "mainFrame"))
            )
        except TimeoutException:
            logger.warning(f"âš ï¸  iframe ë¡œë“œ ì‹¤íŒ¨: {url}")
            failed_url_manager.add_failed_url(url, "iframe_timeout")
            return None
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        WebDriverWait(driver, SELENIUM_WAIT_TIMEOUT).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        # HTML íŒŒì‹±
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ
        title = ""
        title_selectors = [
            'div.se-title-text',
            'span.se-fs-',
            'div.se-component-content',
            'h3.se_textarea'
        ]
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                if title:
                    break
        
        if not title:
            title_elem = soup.find(['h1', 'h2', 'h3'])
            if title_elem:
                title = title_elem.get_text(strip=True)
        
        if not title:
            logger.warning(f"âš ï¸  ì œëª© ì—†ìŒ: {url}")
            failed_url_manager.add_failed_url(url, "no_title")
            driver.switch_to.default_content()
            return None
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        content = ""
        content_selectors = [
            'div.se-main-container',
            'div#postViewArea',
            'div.se-component-content',
            'div.post-view',
            'div.__se_component_area'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = content_elem.get_text(separator=' ', strip=True)
                if len(content) > 50:
                    break
        
        if not content or len(content) < 20:
            logger.warning(f"âš ï¸  ë³¸ë¬¸ ì—†ìŒ ë˜ëŠ” ë„ˆë¬´ ì§§ìŒ: {url}")
            failed_url_manager.add_failed_url(url, "no_content")
            driver.switch_to.default_content()
            return None
        
        # ì „ì²´ í…ìŠ¤íŠ¸ (í•„í„°ë§ìš©)
        full_text = f"{title} {content}"
        
        # ì½˜í…ì¸  í•„í„°ë§
        passes, reason = content_passes_filter(title, content, full_text)
        if not passes:
            logger.debug(f"ğŸš« í•„í„°ë§: {title[:30]}... ({reason})")
            driver.switch_to.default_content()
            return None
        
        # ë‚ ì§œ ì¶”ì¶œ
        published_date = ""
        date_selectors = [
            'span.se_publishDate',
            'span.date',
            'p.date',
            'div.post_date',
            'span.p_date'
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                published_date = parse_korean_date(date_text)
                if published_date:
                    break
        
        # ì¶”ì²œì¸ ì •ë³´ ì¶”ì¶œ
        sponsor_phone = extract_sponsor_phone(full_text)
        sponsor_partner_id = extract_sponsor_partner_id(full_text)
        
        # ì¢‹ì•„ìš”/ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ (v6.6 ë¡œì§ ì‚¬ìš©)
        like_count = extract_like_count(driver, soup)
        comment_count = extract_comment_count(driver, soup)
        
        # í•´ì‹œíƒœê·¸ ì¶”ì¶œ
        hashtags = extract_hashtags(soup, content)
        
        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
        image_urls = []
        img_tags = soup.select('img.se-image-resource, img.__se_img_el, div.se-component-content img')
        for img in img_tags[:10]:  # ìµœëŒ€ 10ê°œ
            src = img.get('src') or img.get('data-src')
            if src and src.startswith('http'):
                image_urls.append(src)
        
        # ë¹„ë””ì˜¤ URL ì¶”ì¶œ
        video_urls = []
        video_tags = soup.select('video source, iframe[src*="youtube"], iframe[src*="youtu.be"]')
        for video in video_tags[:5]:  # ìµœëŒ€ 5ê°œ
            src = video.get('src')
            if src and src.startswith('http'):
                video_urls.append(src)
        
        # iframeì—ì„œ ë‚˜ê°€ê¸°
        driver.switch_to.default_content()
        
        # ë°ì´í„° êµ¬ì„± (v7.1: post_id í˜•ì‹ ë³€ê²½, platform ì¶”ê°€)
        post_data = {
            'platform': 'naver_blog',
            'post_id': f"{blog_id}_{post_id}",  # v7.1: ë³µí•© ID
            'blog_id': blog_id,
            'url': url,
            'title': title,
            'content': content,
            'published_date': published_date,
            'sponsor_phone': sponsor_phone,
            'sponsor_partner_id': sponsor_partner_id,
            'like_count': like_count,
            'comment_count': comment_count,
            'hashtags': hashtags,
            'image_urls': ', '.join(image_urls) if image_urls else "",
            'video_urls': ', '.join(video_urls) if video_urls else "",
            'collected_date': datetime.now().strftime('%Y-%m-%d')
        }
        
        return post_data
    
    except TimeoutException:
        logger.warning(f"âš ï¸  íƒ€ì„ì•„ì›ƒ: {url}")
        failed_url_manager.add_failed_url(url, "timeout")
        try:
            driver.switch_to.default_content()
        except:
            pass
        return None
    
    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {url} - {str(e)}")
        failed_url_manager.add_failed_url(url, "crawl_error", str(e))
        try:
            driver.switch_to.default_content()
        except:
            pass
        return None

# ===========================
# ë©”ì¸ í•¨ìˆ˜
# ===========================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("="*70)
    logger.info("ğŸš€ PM International ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v7.1 ì‹œì‘")
    logger.info("="*70)
    
    # ì´ˆê¸°í™”
    driver = setup_driver()
    stats = CrawlStats()
    failed_url_manager = FailedURLManager()
    
    collected_posts = []
    collected_urls: Set[str] = set()
    collected_fingerprints: Set[str] = set()
    
    try:
        for keyword in SEARCH_KEYWORDS:
            if len(collected_posts) >= TOTAL_TARGET:
                logger.info(f"âœ… ëª©í‘œ ë‹¬ì„± ({TOTAL_TARGET}ê°œ)")
                break
            
            logger.info(f"\nğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰: '{keyword}'")
            
            # ê²€ìƒ‰
            search_data = search_naver_blog(keyword, MAX_SEARCH_RESULTS)
            if not search_data:
                logger.warning(f"âš ï¸  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {keyword}")
                continue
            
            # ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
            search_results = parse_search_results(search_data)
            logger.info(f"ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ")
            
            # í¬ë¡¤ë§
            for result in search_results:
                if len(collected_posts) >= TOTAL_TARGET:
                    break
                
                blog_id = result['blog_id']
                post_id = result['post_id']
                normalized_url = normalize_blog_url(blog_id, post_id)
                
                # ì¤‘ë³µ ì²´í¬
                if normalized_url in collected_urls:
                    stats.add_duplicate()
                    continue
                
                logger.info(f"[{len(collected_posts)+1}/{TOTAL_TARGET}] í¬ë¡¤ë§ ì¤‘...")
                
                # í¬ë¡¤ë§ ì‹¤í–‰
                post_data = crawl_blog_post_selenium(
                    driver, normalized_url, blog_id, post_id, failed_url_manager
                )
                
                if post_data:
                    # ì¤‘ë³µ ì²´í¬ (ì§€ë¬¸ ê¸°ë°˜)
                    fingerprint = generate_post_fingerprint(post_data)
                    if fingerprint not in collected_fingerprints:
                        collected_posts.append(post_data)
                        collected_urls.add(normalized_url)
                        collected_fingerprints.add(fingerprint)
                        stats.add_success()
                        logger.info(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {post_data['title'][:50]}...")
                    else:
                        stats.add_duplicate()
                        logger.debug(f"ğŸ”„ ì¤‘ë³µ (ì§€ë¬¸): {post_data['title'][:30]}...")
                else:
                    stats.add_filtered()
                
                # ìš”ì²­ ê°„ ì§€ì—°
                time.sleep(random.uniform(REQUEST_DELAY_MIN, REQUEST_DELAY_MAX))
            
            # í‚¤ì›Œë“œ ê°„ ëŒ€ê¸°
            if len(collected_posts) < TOTAL_TARGET:
                time.sleep(random.uniform(2, 4))
        
        # í†µê³„ ì¶œë ¥
        stats.print_stats()
        
        # CSVë¡œ ì €ì¥
        if collected_posts:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'naver_blog_pm_v7_1_{timestamp}.csv'
            
            df = pd.DataFrame(collected_posts)
            
            # ì»¬ëŸ¼ ìˆœì„œ ì •ì˜
            column_order = [
                'platform', 'post_id', 'blog_id', 'url', 'title', 'content',
                'published_date', 'sponsor_phone', 'sponsor_partner_id',
                'like_count', 'comment_count', 'hashtags', 'image_urls',
                'video_urls', 'collected_date'
            ]
            
            df = df[column_order]
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            logger.info(f"\n{'='*70}")
            logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
            logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(collected_posts)}ê°œ")
            logger.info(f"ğŸ“‹ ì»¬ëŸ¼: {len(column_order)}ê°œ")
            logger.info(f"{'='*70}")
        else:
            logger.warning("âš ï¸  ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì‹¤íŒ¨ URL ì €ì¥
        failed_url_manager.save_to_file()
        if failed_url_manager.get_failed_count() > 0:
            logger.info(f"âŒ ì‹¤íŒ¨ URL: {failed_url_manager.get_failed_count()}ê°œ (failed_urls.json)")
    
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  ì‚¬ìš©ì ì¤‘ë‹¨")
        stats.print_stats()
    
    except Exception as e:
        logger.error(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        stats.print_stats()
    
    finally:
        driver.quit()
        logger.info("âœ… ë“œë¼ì´ë²„ ì¢…ë£Œ")
        logger.info("="*70)
        logger.info("ğŸ í¬ë¡¤ë§ ì™„ë£Œ")
        logger.info("="*70)

if __name__ == "__main__":
    main()
