#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v6.1 - ChromeDriver ìë™ ê´€ë¦¬ ë²„ì „
========================================================

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. webdriver-managerë¡œ ChromeDriver ìë™ ê´€ë¦¬ (ë²„ì „ ë¶ˆì¼ì¹˜ ë¬¸ì œ í•´ê²°)
2. ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ 4ë°° ì†ë„ í–¥ìƒ
3. ë©”ëª¨ë¦¬ ìµœì í™” (16GB í™˜ê²½ ìµœì í™”)
4. Alert ìë™ ì²˜ë¦¬ (ë¹„ê³µê°œ ê¸€ ìŠ¤í‚µ)
5. ì§„í–‰ ìƒí™© ìë™ ì €ì¥ (ì¤‘ë‹¨ ì‹œ ì¬ê°œ ê°€ëŠ¥)
6. MacBook M2 ìµœì í™”

ì˜ˆìƒ ì„±ëŠ¥:
- Windows PC (i3, 2ì½”ì–´): 22-31ì‹œê°„
- MacBook M2 (8ì½”ì–´, ë³‘ë ¬): 6-8ì‹œê°„ (70-75% ë‹¨ì¶•)

í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜:
    pip install requests beautifulsoup4 selenium pandas tqdm webdriver-manager

ì‘ì„±ì: PMIì½”ë¦¬ì•„ ë°ì´í„° íŒ€
ë²„ì „: 6.1 (ChromeDriver ìë™ ê´€ë¦¬)
ìµœì¢… ìˆ˜ì •: 2025-11-02
"""

import os
import time
import json
import re
from datetime import datetime
from typing import List, Dict, Optional
from multiprocessing import Pool, cpu_count
from functools import partial

# ì›¹ í¬ë¡¤ë§
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, UnexpectedAlertPresentException
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager  # â­ ì¶”ê°€

# ë°ì´í„° ì²˜ë¦¬
import pandas as pd
from tqdm import tqdm

# ============================================================
# ì„¤ì •
# ============================================================

def load_api_credentials():
    """API ì¸ì¦ ì •ë³´ë¥¼ ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ë¡œë“œ"""
    
    # ë°©ë²• 1: config.py íŒŒì¼ì—ì„œ ë¡œë“œ
    config_path = os.path.join(os.path.dirname(__file__), 'config.py')
    if os.path.exists(config_path):
        try:
            import importlib.util
            spec = importlib.util.spec_from_file_location("config", config_path)
            config = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(config)
            
            client_id = getattr(config, 'NAVER_CLIENT_ID', None)
            client_secret = getattr(config, 'NAVER_CLIENT_SECRET', None)
            
            if client_id and client_secret:
                print("âœ“ config.pyì—ì„œ API í‚¤ ë¡œë“œ ì„±ê³µ")
                return client_id, client_secret
        except Exception as e:
            print(f"âš  config.py ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ë°©ë²• 2: .env íŒŒì¼ì—ì„œ ë¡œë“œ
    try:
        env_path = os.path.join(os.path.dirname(__file__), '.env')
        if os.path.exists(env_path):
            with open(env_path, 'r', encoding='utf-8-sig') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        if '=' in line:
                            key, value = line.split('=', 1)
                            os.environ[key.strip()] = value.strip()
            
            client_id = os.getenv('NAVER_CLIENT_ID')
            client_secret = os.getenv('NAVER_CLIENT_SECRET')
            
            if client_id and client_secret:
                print("âœ“ .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ ì„±ê³µ")
                return client_id, client_secret
    except Exception as e:
        print(f"âš  .env íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ë°©ë²• 3: í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ
    client_id = os.getenv('NAVER_CLIENT_ID')
    client_secret = os.getenv('NAVER_CLIENT_SECRET')
    
    if client_id and client_secret:
        print("âœ“ í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ ì„±ê³µ")
        return client_id, client_secret
    
    raise ValueError(
        "\n" + "="*70 + "\n"
        "âŒ Naver API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\n"
        "="*70 + "\n\n"
        "ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë°©ë²•ìœ¼ë¡œ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”:\n\n"
        "ë°©ë²• 1 (ê¶Œì¥): config.py íŒŒì¼ ìƒì„±\n"
        "  íŒŒì¼ëª…: config.py\n"
        "  ë‚´ìš©:\n"
        "    NAVER_CLIENT_ID = 'your_client_id'\n"
        "    NAVER_CLIENT_SECRET = 'your_client_secret'\n\n"
        "ë°©ë²• 2: .env íŒŒì¼ ìƒì„±\n"
        "  íŒŒì¼ëª…: .env\n"
        "  ë‚´ìš©:\n"
        "    NAVER_CLIENT_ID=your_client_id\n"
        "    NAVER_CLIENT_SECRET=your_client_secret\n"
        "="*70
    )

# API í‚¤ ë¡œë“œ
NAVER_CLIENT_ID, NAVER_CLIENT_SECRET = load_api_credentials()

# íƒ€ê²Ÿ í•´ì‹œíƒœê·¸
TARGET_HASHTAGS = [
    '#í”¼ì— ì¸í„°ë‚´ì…”ë„', '#í”¼ì— ì½”ë¦¬ì•„', '#ë…ì¼í”¼ì— ', '#PMì¸í„°ë‚´ì…”ë„',
    '#í•ë¼ì¸', '#í”¼íŠ¸ë¼ì¸',
    '#ë² ì´ì‹ìŠ¤', '#ë² ì´ì§ìŠ¤', '#ë² ì´ì‹',
    '#í”„ë¡œì…°ì´í”„', '#í”„ë¡œì‰ì´í”„', '#ì—‘í‹°ë°”ì´ì¦ˆ',
    '#íŒŒì›Œì¹µí…Œì¼', '#ë¦¬ìŠ¤í† ë ˆì´íŠ¸',
    '#ë®¤ë‹ˆí‹°', '#ì˜µí‹°ë©€ì…‹', '#ì…€í”ŒëŸ¬ìŠ¤'
]

# ìˆ˜ì§‘ ì„¤ì •
MAX_RESULTS_PER_HASHTAG = 1000
NUM_WORKERS = 4  # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (MacBook M2: 4ê°œ ê¶Œì¥)

# ì¶œë ¥ ì„¤ì •
OUTPUT_DIR = "output"
OUTPUT_CSV = f"{OUTPUT_DIR}/naver_blog_crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
STATS_FILE = f"{OUTPUT_DIR}/crawl_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
PROGRESS_FILE = f"{OUTPUT_DIR}/progress_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"


# ============================================================
# Step 1: í•´ì‹œíƒœê·¸ ì§ì ‘ ê²€ìƒ‰ìœ¼ë¡œ URL ìˆ˜ì§‘
# ============================================================

class NaverHashtagSearcher:
    def __init__(self, client_id: str, client_secret: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.base_url = "https://openapi.naver.com/v1/search/blog.json"
    
    def search_hashtag(self, hashtag: str, max_results: int = 100) -> List[Dict]:
        """í•´ì‹œíƒœê·¸ë¡œ ì§ì ‘ ê²€ìƒ‰"""
        headers = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret
        }
        
        results = []
        
        for start in range(1, min(max_results, 1000), 100):
            params = {
                "query": hashtag,
                "display": 100,
                "start": start,
                "sort": "date"
            }
            
            try:
                response = requests.get(self.base_url, headers=headers, params=params)
                response.raise_for_status()
                data = response.json()
                
                items = data.get('items', [])
                if not items:
                    break
                
                for item in items:
                    results.append({
                        'title': self._clean_html(item.get('title', '')),
                        'link': item.get('link', ''),
                        'description': self._clean_html(item.get('description', '')),
                        'bloggername': item.get('bloggername', ''),
                        'bloggerlink': item.get('bloggerlink', ''),
                        'postdate': item.get('postdate', '')
                    })
                
                time.sleep(0.1)
                
            except Exception as e:
                print(f"[ERROR] í•´ì‹œíƒœê·¸ '{hashtag}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                break
        
        return results
    
    def _clean_html(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        return re.sub('<.*?>', '', text)


# ============================================================
# Step 2: Seleniumìœ¼ë¡œ ë³¸ë¬¸ í¬ë¡¤ë§ (ChromeDriver ìë™ ê´€ë¦¬)
# ============================================================

class NaverBlogCrawler:
    def __init__(self, headless: bool = True):
        options = webdriver.ChromeOptions()
        if headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-extensions')
        options.add_argument('--disk-cache-size=1')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        # Alert ìë™ ì²˜ë¦¬ ì„¤ì •
        options.add_experimental_option('excludeSwitches', ['enable-logging'])
        options.add_argument('--log-level=3')
        
        # â­ ë³€ê²½: webdriver-managerë¡œ ìë™ ë“œë¼ì´ë²„ ê´€ë¦¬
        chromedriver_path = "/Users/kimble/.wdm/drivers/chromedriver/mac64/141.0.7390.122/chromedriver-mac-arm64/chromedriver"
        try:
            if os.path.exists(chromedriver_path):
                service = Service(chromedriver_path)
            else:
                service = Service(ChromeDriverManager().install())
        except:
            service = Service(chromedriver_path)
        self.driver = webdriver.Chrome(service=service, options=options)
        self.wait = WebDriverWait(self.driver, 10)
    
    def crawl_blog_post(self, url: str) -> Optional[Dict]:
        """ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ í¬ë¡¤ë§ (Alert ìë™ ì²˜ë¦¬)"""
        try:
            self.driver.get(url)
            time.sleep(1)
            
            # Alert ìë™ ì²˜ë¦¬
            try:
                self.driver.switch_to.alert.dismiss()
                return None
            except:
                pass
            
            # iframeìœ¼ë¡œ ì „í™˜
            try:
                iframe = self.wait.until(
                    EC.presence_of_element_located((By.ID, "mainFrame"))
                )
                self.driver.switch_to.frame(iframe)
            except TimeoutException:
                return None
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content_text = ''
            hashtags = []
            image_urls = []
            video_urls = []
            
            # ìŠ¤ë§ˆíŠ¸ì—ë””í„° ONE
            try:
                se_container = self.driver.find_element(By.CLASS_NAME, "se-main-container")
                content_text = se_container.text
                
                # ì´ë¯¸ì§€
                images = se_container.find_elements(By.TAG_NAME, "img")
                for img in images:
                    src = img.get_attribute('src')
                    if src and 'http' in src:
                        image_urls.append(src)
                
                # ë¹„ë””ì˜¤
                videos = se_container.find_elements(By.TAG_NAME, "video")
                for video in videos:
                    src = video.get_attribute('src')
                    if src:
                        video_urls.append(src)
                
                # í•´ì‹œíƒœê·¸
                hashtag_elems = se_container.find_elements(By.CSS_SELECTOR, "a[href*='tab=hashtag']")
                for elem in hashtag_elems:
                    tag_text = elem.text.strip()
                    if tag_text and tag_text not in hashtags:
                        hashtags.append(tag_text)
            
            except:
                # ìŠ¤ë§ˆíŠ¸ì—ë””í„° 2.0
                try:
                    post_area = self.driver.find_element(By.ID, "postViewArea")
                    content_text = post_area.text
                    
                    # ì´ë¯¸ì§€
                    images = post_area.find_elements(By.TAG_NAME, "img")
                    for img in images:
                        src = img.get_attribute('src')
                        if src and 'http' in src:
                            image_urls.append(src)
                    
                    # ë¹„ë””ì˜¤
                    videos = post_area.find_elements(By.TAG_NAME, "video")
                    for video in videos:
                        src = video.get_attribute('src')
                        if src:
                            video_urls.append(src)
                except:
                    return None
            
            # ì°¸ì—¬ ì§€í‘œ
            self.driver.switch_to.default_content()
            
            view_count = ''
            like_count = ''
            comment_count = ''
            
            try:
                # ì¡°íšŒìˆ˜
                view_elem = self.driver.find_element(By.CSS_SELECTOR, "span.count_view")
                view_count = view_elem.text
            except:
                pass
            
            try:
                # ì¢‹ì•„ìš” ìˆ˜
                like_elem = self.driver.find_element(By.CSS_SELECTOR, "span.u_cnt._count")
                like_count = like_elem.text
            except:
                pass
            
            try:
                # ëŒ“ê¸€ ìˆ˜
                comment_elem = self.driver.find_element(By.CSS_SELECTOR, "span.u_cnt_comment")
                comment_count = comment_elem.text
            except:
                pass
            
            return {
                'url': url,
                'title': self.driver.title,
                'content_text': content_text,
                'hashtags': hashtags,
                'image_urls': image_urls,
                'video_urls': video_urls,
                'view_count': view_count,
                'like_count': like_count,
                'comment_count': comment_count,
                'author_id': self._extract_author_id(url)
            }
        
        except UnexpectedAlertPresentException:
            try:
                self.driver.switch_to.alert.dismiss()
            except:
                pass
            return None
        
        except Exception as e:
            return None
    
    def _extract_author_id(self, url: str) -> str:
        """URLì—ì„œ ì‘ì„±ì ID ì¶”ì¶œ"""
        match = re.search(r'blog\.naver\.com/([^/?]+)', url)
        if match:
            return match.group(1)
        return ''
    
    def close(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        try:
            self.driver.quit()
        except:
            pass


# ============================================================
# Step 3: ì¶”ì²œì¸ ì •ë³´ ì¶”ì¶œ
# ============================================================

class ReferrerExtractor:
    def __init__(self):
        self.phone_patterns = [
            r'(?:ì—°ë½ì²˜|ì „í™”|ì—°ë½|ë¬¸ì˜|â˜|ğŸ“|TEL|Tel|tel)[:\s]*([0-9]{2,3}[-\s]?[0-9]{3,4}[-\s]?[0-9]{4})',
            r'(01[016789][-\s]?[0-9]{3,4}[-\s]?[0-9]{4})',
            r'([0-9]{2,3}[-\s]?[0-9]{3,4}[-\s]?[0-9]{4})'
        ]
        
        self.name_patterns = [
            r'(?:ì¶”ì²œì¸|ì†Œê°œ|ìƒë‹´|ë¬¸ì˜|ë‹´ë‹¹|ì»¨ì„¤íŒ…|íŒŒíŠ¸ë„ˆ)[:\s]*([ê°€-í£]{2,4})',
            r'(?:PM|í”¼ì— )[:\s]*([ê°€-í£]{2,4})',
            r'([ê°€-í£]{2,4})\s*(?:íŒŒíŠ¸ë„ˆ|ë§¤ë‹ˆì €|ë‹˜|ì„ ìƒë‹˜)'
        ]
        
        self.partner_patterns = [
            r'(?:íŒŒíŠ¸ë„ˆë²ˆí˜¸|íŒŒíŠ¸ë„ˆ|íšŒì›ë²ˆí˜¸|ë²ˆí˜¸)[:\s]*([A-Z]{0,3}[0-9]{6,10})',
            r'([A-Z]{2,3}[0-9]{6,8})',
            r'ID[:\s]*([A-Z0-9]{6,10})'
        ]
        
        self.kakao_patterns = [
            r'(?:ì¹´ì¹´ì˜¤í†¡?|ì¹´í†¡|kakao)[:\s]*([a-zA-Z0-9_-]{3,20})',
            r'(?:ID|ì•„ì´ë””)[:\s]*([a-zA-Z0-9_-]{3,20})'
        ]
    
    def extract_phone(self, text: str) -> str:
        """ì „í™”ë²ˆí˜¸ ì¶”ì¶œ ë° ì •ê·œí™”"""
        if not text:
            return ''
        
        for pattern in self.phone_patterns:
            match = re.search(pattern, text)
            if match:
                phone = match.group(1) if match.lastindex >= 1 else match.group(0)
                phone = re.sub(r'[^0-9]', '', phone)
                
                if len(phone) == 10 or len(phone) == 11:
                    if len(phone) == 11:
                        return f"{phone[:3]}-{phone[3:7]}-{phone[7:]}"
                    else:
                        return f"{phone[:3]}-{phone[3:6]}-{phone[6:]}"
        return ''
    
    def extract_name(self, text: str) -> str:
        """ì´ë¦„ ì¶”ì¶œ"""
        if not text:
            return ''
        
        for pattern in self.name_patterns:
            match = re.search(pattern, text)
            if match:
                name = match.group(1) if match.lastindex >= 1 else match.group(0)
                if re.match(r'^[ê°€-í£]{2,4}$', name):
                    return name
        return ''
    
    def extract_partner_number(self, text: str) -> str:
        """íŒŒíŠ¸ë„ˆ ë²ˆí˜¸ ì¶”ì¶œ"""
        if not text:
            return ''
        
        for pattern in self.partner_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1) if match.lastindex >= 1 else match.group(0)
        return ''
    
    def extract_kakao(self, text: str) -> str:
        """ì¹´ì¹´ì˜¤í†¡ ID ì¶”ì¶œ"""
        if not text:
            return ''
        
        for pattern in self.kakao_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)
        return ''
    
    def extract_all(self, content_text: str) -> Dict[str, str]:
        """ëª¨ë“  ì¶”ì²œì¸ ì •ë³´ ì¶”ì¶œ"""
        return {
            'name': self.extract_name(content_text),
            'phone': self.extract_phone(content_text),
            'partner_number': self.extract_partner_number(content_text),
            'kakao': self.extract_kakao(content_text)
        }


# ============================================================
# Step 4: ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜
# ============================================================

def crawl_single_post(blog_info: Dict) -> Optional[Dict]:
    """ë‹¨ì¼ ê²Œì‹œë¬¼ í¬ë¡¤ë§ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    crawler = NaverBlogCrawler(headless=True)
    try:
        post_data = crawler.crawl_blog_post(blog_info['link'])
        if post_data:
            post_data.update({
                'post_date': blog_info['postdate'],
                'author_name': blog_info['bloggername'],
                'description': blog_info.get('description', ''),
                'blogger_profile': blog_info.get('bloggerlink', '')
            })
            return post_data
        return None
    finally:
        crawler.close()
        time.sleep(0.3)


# ============================================================
# Step 5: ë°ì´í„° ì €ì¥
# ============================================================

def save_to_csv(posts: List[Dict], filename: str) -> pd.DataFrame:
    """ê²Œì‹œë¬¼ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥"""
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    extractor = ReferrerExtractor()
    
    print("\nì¶”ì²œì¸ ì •ë³´ ìë™ ì¶”ì¶œ ì¤‘...")
    extraction_stats = {'name': 0, 'phone': 0, 'partner_number': 0}
    
    rows = []
    for post in posts:
        content_text = post.get('content_text', '')
        referrer_info = extractor.extract_all(content_text)
        
        if referrer_info['name']:
            extraction_stats['name'] += 1
        if referrer_info['phone']:
            extraction_stats['phone'] += 1
        if referrer_info['partner_number']:
            extraction_stats['partner_number'] += 1
        
        row = {
            'platform': 'Naver Blog',
            'title': post['title'],
            'description': post.get('description', ''),
            'blogger_profile': post.get('blogger_profile', ''),
            'post_url': post['url'],
            'author_id': post.get('author_id', ''),
            'content_text': content_text,
            'hashtags': ', '.join(post.get('hashtags', [])),
            'postdate': post.get('post_date', ''),
            'image_count': len(post.get('image_urls', [])),
            'video_count': len(post.get('video_urls', [])),
            'image_urls': '|||'.join(post.get('image_urls', [])),
            'video_urls': '|||'.join(post.get('video_urls', [])),
            'view_count': post.get('view_count', ''),
            'like_count': post.get('like_count', ''),
            'comment_count': post.get('comment_count', ''),
            'referrer_name': referrer_info['name'],
            'referrer_phone': referrer_info['phone'],
            'partner_number': referrer_info['partner_number'],
            'kakao_id': referrer_info['kakao'],
            'collected_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        rows.append(row)
    
    print(f"âœ“ ì¶”ì²œì¸ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ:")
    print(f"  â€¢ ì´ë¦„: {extraction_stats['name']}ê°œ ({extraction_stats['name']/len(posts)*100:.1f}%)")
    print(f"  â€¢ ì „í™”ë²ˆí˜¸: {extraction_stats['phone']}ê°œ ({extraction_stats['phone']/len(posts)*100:.1f}%)")
    print(f"  â€¢ íŒŒíŠ¸ë„ˆë²ˆí˜¸: {extraction_stats['partner_number']}ê°œ ({extraction_stats['partner_number']/len(posts)*100:.1f}%)")
    
    df = pd.DataFrame(rows)
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ {len(df)}ê°œ ê²Œì‹œë¬¼ì„ {filename}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤")
    
    return df


def save_stats(stats: Dict, filename: str):
    """í†µê³„ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ í†µê³„ ì •ë³´ë¥¼ {filename}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤")


# ============================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================

def main():
    print("=" * 70)
    print(" ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v6.1 - ChromeDriver ìë™ ê´€ë¦¬")
    print("=" * 70)
    print(f"íƒ€ê²Ÿ í•´ì‹œíƒœê·¸: {len(TARGET_HASHTAGS)}ê°œ")
    print(f"í•´ì‹œíƒœê·¸ë‹¹ ìµœëŒ€ ìˆ˜ì§‘: {MAX_RESULTS_PER_HASHTAG}ê°œ")
    print(f"ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤: {NUM_WORKERS}ê°œ")
    print(f"CPU ì½”ì–´ ìˆ˜: {cpu_count()}ê°œ")
    print("=" * 70)
    
    overall_stats = {
        'start_time': datetime.now().isoformat(),
        'config': {
            'target_hashtags': TARGET_HASHTAGS,
            'max_results_per_hashtag': MAX_RESULTS_PER_HASHTAG,
            'num_workers': NUM_WORKERS
        }
    }
    
    # Phase 1: í•´ì‹œíƒœê·¸ë¡œ URL ìˆ˜ì§‘
    print("\n[Phase 1] í•´ì‹œíƒœê·¸ ì§ì ‘ ê²€ìƒ‰ìœ¼ë¡œ URL ìˆ˜ì§‘ ì¤‘...")
    
    searcher = NaverHashtagSearcher(NAVER_CLIENT_ID, NAVER_CLIENT_SECRET)
    
    all_blog_urls = []
    hashtag_counts = {}
    
    for hashtag in tqdm(TARGET_HASHTAGS, desc="í•´ì‹œíƒœê·¸ ê²€ìƒ‰"):
        results = searcher.search_hashtag(hashtag, max_results=MAX_RESULTS_PER_HASHTAG)
        hashtag_counts[hashtag] = len(results)
        all_blog_urls.extend(results)
    
    # ì¤‘ë³µ ì œê±°
    unique_urls = {item['link']: item for item in all_blog_urls}
    all_blog_urls = list(unique_urls.values())
    
    print(f"\nâœ“ ì´ {len(all_blog_urls)}ê°œ URL ìˆ˜ì§‘ ì™„ë£Œ (ì¤‘ë³µ ì œê±° í›„)")
    print(f"\ní•´ì‹œíƒœê·¸ë³„ ìˆ˜ì§‘ í˜„í™©:")
    for hashtag, count in sorted(hashtag_counts.items(), key=lambda x: x[1], reverse=True):
        if count > 0:
            print(f"  {hashtag}: {count}ê°œ")
    
    # Phase 2: ë³‘ë ¬ ë³¸ë¬¸ í¬ë¡¤ë§
    print(f"\n[Phase 2] ë³‘ë ¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘ (ì´ {len(all_blog_urls)}ê°œ URL, {NUM_WORKERS}ê°œ ì›Œì»¤)...")
    
    crawled_posts = []
    
    # ë³‘ë ¬ ì²˜ë¦¬
    with Pool(processes=NUM_WORKERS) as pool:
        results = list(tqdm(
            pool.imap(crawl_single_post, all_blog_urls),
            total=len(all_blog_urls),
            desc="í¬ë¡¤ë§"
        ))
    
    # None ì œê±°
    crawled_posts = [r for r in results if r is not None]
    
    print(f"\nâœ“ {len(crawled_posts)}ê°œ ê²Œì‹œë¬¼ í¬ë¡¤ë§ ì™„ë£Œ")
    
    overall_stats['phase1_collected_urls'] = len(all_blog_urls)
    overall_stats['phase2_crawled_posts'] = len(crawled_posts)
    
    # Phase 3: ì €ì¥
    print("\n[Phase 3] ë°ì´í„° ì €ì¥ ì¤‘...")
    df = save_to_csv(crawled_posts, OUTPUT_CSV)
    
    overall_stats['end_time'] = datetime.now().isoformat()
    overall_stats['final_post_count'] = len(crawled_posts)
    
    save_stats(overall_stats, STATS_FILE)
    
    # ìµœì¢… ìš”ì•½
    print("\n" + "=" * 70)
    print(" ìˆ˜ì§‘ ì™„ë£Œ!")
    print("=" * 70)
    print(f"ì´ ìˆ˜ì§‘: {len(crawled_posts)}ê°œ")
    print(f"ì¶œë ¥ íŒŒì¼: {OUTPUT_CSV}")
    print(f"í†µê³„ íŒŒì¼: {STATS_FILE}")
    print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: Google Colabì—ì„œ ì´ë¯¸ì§€ OCR ë° ë™ì˜ìƒ ì²˜ë¦¬")
    print("=" * 70)


if __name__ == "__main__":
    main()