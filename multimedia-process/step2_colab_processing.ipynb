{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMI Korea SNS ì½˜í…ì¸  OCR ë° ìŒì„±ì¸ì‹ ì²˜ë¦¬ ë…¸íŠ¸ë¶\n",
    "\n",
    "## ğŸš€ ì‹¤í–‰ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸:\n",
    "1. ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > **GPU (T4 ê¶Œì¥)**\n",
    "2. `extracted_image_urls.csv` ì¤€ë¹„\n",
    "3. `extracted_video_urls.csv` ì¤€ë¹„\n",
    "4. ê° ì…€ì„ **ìˆœì„œëŒ€ë¡œ** ì‹¤í–‰ (Shift+Enter)\n",
    "\n",
    "## â±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„:\n",
    "- ì´ë¯¸ì§€ OCR: ~40-50ë¶„ (1,875ê°œ)\n",
    "- ë¹„ë””ì˜¤ í”„ë ˆì„ OCR: ~8-12ë¶„ (61ê°œ)\n",
    "- ìœ íŠœë¸Œ ìë§‰: ~3-5ë¶„ (104ê°œ)\n",
    "- Whisper ìŒì„±ì¸ì‹: ~25-35ë¶„\n",
    "\n",
    "**ì´ ì˜ˆìƒ: ì•½ 1.5ì‹œê°„**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ì…€ 1] GPU í™•ì¸ ë° ê¸°ë³¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# GPU í™•ì¸\n",
    "!nvidia-smi\n",
    "\n",
    "# ì‘ì—… ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "!mkdir -p /content/pmi_data\n",
    "!mkdir -p /content/pmi_data/results\n",
    "!mkdir -p /content/pmi_data/temp_videos\n",
    "\n",
    "print(\"\\nâœ… GPU í™•ì¸ ì™„ë£Œ!\")\n",
    "print(\"ğŸ“ ì‘ì—… ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ì…€ 2] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install -q easyocr\n",
    "\n",
    "# ë¹„ë””ì˜¤ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q yt-dlp\n",
    "\n",
    "# ìŒì„±ì¸ì‹ (Whisper)\n",
    "!pip install -q openai-whisper\n",
    "\n",
    "# ìœ íŠœë¸Œ ìë§‰ API\n",
    "!pip install -q youtube-transcript-api\n",
    "\n",
    "# ê¸°íƒ€ ìœ í‹¸ë¦¬í‹°\n",
    "!pip install -q tqdm pandas pillow requests\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ì…€ 3] íŒŒì¼ ì—…ë¡œë“œ (CSV íŒŒì¼ë“¤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import pandas as pd\n",
    "\n",
    "print(\"ğŸ“¤ extracted_image_urls.csv íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# íŒŒì¼ ì´ë™\n",
    "for filename in uploaded.keys():\n",
    "    !mv {filename} /content/pmi_data/\n",
    "\n",
    "print(\"\\nğŸ“¤ extracted_video_urls.csv íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    !mv {filename} /content/pmi_data/\n",
    "\n",
    "# íŒŒì¼ í™•ì¸\n",
    "image_df = pd.read_csv('/content/pmi_data/extracted_image_urls.csv')\n",
    "video_df = pd.read_csv('/content/pmi_data/extracted_video_urls.csv')\n",
    "\n",
    "print(f\"\\nâœ… ì—…ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(f\"   - ì´ë¯¸ì§€: {len(image_df)}ê°œ\")\n",
    "print(f\"   - ë¹„ë””ì˜¤: {len(video_df)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ì…€ 4] OCR ë° ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# EasyOCR ë¦¬ë” ì´ˆê¸°í™” (GPU ì‚¬ìš©, í•œêµ­ì–´+ì˜ì–´)\n",
    "print(\"ğŸ”§ EasyOCR ì´ˆê¸°í™” ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ 1-2ë¶„ ì†Œìš”)\")\n",
    "reader = easyocr.Reader(['ko', 'en'], gpu=True)\n",
    "print(\"âœ… EasyOCR ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "\n",
    "def download_image(url, timeout=10):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "            'Referer': 'https://blog.naver.com/'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        return np.array(image)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def perform_ocr(image_array):\n",
    "    try:\n",
    "        results = reader.readtext(image_array)\n",
    "        texts = [result[1] for result in results]\n",
    "        combined_text = ' '.join(texts)\n",
    "        if results:\n",
    "            avg_confidence = sum([result[2] for result in results]) / len(results)\n",
    "        else:\n",
    "            avg_confidence = 0.0\n",
    "        return combined_text, avg_confidence\n",
    "    except Exception as e:\n",
    "        return \"\", 0.0\n",
    "\n",
    "def extract_first_frame(video_path):\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        for _ in range(5):\n",
    "            cap.read()\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        if ret:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            return frame_rgb\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "print(\"âœ… í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ì…€ 5] ì´ë¯¸ì§€ OCR ì¼ê´„ ì²˜ë¦¬\n",
    "**ì˜ˆìƒ ì‹œê°„: 40-50ë¶„** â˜•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“¸ ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì‹œì‘\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "image_results = []\n",
    "\n",
    "for idx, row in tqdm(image_df.iterrows(), total=len(image_df), desc=\"ì´ë¯¸ì§€ OCR\"):\n",
    "    post_id = row['post_id']\n",
    "    url = row['url']\n",
    "    image = download_image(url)\n",
    "    \n",
    "    if image is not None:\n",
    "        ocr_text, confidence = perform_ocr(image)\n",
    "        image_results.append({\n",
    "            'post_id': post_id,\n",
    "            'url': url,\n",
    "            'ocr_text': ocr_text,\n",
    "            'confidence': confidence,\n",
    "            'status': 'success'\n",
    "        })\n",
    "    else:\n",
    "        image_results.append({\n",
    "            'post_id': post_id,\n",
    "            'url': url,\n",
    "            'ocr_text': '',\n",
    "            'confidence': 0.0,\n",
    "            'status': 'download_failed'\n",
    "        })\n",
    "    time.sleep(0.1)\n",
    "\n",
    "image_ocr_df = pd.DataFrame(image_results)\n",
    "success_count = (image_ocr_df['status'] == 'success').sum()\n",
    "with_text = (image_ocr_df['ocr_text'].str.len() > 0).sum()\n",
    "\n",
    "print(f\"\\nâœ… ì´ë¯¸ì§€ OCR ì™„ë£Œ!\")\n",
    "print(f\"   - ì„±ê³µ: {success_count}/{len(image_df)}\")\n",
    "print(f\"   - í…ìŠ¤íŠ¸ ì¶”ì¶œ: {with_text}ê°œ\")\n",
    "print(f\"   - í‰ê·  ì‹ ë¢°ë„: {image_ocr_df['confidence'].mean():.2%}\")\n",
    "\n",
    "image_ocr_df.to_csv('/content/pmi_data/results/image_ocr_results.csv', index=False, encoding='utf-8-sig')\n",
    "print(f\"ğŸ’¾ ì¤‘ê°„ ê²°ê³¼ ì €ì¥: image_ocr_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ì…€ 6] ë¹„ë””ì˜¤ ì²« í”„ë ˆì„ OCR\n",
    "**ì˜ˆìƒ ì‹œê°„: 8-12ë¶„**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ğŸ¬ ë¹„ë””ì˜¤ ì²« í”„ë ˆì„ OCR ì²˜ë¦¬ ì‹œì‘\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "video_frame_results = []\n",
    "naver_videos = video_df[video_df['type'] == 'naver_blog'].copy()\n",
    "\n",
    "print(f\"ì²˜ë¦¬ ëŒ€ìƒ: ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë¹„ë””ì˜¤ {len(naver_videos)}ê°œ\\n\")\n",
    "\n",
    "for idx, row in tqdm(naver_videos.iterrows(), total=len(naver_videos), desc=\"ë¹„ë””ì˜¤ í”„ë ˆì„ OCR\"):\n",
    "    post_id = row['post_id']\n",
    "    url = row['url']\n",
    "    \n",
    "    try:\n",
    "        temp_video = f\"/content/pmi_data/temp_videos/video_{idx}.mp4\"\n",
    "        headers = {'User-Agent': 'Mozilla/5.0', 'Referer': 'https://blog.naver.com/'}\n",
    "        response = requests.get(url, headers=headers, timeout=30, stream=True)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            with open(temp_video, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            \n",
    "            frame = extract_first_frame(temp_video)\n",
    "            if frame is not None:\n",
    "                ocr_text, confidence = perform_ocr(frame)\n",
    "                video_frame_results.append({\n",
    "                    'post_id': post_id, 'url': url, 'frame_ocr_text': ocr_text,\n",
    "                    'confidence': confidence, 'status': 'success'\n",
    "                })\n",
    "            else:\n",
    "                video_frame_results.append({\n",
    "                    'post_id': post_id, 'url': url, 'frame_ocr_text': '',\n",
    "                    'confidence': 0.0, 'status': 'frame_extraction_failed'\n",
    "                })\n",
    "            if os.path.exists(temp_video):\n",
    "                os.remove(temp_video)\n",
    "        else:\n",
    "            video_frame_results.append({'post_id': post_id, 'url': url, 'frame_ocr_text': '', 'confidence': 0.0, 'status': 'download_failed'})\n",
    "    except Exception as e:\n",
    "        video_frame_results.append({'post_id': post_id, 'url': url, 'frame_ocr_text': '', 'confidence': 0.0, 'status': f'error: {str(e)[:50]}'})\n",
    "    time.sleep(0.2)\n",
    "\n",
    "if video_frame_results:\n",
    "    video_frame_df = pd.DataFrame(video_frame_results)\n",
    "    video_frame_df.to_csv('/content/pmi_data/results/video_frame_ocr_results.csv', index=False, encoding='utf-8-sig')\n",
    "    success = (video_frame_df['status'] == 'success').sum()\n",
    "    print(f\"\\nâœ… ë¹„ë””ì˜¤ í”„ë ˆì„ OCR ì™„ë£Œ! ì„±ê³µ: {success}/{len(naver_videos)}\")\n",
    "    print(f\"ğŸ’¾ ì €ì¥: video_frame_ocr_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ì…€ 7] ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œ\n",
    "**ì˜ˆìƒ ì‹œê°„: 3-5ë¶„**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“º ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œ ì‹œì‘\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "youtube_videos = video_df[video_df['type'] == 'youtube'].copy()\n",
    "print(f\"ì²˜ë¦¬ ëŒ€ìƒ: ìœ íŠœë¸Œ ë¹„ë””ì˜¤ {len(youtube_videos)}ê°œ\\n\")\n",
    "\n",
    "youtube_transcript_results = []\n",
    "\n",
    "for idx, row in tqdm(youtube_videos.iterrows(), total=len(youtube_videos), desc=\"ìœ íŠœë¸Œ ìë§‰\"):\n",
    "    post_id = row['post_id']\n",
    "    url = row['url']\n",
    "    video_id = row.get('youtube_video_id', '')\n",
    "    \n",
    "    if not video_id:\n",
    "        youtube_transcript_results.append({'post_id': post_id, 'url': url, 'video_id': '', 'transcript': '', 'language': '', 'status': 'no_video_id'})\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        try:\n",
    "            transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['ko'])\n",
    "            language = 'ko'\n",
    "        except:\n",
    "            try:\n",
    "                transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "                language = 'en'\n",
    "            except:\n",
    "                transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "                language = 'auto'\n",
    "        full_transcript = ' '.join([item['text'] for item in transcript_list])\n",
    "        youtube_transcript_results.append({'post_id': post_id, 'url': url, 'video_id': video_id, 'transcript': full_transcript, 'language': language, 'status': 'success'})\n",
    "    except TranscriptsDisabled:\n",
    "        youtube_transcript_results.append({'post_id': post_id, 'url': url, 'video_id': video_id, 'transcript': '', 'language': '', 'status': 'transcripts_disabled'})\n",
    "    except NoTranscriptFound:\n",
    "        youtube_transcript_results.append({'post_id': post_id, 'url': url, 'video_id': video_id, 'transcript': '', 'language': '', 'status': 'no_transcript'})\n",
    "    except Exception as e:\n",
    "        youtube_transcript_results.append({'post_id': post_id, 'url': url, 'video_id': video_id, 'transcript': '', 'language': '', 'status': f'error: {str(e)[:50]}'})\n",
    "\n",
    "if youtube_transcript_results:\n",
    "    youtube_df_result = pd.DataFrame(youtube_transcript_results)\n",
    "    youtube_df_result.to_csv('/content/pmi_data/results/youtube_transcript_results.csv', index=False, encoding='utf-8-sig')\n",
    "    success = (youtube_df_result['status'] == 'success').sum()\n",
    "    no_transcript = (youtube_df_result['status'].isin(['no_transcript', 'transcripts_disabled'])).sum()\n",
    "    print(f\"\\nâœ… ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œ ì™„ë£Œ!\")\n",
    "    print(f\"   - ìë§‰ ìˆìŒ: {success}/{len(youtube_videos)}\")\n",
    "    print(f\"   - ìë§‰ ì—†ìŒ: {no_transcript}/{len(youtube_videos)}\")\n",
    "    print(f\"ğŸ’¾ ì €ì¥: youtube_transcript_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ì…€ 8] Whisper ìŒì„±ì¸ì‹\n",
    "**ì˜ˆìƒ ì‹œê°„: 25-35ë¶„** â˜•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import subprocess\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ™ï¸ Whisper ìŒì„±ì¸ì‹ ì²˜ë¦¬ ì‹œì‘\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"ğŸ”§ Whisper ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "print(\"âœ… Whisper ì¤€ë¹„ ì™„ë£Œ!\\n\")\n",
    "\n",
    "youtube_no_transcript = youtube_df_result[youtube_df_result['status'] != 'success']['video_id'].tolist()\n",
    "naver_video_items = naver_videos[['post_id', 'url']].to_dict('records')\n",
    "\n",
    "print(f\"ì²˜ë¦¬ ëŒ€ìƒ:\")\n",
    "print(f\"  - ìë§‰ ì—†ëŠ” ìœ íŠœë¸Œ: {len(youtube_no_transcript)}ê°œ\")\n",
    "print(f\"  - ë„¤ì´ë²„ ë¹„ë””ì˜¤: {len(naver_video_items)}ê°œ\")\n",
    "print(f\"  ì´ {len(youtube_no_transcript) + len(naver_video_items)}ê°œ\\n\")\n",
    "\n",
    "whisper_results = []\n",
    "\n",
    "if youtube_no_transcript:\n",
    "    print(\"ğŸ“º ìë§‰ ì—†ëŠ” ìœ íŠœë¸Œ ë¹„ë””ì˜¤ Whisper ì²˜ë¦¬...\")\n",
    "    for video_id in tqdm(youtube_no_transcript, desc=\"ìœ íŠœë¸Œ Whisper\"):\n",
    "        try:\n",
    "            audio_path = f\"/content/pmi_data/temp_videos/yt_{video_id}.mp3\"\n",
    "            cmd = ['yt-dlp', '-x', '--audio-format', 'mp3', '-o', audio_path, f'https://www.youtube.com/watch?v={video_id}']\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)\n",
    "            \n",
    "            if os.path.exists(audio_path):\n",
    "                result = whisper_model.transcribe(audio_path, language='ko')\n",
    "                transcript = result['text']\n",
    "                whisper_results.append({'video_id': video_id, 'type': 'youtube', 'transcript': transcript, 'status': 'success'})\n",
    "                os.remove(audio_path)\n",
    "            else:\n",
    "                whisper_results.append({'video_id': video_id, 'type': 'youtube', 'transcript': '', 'status': 'download_failed'})\n",
    "        except Exception as e:\n",
    "            whisper_results.append({'video_id': video_id, 'type': 'youtube', 'transcript': '', 'status': f'error: {str(e)[:50]}'})\n",
    "        time.sleep(1)\n",
    "\n",
    "if len(naver_video_items) > 0:\n",
    "    print(\"\\nğŸ“¹ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë¹„ë””ì˜¤ Whisper ì²˜ë¦¬...\")\n",
    "    for item in tqdm(naver_video_items, desc=\"ë„¤ì´ë²„ Whisper\"):\n",
    "        post_id = item['post_id']\n",
    "        url = item['url']\n",
    "        try:\n",
    "            video_path = f\"/content/pmi_data/temp_videos/naver_{post_id}.mp4\"\n",
    "            headers = {'User-Agent': 'Mozilla/5.0', 'Referer': 'https://blog.naver.com/'}\n",
    "            response = requests.get(url, headers=headers, timeout=60, stream=True)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                with open(video_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                result = whisper_model.transcribe(video_path, language='ko')\n",
    "                transcript = result['text']\n",
    "                whisper_results.append({'post_id': post_id, 'url': url, 'type': 'naver_blog', 'transcript': transcript, 'status': 'success'})\n",
    "                os.remove(video_path)\n",
    "            else:\n",
    "                whisper_results.append({'post_id': post_id, 'url': url, 'type': 'naver_blog', 'transcript': '', 'status': 'download_failed'})\n",
    "        except Exception as e:\n",
    "            whisper_results.append({'post_id': post_id, 'url': url, 'type': 'naver_blog', 'transcript': '', 'status': f'error: {str(e)[:50]}'})\n",
    "        time.sleep(1)\n",
    "\n",
    "if whisper_results:\n",
    "    whisper_df = pd.DataFrame(whisper_results)\n",
    "    whisper_df.to_csv('/content/pmi_data/results/whisper_transcript_results.csv', index=False, encoding='utf-8-sig')\n",
    "    success = (whisper_df['status'] == 'success').sum()\n",
    "    print(f\"\\nâœ… Whisper ìŒì„±ì¸ì‹ ì™„ë£Œ! ì„±ê³µ: {success}/{len(whisper_results)}\")\n",
    "    print(f\"ğŸ’¾ ì €ì¥: whisper_transcript_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ì…€ 9] ê²°ê³¼ í†µí•© ë° ìµœì¢… ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ğŸ“Š ê²°ê³¼ í†µí•© ë° ìµœì¢… ì €ì¥\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_images = len(image_ocr_df)\n",
    "successful_images = (image_ocr_df['status'] == 'success').sum()\n",
    "images_with_text = (image_ocr_df['ocr_text'].str.len() > 0).sum()\n",
    "\n",
    "total_videos = len(video_df)\n",
    "video_frame_success = len(video_frame_df) if 'video_frame_df' in locals() else 0\n",
    "youtube_success = (youtube_df_result['status'] == 'success').sum() if 'youtube_df_result' in locals() else 0\n",
    "whisper_success = (whisper_df['status'] == 'success').sum() if 'whisper_df' in locals() else 0\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ“Š ìµœì¢… ì²˜ë¦¬ ê²°ê³¼:\n",
    "\n",
    "1ï¸âƒ£ ì´ë¯¸ì§€ OCR:\n",
    "   - ì „ì²´: {total_images}ê°œ\n",
    "   - ì„±ê³µ: {successful_images}ê°œ ({successful_images/total_images*100:.1f}%)\n",
    "   - í…ìŠ¤íŠ¸ ì¶”ì¶œ: {images_with_text}ê°œ\n",
    "\n",
    "2ï¸âƒ£ ë¹„ë””ì˜¤ ì²˜ë¦¬:\n",
    "   - ì „ì²´: {total_videos}ê°œ\n",
    "   - í”„ë ˆì„ OCR: {video_frame_success}ê°œ\n",
    "   - ìœ íŠœë¸Œ ìë§‰: {youtube_success}ê°œ\n",
    "   - Whisper ìŒì„±ì¸ì‹: {whisper_success}ê°œ\n",
    "\n",
    "ğŸ“ ìƒì„±ëœ íŒŒì¼:\n",
    "   1. image_ocr_results.csv\n",
    "   2. video_frame_ocr_results.csv\n",
    "   3. youtube_transcript_results.csv\n",
    "   4. whisper_transcript_results.csv\n",
    "\"\"\")\n",
    "\n",
    "!cd /content/pmi_data/results && zip -r pmi_ocr_results.zip *.csv\n",
    "print(\"âœ… ê²°ê³¼ íŒŒì¼ ì••ì¶• ì™„ë£Œ: pmi_ocr_results.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ì…€ 10] ê²°ê³¼ ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"ğŸ“¥ ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "files.download('/content/pmi_data/results/pmi_ocr_results.zip')\n",
    "\n",
    "print(\"\\nğŸ‰ ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "print(\"ì••ì¶• íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œì»¬ì—ì„œ ì••ì¶•ì„ í’€ê³  í™•ì¸í•˜ì„¸ìš”.\")\n",
    "print(\"\\nğŸ“Š ë‹¤ìŒ ë‹¨ê³„: ì›ë³¸ í¬ë¡¤ë§ ë°ì´í„°ì™€ ë³‘í•©í•˜ì—¬ ë¶„ì„ ì‹œì‘!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
